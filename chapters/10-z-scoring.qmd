---
title: "Comparing Provider Performance: Z-scoring"
---

```{r}
#| include: false
# Setup Python environment
library(reticulate)
```

::: {.callout-tip icon=false}
## Problem This Method Solves

You need to compare provider performance against a national benchmark and identify providers whose performance is statistically different from the average. For example:

- Which GP practices have unusually high or low cancer referral rates?
- Which care homes have incident rates significantly different from the national average?
- Which hospitals have waiting times that warrant investigation?

Z-scoring standardizes performance across providers, accounting for expected variation, so you can identify genuinely concerning differences.
:::

## What Z-scoring Does {#sec-z-scoring}

Z-scoring measures how many standard deviations a provider's observed value is from the expected (mean) value. It answers: "Is this provider's performance unusual enough that it's unlikely to be due to chance?"

**Key advantages**:
- Standardizes different indicators to the same scale
- Accounts for natural variation in the data
- Provides consistent banding across all indicators
- Identifies statistical outliers systematically

**What it does NOT do**:
- Prove causation (a high z-score shows difference, not why)
- Account for case-mix or risk adjustment (unless you build that in first)
- Replace professional judgment about whether to investigate

## Before You Start

::: {.callout-note icon=false}
## Note on Code Examples
The worked examples in this chapter use pseudo-randomly generated data to illustrate the methods. To ensure consistent results between R and Python, the Python code uses the same dataset generated by R (via `r.data_name`). The commented-out Python code shows how you would generate equivalent data independently if needed.
:::

Check these requirements before applying z-scoring:

### Data Requirements
- [ ] **Data type**: Continuous data (rates, percentages, ratios) or proportions from counts
- [ ] **Sample size**: At least 30 providers for reliable results
- [ ] **Comparability**: Providers are reasonably comparable (same indicator definition)
- [ ] **Quality checks**: Completed pre-analysis QA (see @sec-qa-principles)

### When Z-scoring May Not Be Appropriate
- **Very small denominators** (e.g., <10 events): Standard z-scoring can be unreliable, see Overthinking box
- **Highly skewed data**: May need Working z-scores instead, see Overthinking box
- **Different populations**: If providers serve fundamentally different populations, adjust first
- **Categorical outcomes**: Use different methods for categorical data

### What You Need
- Provider-level data with numerator and denominator (for proportions) or observed values (for continuous measures)
- National or comparison group mean and standard deviation
- Statistical software (R recommended, examples provided)

::: {.callout-note icon=false}
## ðŸ“– Complete Worked Examples

**Example 1: GP Access Analysis** demonstrates z-scoring with realistic CQC data:

- Handling survey data with missing values and varying response rates
- Checking z-scoring assumptions (normality, independence, sample sizes)
- Dealing with outliers and small denominators
- Calculating z-scores and assigning performance bands
- Conducting sensitivity analysis and interpreting results

**Time**: 2-3 hours | **Complexity**: Medium

[View Example 1: GP Access Analysis â†’](../examples/example-01-complete.qmd)

**Example 3: Mental Health Wait Times** demonstrates z-scoring with skewed data:

- Handling non-normal distributions
- Deciding between standard and Working z-scores
- Applying transformations appropriately
- Interpreting results when assumptions are violated

**Time**: 2-3 hours | **Complexity**: Medium-High

[View Example 3: Mental Health Wait Times â†’](../examples/example-03-complete.qmd)
:::

## Step-by-Step Guide

### Step 1: Calculate the Indicator Value

For each provider, calculate the indicator you're analysing.

**For proportions** (e.g., percentage of patients seen within target):
$$\text{Proportion} = \frac{\text{Number meeting target}}{\text{Total number}}$$

**For rates** (e.g., incidents per 1,000 bed days):
$$\text{Rate} = \frac{\text{Number of events}}{\text{Denominator}} \times 1000$$

**For continuous measures** (e.g., mean waiting time):
Use the observed value directly.

### Step 2: Calculate National Mean and Standard Deviation

Calculate the mean (Î¼) and standard deviation (Ïƒ) across all providers:

$$\mu = \frac{\sum x_i}{n}$$

$$\sigma = \sqrt{\frac{\sum (x_i - \mu)^2}{n-1}}$$

where $x_i$ is each provider's indicator value and $n$ is the number of providers.

### Step 3: Calculate Z-scores

For each provider, calculate how many standard deviations they are from the mean:

$$Z = \frac{x - \mu}{\sigma}$$

where:
- $x$ = provider's observed value
- $\mu$ = national mean
- $\sigma$ = national standard deviation

### Step 4: Assign Bands

CQC typically uses these z-score bands:

| Z-score Range | Interpretation | Expected % of Providers |
|---------------|----------------|------------------------|
| z < -3 | Significantly below average | 0.1% |
| -3 â‰¤ z < -2 | Below average | 2.1% |
| -2 â‰¤ z < -1 | Slightly below average | 13.6% |
| -1 â‰¤ z < +1 | As expected | 68.2% |
| +1 â‰¤ z < +2 | Slightly above average | 13.6% |
| +2 â‰¤ z < +3 | Above average | 2.1% |
| z â‰¥ +3 | Significantly above average | 0.1% |

![Normal distribution showing z-score bands and expected percentages. The vertical dashed lines mark standard deviations (Ïƒ) from the mean (Î¼). In a normal distribution, 68.2% of values fall within Â±1Ïƒ, 95.4% within Â±2Ïƒ, and 99.7% within Â±3Ïƒ.](../figures/zscore-distribution.png){#fig-zscore-distribution width=100%}

**Direction matters**: For some indicators, high values are concerning (e.g., incident rates). For others, low values are concerning (e.g., percentage meeting targets). Interpret bands accordingly.

### Step 5: Visualize and Interpret

Plot the distribution to check it looks reasonable:
- Does the distribution look roughly normal?
- Are the proportions in each band close to expected?
- Are there any extreme outliers?

A histogram of z-scores with reference lines at Â±1, Â±2, and Â±3 standard deviations helps visualize the distribution and identify any issues.

## Worked Example: GP Cancer Referral Rates

**Scenario**: You're analysing 2-week-wait cancer referral rates across 150 GP practices. You want to identify practices with unusually high or low referral rates.

**Data structure**:

:::: {.panel-tabset}
## R
```{r}
#| label: zscore-example-data
# Simulated data for illustration (seed for reproducibility)
set.seed(42)
gp_data <- data.frame(
  practice_id = paste0("GP", 1:150),
  referrals = rpois(150, lambda = 45),  # Number of referrals
  list_size = rnorm(150, mean = 8000, sd = 2000)  # Practice size
)

# Calculate referral rate per 1,000 patients
gp_data$referral_rate <- (gp_data$referrals / gp_data$list_size) * 1000
```

## Python
```{python}
import pandas as pd
import numpy as np

# Use the same data from R for consistent results
gp_data = r.gp_data

# Alternative: Generate data in Python (commented out for consistency)
# np.random.seed(42)
# gp_data = pd.DataFrame({
#     'practice_id': [f'GP{i}' for i in range(1, 151)],
#     'referrals': np.random.poisson(45, 150),
#     'list_size': np.random.normal(8000, 2000, 150)
# })
# gp_data['referral_rate'] = (gp_data['referrals'] / gp_data['list_size']) * 1000
```
::::

**Step 1: Calculate indicator** (already done above)

**Step 2: Calculate national statistics**:

:::: {.panel-tabset}
## R
```{r}
#| label: zscore-national-stats
national_mean <- mean(gp_data$referral_rate)
national_sd <- sd(gp_data$referral_rate)

cat("National mean:", round(national_mean, 2), "per 1,000\n")
cat("National SD:", round(national_sd, 2), "\n")
```

## Python
```{python}
national_mean = gp_data['referral_rate'].mean()
national_sd = gp_data['referral_rate'].std()

print(f"National mean: {national_mean:.2f} per 1,000")
print(f"National SD: {national_sd:.2f}")
```
::::

**Step 3: Calculate z-scores**:

:::: {.panel-tabset}
## R
```{r}
#| label: zscore-calculate
gp_data$z_score <- (gp_data$referral_rate - national_mean) / national_sd
```

## Python
```{python}
gp_data['z_score'] = (gp_data['referral_rate'] - national_mean) / national_sd
```
::::

**Step 4: Assign bands**:

:::: {.panel-tabset}
## R
```{r}
#| label: zscore-bands
gp_data$band <- cut(gp_data$z_score,
                    breaks = c(-Inf, -3, -2, -1, 1, 2, 3, Inf),
                    labels = c("Sig. below", "Below", "Slightly below",
                              "As expected", "Slightly above", "Above", "Sig. above"))

# Check distribution
table(gp_data$band)
```

## Python
```{python}
gp_data['band'] = pd.cut(gp_data['z_score'],
                         bins=[-np.inf, -3, -2, -1, 1, 2, 3, np.inf],
                         labels=['Sig. below', 'Below', 'Slightly below',
                                'As expected', 'Slightly above', 'Above', 'Sig. above'])

# Check distribution
gp_data['band'].value_counts().sort_index()
```
::::

**Step 5: Visualize the distribution**:

:::: {.panel-tabset}
## R
```{r}
#| label: zscore-viz-worked
#| fig-width: 8
#| fig-height: 5
hist(gp_data$z_score, breaks = 30, 
     main = "Distribution of Z-scores: GP Cancer Referral Rates",
     xlab = "Z-score", col = "lightblue")
abline(v = c(-3, -2, -1, 1, 2, 3), col = "red", lty = 2)
```

## Python
```{python}
import matplotlib.pyplot as plt

plt.hist(gp_data['z_score'], bins=30, color='lightblue', edgecolor='white')
plt.axvline(x=-3, color='red', linestyle='--')
plt.axvline(x=-2, color='red', linestyle='--')
plt.axvline(x=-1, color='red', linestyle='--')
plt.axvline(x=1, color='red', linestyle='--')
plt.axvline(x=2, color='red', linestyle='--')
plt.axvline(x=3, color='red', linestyle='--')
plt.title('Distribution of Z-scores: GP Cancer Referral Rates')
plt.xlabel('Z-score')
plt.ylabel('Frequency')
plt.show()
```
::::

**Step 6: Identify practices for investigation**:

:::: {.panel-tabset}
## R
```{r}
#| label: zscore-identify-practices
# Practices significantly above average (potential over-referral)
high_referrers <- gp_data[gp_data$z_score >= 2, 
                          c("practice_id", "referral_rate", "z_score", "band")]

# Practices significantly below average (potential under-referral)
low_referrers <- gp_data[gp_data$z_score <= -2,
                         c("practice_id", "referral_rate", "z_score", "band")]

print(high_referrers)
print(low_referrers)
```

## Python
```{python}
# Practices significantly above average (potential over-referral)
high_referrers = gp_data[gp_data['z_score'] >= 2][['practice_id', 'referral_rate', 'z_score', 'band']]

# Practices significantly below average (potential under-referral)
low_referrers = gp_data[gp_data['z_score'] <= -2][['practice_id', 'referral_rate', 'z_score', 'band']]

print(high_referrers)
print(low_referrers)
```
::::

**Step 7: Visualize**:

:::: {.panel-tabset}
## R
```{r}
#| label: zscore-visualize-final
#| fig-width: 12
#| fig-height: 5
par(mfrow = c(1, 2))

# Distribution of z-scores
hist(gp_data$z_score, breaks = 30, 
     main = "Distribution of GP Cancer Referral Z-scores",
     xlab = "Z-score", col = "lightblue", border = "white")
abline(v = c(-3, -2, 2, 3), col = "red", lty = 2, lwd = 2)
abline(v = 0, col = "blue", lty = 1, lwd = 2)

# Scatter plot: rate vs practice size
plot(gp_data$list_size, gp_data$referral_rate,
     xlab = "Practice Size (List Size)",
     ylab = "Referral Rate per 1,000",
     main = "Cancer Referral Rates by Practice Size",
     col = ifelse(abs(gp_data$z_score) >= 2, "red", "grey"),
     pch = 19)
legend("topright", legend = c("As expected", "Flagged (|z| >= 2)"),
       col = c("grey", "red"), pch = 19)

par(mfrow = c(1, 1))
```

## Python
```{python}
import matplotlib.pyplot as plt

# Distribution of z-scores
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

ax1.hist(gp_data['z_score'], bins=30, color='lightblue', edgecolor='white')
ax1.axvline(x=-3, color='red', linestyle='--', linewidth=2)
ax1.axvline(x=-2, color='red', linestyle='--', linewidth=2)
ax1.axvline(x=2, color='red', linestyle='--', linewidth=2)
ax1.axvline(x=3, color='red', linestyle='--', linewidth=2)
ax1.axvline(x=0, color='blue', linestyle='-', linewidth=2)
ax1.set_title('Distribution of GP Cancer Referral Z-scores')
ax1.set_xlabel('Z-score')
ax1.set_ylabel('Frequency')

# Scatter plot: rate vs practice size
colors = ['red' if abs(z) >= 2 else 'grey' for z in gp_data['z_score']]
ax2.scatter(gp_data['list_size'], gp_data['referral_rate'], c=colors, alpha=0.6)
ax2.set_xlabel('Practice Size (List Size)')
ax2.set_ylabel('Referral Rate per 1,000')
ax2.set_title('Cancer Referral Rates by Practice Size')
ax2.legend(['As expected', 'Flagged (|z| >= 2)'], loc='upper right')

plt.tight_layout()
plt.show()
```
::::

## Interpreting Results

### What Z-scores Tell You

**z = 0**: Provider is exactly at the national average

**z = +1**: Provider is 1 standard deviation above average (better or worse depending on indicator direction)

**z = +2**: Provider is 2 standard deviations above averageâ€”only ~2% of providers this high by chance

**z = +3 or higher**: Provider is 3+ standard deviations above averageâ€”only ~0.1% this high by chance, warrants investigation

### Regulatory Interpretation

**|z| < 1 (68% of providers)**: Performance as expected. No action needed.

**1 â‰¤ |z| < 2 (27% of providers)**: Slightly different from average. Monitor but don't necessarily investigate.

**2 â‰¤ |z| < 3 (4% of providers)**: Notably different. Consider for investigation depending on:
- Clinical significance of the difference
- Provider history
- Other indicators
- Available inspection resources

**|z| â‰¥ 3 (0.2% of providers)**: Highly unusual. Strong candidate for investigation unless there's a clear explanation (e.g., specialist service, data error).

**Critical point**: Statistical significance â‰  clinical significance. A z-score of 2.5 might be statistically unusual but clinically unimportant if the absolute difference is tiny. Always consider both.

## Common Pitfalls

**Pitfall 1: Ignoring direction**
- High z-scores are concerning for negative indicators (incidents, mortality)
- Low z-scores are concerning for positive indicators (screening uptake, quality measures)
- Always check which direction matters

**Pitfall 2: Treating bands as hard thresholds**
- A provider with z = 1.99 is not fundamentally different from z = 2.01
- Use bands as guidance, not mechanical rules

**Pitfall 3: Ignoring small denominators**
- Providers with few patients/events have more uncertain estimates
- Their z-scores are less reliable, see Overthinking box

**Pitfall 4: Multiple testing**
- If you run z-scoring on 100 indicators, you'll flag some providers by chance
- Consider the full picture, not just one indicator

**Pitfall 5: Assuming causation**
- Z-scoring identifies difference, not cause
- Investigation determines whether it's a quality issue, case-mix, data error, or chance

## Documenting Your Analysis

For QA purposes, record:

**Data**:
- Indicator definition
- Time period
- Number of providers (before/after exclusions)
- Exclusion criteria applied

**Calculations**:
- National mean and SD
- Z-score formula used
- Banding thresholds applied

**Results**:
- Number/percentage of providers in each band
- Whether distribution looks normal
- Providers flagged for investigation

**Decisions**:
- Why you chose standard vs working z-scores
- How you handled small denominators
- What follow-up actions you recommend

::: {.callout-note icon=false}
## Rethinking: Why Z-scoring Succeeded at CQC

CQC has used z-scoring extensively for provider comparison since the early 2010s. Why has it become a core method?

**Consistency**: The same approach works across diverse indicatorsâ€”incident rates, waiting times, quality measures. This consistency aids interpretation and reduces analyst burden.

**Transparency**: The method is mathematically simple and explainable to non-statisticians. Stakeholders understand "3 standard deviations from average" more easily than complex model outputs.

**Scalability**: With hundreds of indicators and thousands of providers, we need automated, systematic approaches. Z-scoring provides this while still allowing professional judgment.

**Fairness perception**: Providers understand being compared to peers. Z-scoring makes this comparison explicit and quantified.

**Regulatory fit**: We need to prioritize inspection resources. Z-scoring provides a defensible, reproducible way to identify outliers for investigation.

**Pattern recognition**: Z-scoring reveals patterns across many providers (significant sameness) rather than focusing on isolated comparisons (significant differences). This system-level perspective helps identify widespread issues and good practices.

**Limitations acknowledged**: CQC has learned that z-scoring isn't perfectâ€”hence the development of Working z-scores for non-normal data, and ongoing work on risk adjustment. But the core approach remains valuable because it balances statistical rigor with operational feasibility.

The key lesson: A good-enough method that's consistently applied and well-understood often beats a perfect method that's complex and inconsistently used.
:::

::: {.callout-note icon=false}
## Rethinking: Z-scores vs Funnel Plots

Funnel plots and z-scores are closely related: both identify providers outside expected variation. So when should you use each?

**Z-scores**:
- Numerical output: easy to sort, filter, automate
- Clear banding for decision-making
- Works well in tables and databases
- Less visual, more analytical

**Funnel plots**:
- Visual output: great for presentations and reports
- Shows relationship between sample size and uncertainty
- Makes small-denominator issues obvious
- Less useful for automated flagging

**CQC practice**: Use z-scores for analysis and provider flagging. Use funnel plots for communicating results to stakeholders. They're complementary, not competing.

**Technical note**: Funnel plots typically show 95% and 99.8% confidence limits, which correspond roughly to z-scores of Â±2 and Â±3. A provider outside the funnel is a provider with |z| > 2 or 3.
:::

::: {.callout-warning icon=false}
## Overthinking: When Standard Z-scoring Fails

Standard z-scoring assumes your data is approximately normally distributed with constant variance. When these assumptions fail, results can be misleading.

**Problem 1: Small denominators**

With small denominators (e.g., a care home with 15 beds), percentages can only take certain values (0%, 6.7%, 13.3%, etc.). This creates:
- Discrete jumps in possible z-scores
- Overconfidence in flagging small providers
- Unfair comparison with large providers

**Solution**: Working z-scores use the exact distribution (Binomial for proportions, Poisson for counts) rather than assuming normality. This properly accounts for small-denominator uncertainty.

**Rule of thumb**: If denominators <30 or expected events <10, consider Working z-scores.

**Problem 2: Overdispersion**

Sometimes data has more variation than expected under standard assumptions. For example, incident counts might vary more than a Poisson distribution predicts due to clustering or systematic differences.

**Detection**: If >5% of providers fall outside Â±2 SD when you expect ~5%, you may have overdispersion.

**Solution**: Working z-scores can model overdispersion. Alternatively, investigate why variation is higher than expectedâ€”it might indicate real systematic differences.

**Problem 3: Skewed data**

If your indicator is heavily skewed (e.g., length of stay with long right tail), the normal distribution assumption fails. Standard z-scores will:
- Flag too many providers in the tail direction
- Miss providers in the other direction
- Give misleading band percentages

**Detection**: Plot histogram. If mean >> median or you see long tail, you have skewness.

**Solutions**:
1. Transform data (e.g., log transformation) before z-scoring
2. Use Working z-scores with appropriate distribution
3. Use median-based methods instead

**When to escalate**: If you're unsure whether your data meets assumptions, or if you need Working z-scores, consult the Guild. Working z-scores require model-based approaches (Beta distribution for proportions, Poisson/Negative Binomial for counts) that need specialist implementation.
:::

::: {.callout-warning icon=false}
## Overthinking: Working Z-scores Technical Details

**Working z-scores** extend standard z-scoring to non-normal data by using the exact distribution of the indicator.

**For proportions** (e.g., percentage of patients meeting target):

Apply arcsine transformation to stabilize variance:

$$Y_i = \arcsin(\sqrt{r_i/n_i})$$
$$T = \arcsin(\sqrt{t})$$

where $r_i$ = numerator, $n_i$ = denominator, $t$ = target proportion.

Provider-specific standard deviation:

$$s_i = \frac{1}{2\sqrt{n_i}}$$

Working z-score:

$$Z_i = \frac{Y_i - T}{s_i} = \frac{\arcsin(\sqrt{r_i/n_i}) - \arcsin(\sqrt{t})}{1/(2\sqrt{n_i})}$$

**For count ratios** (e.g., standardized mortality ratios):

Apply logarithmic transformation (adding 0.5 to handle zeros):

$$Y_i = \log_e\left(\frac{O_{i,1} + 0.5}{O_{i,2} + 0.5}\right)$$
$$T = \log_e(t)$$

Provider-specific standard deviation:

$$s_i = \sqrt{\frac{1}{O_{i,1} + 0.5} + \frac{1}{O_{i,2} + 0.5}}$$

Working z-score:

$$Z_i = \frac{Y_i - T}{s_i}$$

**Overdispersion adjustment**: After calculating z-scores, check if variance exceeds 1.0. If overdispersed, apply correction factor (see CQC Working z-scores guidance).

**Implementation**: Working z-scores require:
- Fitting distributional models to data
- Estimating parameters (Beta shape parameters, Poisson/NB dispersion)
- Calculating z-scores from model-based expectations

**CQC practice**: The Guild has R scripts for Working z-scores. Don't implement from scratchâ€”use established code that's been validated.

**When to use**:
- Denominators <30 or expected events <10
- Clear evidence of overdispersion
- Proportions near 0% or 100% (where normal approximation fails)
- Stakeholder concerns about fairness to small providers

**When NOT to use**:
- Data meets standard z-score assumptions (simpler is better)
- Sample sizes large enough that normal approximation works well
- You don't have specialist support to implement correctly

**Key principle**: Working z-scores are more technically correct but more complex. Use them when standard z-scores would be misleading, not as default.
:::

## Related Approaches

**For different data challenges**:
- **Heavily skewed data** â†’ See Overthinking box on Working z-scores above, or consult Guild for transformation approaches
- **Proportions near 0% or 100%** â†’ Wilson score intervals (Working z-scores)
- **Very small denominators** â†’ Working z-scores or funnel plots
- **Monitoring over time** â†’ Statistical Process Control (@sec-spc-basics)
- **Only two groups to compare** â†’ Consider t-tests (@sec-t-tests) if no national benchmark

**For foundational concepts**:
- **Understanding variation** â†’ @sec-variation-uncertainty
- **Data quality checks** â†’ @sec-qa-principles
- **Missing data investigation** â†’ @sec-missing-data-foundations

**For complete workflows**:
- **GP Access Analysis** â†’ Example 1 (worked example with z-scoring)
- **Mental Health Wait Times** â†’ Example 3 (z-scoring with skewed data)

## When to Escalate to the Guild

Escalate to the Quantitative Guild when:

- **Distribution extremely skewed** despite transformations or log-scaling
- **Negative variance estimates** appear in your calculations
- **Systematic patterns in residuals** after z-scoring (suggests model misspecification)
- **Multiple testing** across many indicators simultaneously (need correction)
- **Working z-scores needed** but unsure of implementation or interpretation
- **Small numbers** combined with high stakes decisions
- **Stakeholder challenge** to methodology requiring specialist defense
- **New indicator development** where z-scoring appropriateness is unclear

## Key Takeaways

::: {.callout-important icon=false}
## Essential Points

1. **Z-scoring standardizes comparison** across providers and indicators
2. **Interpret bands as guidance**, not mechanical thresholds
3. **Statistical significance â‰  clinical significance**: consider both
4. **Check assumptions**: Normal distribution, adequate sample sizes
5. **Small denominators need special handling**: consider Working z-scores
6. **Document decisions** for QA and defensibility
7. **Z-scores identify difference, not cause**: investigation determines why

Z-scoring is CQC's workhorse method for provider comparison. Master it, understand its limitations, and know when to escalate to specialist methods.
:::

## Further Reading

**Internal CQC Resources**:

- **Z-scoring Guidance Document** (Internal): Detailed technical specifications for Standard and Working z-scores
- **QA Framework**: Documentation standards for z-scoring analyses
- **Guild Terms of Reference**: When and how to escalate for specialist support

**External Guidance**:

- **Spiegelhalter DJ** (2005). "Funnel plots for comparing institutional performance." *Statistics in Medicine*, 24(8), 1185-1202. (Foundational paper on funnel plots and provider comparison)
- **AQuA Book** (Chapter on Uncertainty): Communicating uncertainty in performance metrics
- **NHS Digital Statistical Process Control guidance**: Alternative approaches to provider comparison

**Online Resources**:

- Understanding z-scores: https://www.statology.org/z-score/
- Funnel plots explained: https://www.phc.ox.ac.uk/research/resources/funnel-plots
- Normal distribution properties: https://www.statology.org/normal-distribution/
