---
title: "Outlier Detection and Treatment"
---

```{r}
#| include: false
library(reticulate)
```

::: {.callout-tip icon=false}
## Problem This Method Solves

You need to systematically identify and investigate data points that stand out from the rest of your dataset. For example:

- A care home reporting staff-to-resident ratios that seem implausibly high or low
- Hospital waiting times that are extreme compared to similar providers
- Incident rates that warrant immediate investigation
- Data quality issues that need identification before formal analysis

Outlier detection provides computational tools to flag unusual observations for investigation, going beyond visual inspection to provide systematic, reproducible criteria.
:::

## What Outlier Detection Does {#sec-outlier-methods}

Outlier detection methods apply statistical rules to identify observations that are unusually far from the bulk of the data. They answer: "Which values are extreme enough to warrant investigation?"

**Key advantages**:
- Provides systematic, reproducible criteria for flagging unusual values
- Scales to large datasets where visual inspection is impractical
- Offers multiple methods for different data characteristics
- Helps prioritize investigation efforts

**What it does NOT do**:
- Tell you whether an outlier is an error or genuine extreme value (requires investigation)
- Provide a single "correct" answer (different methods flag different points)
- Replace the conceptual understanding needed before analysis (see @sec-unusual-observations)
- Automatically fix data problems (treatment requires judgment)

## Before You Start

::: {.callout-note icon=false}
## Note on Code Examples
The worked examples in this chapter use pseudo-randomly generated data to illustrate the methods. To ensure consistent results between R and Python, the Python code uses the same dataset generated by R (via `r.data_name`). The commented-out Python code shows how you would generate equivalent data independently if needed.
:::

Check these requirements before applying outlier detection:

### Prerequisites
- [ ] **Conceptual foundation**: Read @sec-unusual-observations first to understand what "unusual" means
- [ ] **Data type**: Continuous or count data (categorical data needs different approaches)
- [ ] **Context understanding**: Know what values are plausible for your indicator
- [ ] **Quality checks**: Completed basic data quality checks (see @sec-qa-principles)

### When Outlier Detection May Not Be Appropriate
- **Bimodal distributions**: Two distinct groups may not have "outliers" but different populations
- **Very small samples** (<20): Most methods unreliable
- **Expected extreme values**: If you know some values should be extreme (e.g., specialist units)
- **Time-series data**: Use SPC methods instead (@sec-spc-basics)

### What You Need
- Provider-level or observation-level data
- Understanding of the indicator and what constitutes a plausible range
- Statistical software (R or Python)
- Documentation of why you're detecting outliers (investigation? data quality?)

::: {.callout-note icon=false}
## ðŸ“– Complete Worked Example

**Example 10: Care Home Staffing Investigation** demonstrates outlier detection in practice:

- Identifying extreme staffing ratios using multiple methods
- Investigating whether outliers are errors or genuine extremes
- Handling small provider variation
- Documenting decisions for regulatory defensibility
- Communicating findings to inspectors

**Time**: 2-3 hours | **Complexity**: Medium

[View Example 10: Care Home Staffing Investigation â†’](../examples/example-10-outliers.qmd)
:::

## Step-by-Step Guide

### Step 1: Visualize Your Data First

Always start with visualization to understand the distribution and identify potential outliers visually.

**Create distribution plots**:
- Histogram with density curve to see overall shape
- Box plot to identify outliers using IQR method automatically

**What to look for**:
- Is the distribution symmetric or skewed?
- How many potential outliers are visible?
- Are outliers in one tail or both?
- Are outliers isolated points or small clusters?

### Step 2: Apply Multiple Detection Methods

Use 2-3 detection methods and look for consensus. Agreement across methods strengthens the case for investigation.

**Method 1: Z-score**  
Flags observations more than k standard deviations from the mean (typically k = 2.5 or 3).

- **Formula**: $Z = \frac{x - \mu}{\sigma}$
- **When to use**: Data approximately normal, large sample (n > 30)
- **Limitation**: Sensitive to extreme values, assumes normality

**Method 2: Modified Z-score**  
Uses median and median absolute deviation (MAD) instead of mean/SD. More robust to outliers.

- **Formula**: $Z_{mod} = \frac{0.6745(x - median)}{MAD}$
- **Threshold**: Typically |Z_mod| > 3.5
- **When to use**: Data skewed or contains outliers that distort mean/SD
- **Advantage**: Robust to outliers, works with non-normal data

**Method 3: IQR Method (Tukey's Fences)**  
Uses interquartile range to define fences. The method box plots use.

- **Fences**: $Q1 - 1.5 \times IQR$ and $Q3 + 1.5 \times IQR$
- **When to use**: Non-normal data, want method matching box plots
- **Note**: Can flag many points in skewed distributions

### Step 3: Compare Method Results

Count how many methods flag each observation:
- **3 methods agree**: Highest priority for investigation
- **2 methods agree**: High priority
- **1 method only**: May be method-specific artifact

### Step 4: Investigate Flagged Observations

Detection is just the first step. Every flagged observation requires investigation.

**For each flagged observation, ask**:

1. **Data quality**: Could this be an error?
   - Implausible value (negative, impossible)?
   - Wrong unit or scale?
   - Missing data coded as extreme value?

2. **Context**: Why might this be genuinely extreme?
   - Small provider (high variation expected)?
   - Specialist service (different population)?
   - Known event (incident, merger, closure)?

3. **Consistency**: Do related values support this?
   - Other time periods for same provider?
   - Related indicators?
   - Inspection reports or narrative data?

### Step 5: Decide on Treatment

Based on investigation, choose appropriate action:

**Option 1: Keep as genuine extreme** â†’ Verified genuine, provides important information  
**Option 2: Remove from analysis** â†’ Confirmed data error, provider not comparable  
**Option 3: Adjust or transform** â†’ Systematic bias identified  
**Option 4: Separate analysis** â†’ Distinct subgroups identified

Document all decisions in a decision log for QA purposes.

## Worked Example: Care Home Staffing Ratios

### Scenario

You're analysing staffing ratios (qualified staff per 10 residents) across 80 care homes. You need to identify extreme values that warrant investigation before conducting your main analysis.

**Data**: Simulated staffing ratios (most homes 2.5-4.5 staff per 10 residents, with some genuine extremes and data errors)

### Step 1: Generate and Visualize Data

::: {.panel-tabset}
## R
```{r}
#| label: outlier-data
#| echo: true
set.seed(123)

# Generate 80 care homes with staffing ratios
care_homes <- data.frame(
  home_id = sprintf("CH%03d", 1:80),
  staff_per_10 = c(
    rnorm(65, mean = 3.2, sd = 0.7),      # Normal homes
    rnorm(5, mean = 5.5, sd = 0.3),        # High staffing (specialist units)
    rnorm(3, mean = 1.5, sd = 0.2),        # Low staffing
    c(0.02, 0.05, 9.8, 11.2, 0, 8.5, 7.2)  # Data errors and extremes
  )
)

# Ensure non-negative
care_homes$staff_per_10 <- pmax(care_homes$staff_per_10, 0)

# Display summary
summary(care_homes$staff_per_10)
```

```{r}
#| label: outlier-viz
#| echo: true
#| fig-width: 10
#| fig-height: 4

# Visualize distribution
par(mfrow = c(1, 2))

# Histogram
hist(care_homes$staff_per_10, breaks = 20, 
     main = "Distribution of Staffing Ratios",
     xlab = "Staff per 10 Residents",
     col = "lightblue", border = "white")

# Box plot
boxplot(care_homes$staff_per_10, 
        main = "Box Plot Showing Outliers",
        ylab = "Staff per 10 Residents",
        col = "lightblue", outcol = "red", outpch = 19)
abline(h = c(2.5, 4.0), lty = 2, col = "darkgreen")
text(1.3, 2.5, "Min guideline", col = "darkgreen", cex = 0.8)
text(1.3, 4.0, "Good practice", col = "darkgreen", cex = 0.8)
```

## Python
```{python}
#| label: outlier-data-py
#| echo: true

# Use R's data for consistency
care_homes = r.care_homes.copy()

# Display summary
print(care_homes['staff_per_10'].describe())
```

```{python}
#| label: outlier-viz-py
#| echo: true
#| fig-width: 10
#| fig-height: 4

import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

# Histogram
ax1.hist(care_homes['staff_per_10'], bins=20, color='lightblue', edgecolor='white')
ax1.set_xlabel('Staff per 10 Residents')
ax1.set_ylabel('Frequency')
ax1.set_title('Distribution of Staffing Ratios')
ax1.grid(alpha=0.3)

# Box plot
ax2.boxplot(care_homes['staff_per_10'], patch_artist=True,
            boxprops=dict(facecolor='lightblue'),
            flierprops=dict(marker='o', markerfacecolor='red', markersize=8))
ax2.axhline(2.5, linestyle='--', color='darkgreen', alpha=0.6)
ax2.axhline(4.0, linestyle='--', color='darkgreen', alpha=0.6)
ax2.set_ylabel('Staff per 10 Residents')
ax2.set_title('Box Plot Showing Outliers')
ax2.set_xticks([])
ax2.grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.show()
```
:::

**Observation**: Right-skewed distribution with several extreme values in both tails. Box plot shows ~10 potential outliers.

### Step 2: Apply Detection Methods

::: {.panel-tabset}
## R
```{r}
#| label: outlier-detect
#| echo: true

# Z-score method
care_homes$z_score <- scale(care_homes$staff_per_10)[,1]
care_homes$outlier_z <- abs(care_homes$z_score) > 2.5

# Modified Z-score method
median_val <- median(care_homes$staff_per_10)
mad_val <- mad(care_homes$staff_per_10)
care_homes$modified_z <- 0.6745 * (care_homes$staff_per_10 - median_val) / mad_val
care_homes$outlier_modz <- abs(care_homes$modified_z) > 3.5

# IQR method
Q1 <- quantile(care_homes$staff_per_10, 0.25)
Q3 <- quantile(care_homes$staff_per_10, 0.75)
IQR_val <- IQR(care_homes$staff_per_10)
lower_fence <- Q1 - 1.5 * IQR_val
upper_fence <- Q3 + 1.5 * IQR_val
care_homes$outlier_iqr <- care_homes$staff_per_10 < lower_fence | 
                          care_homes$staff_per_10 > upper_fence

# Summary table
detection_summary <- data.frame(
  Method = c("Z-score (|z| > 2.5)", "Modified Z-score (|z| > 3.5)", "IQR (k=1.5)"),
  Outliers = c(sum(care_homes$outlier_z), 
               sum(care_homes$outlier_modz), 
               sum(care_homes$outlier_iqr))
)
print(detection_summary, row.names = FALSE)
cat("\nIQR Fences: [", round(lower_fence, 2), ",", round(upper_fence, 2), "]\n")
```

## Python
```{python}
#| label: outlier-detect-py
#| echo: true

from scipy import stats
from scipy.stats import median_abs_deviation

# Z-score method
care_homes['z_score'] = stats.zscore(care_homes['staff_per_10'])
care_homes['outlier_z'] = abs(care_homes['z_score']) > 2.5

# Modified Z-score method
median_val = care_homes['staff_per_10'].median()
mad_val = median_abs_deviation(care_homes['staff_per_10'])
care_homes['modified_z'] = 0.6745 * (care_homes['staff_per_10'] - median_val) / mad_val
care_homes['outlier_modz'] = abs(care_homes['modified_z']) > 3.5

# IQR method
Q1 = care_homes['staff_per_10'].quantile(0.25)
Q3 = care_homes['staff_per_10'].quantile(0.75)
IQR_val = Q3 - Q1
lower_fence = Q1 - 1.5 * IQR_val
upper_fence = Q3 + 1.5 * IQR_val
care_homes['outlier_iqr'] = ((care_homes['staff_per_10'] < lower_fence) | 
                             (care_homes['staff_per_10'] > upper_fence))

# Summary table
import pandas as pd
detection_summary = pd.DataFrame({
    'Method': ['Z-score (|z| > 2.5)', 'Modified Z-score (|z| > 3.5)', 'IQR (k=1.5)'],
    'Outliers': [care_homes['outlier_z'].sum(), 
                 care_homes['outlier_modz'].sum(), 
                 care_homes['outlier_iqr'].sum()]
})
print(detection_summary.to_string(index=False))
print(f"\nIQR Fences: [{lower_fence:.2f}, {upper_fence:.2f}]")
```
:::

**Analysis**: Modified Z-score is more conservative (fewer false positives) due to robustness. IQR matches box plot visualization.

### Step 3: Compare Methods

::: {.panel-tabset}
## R
```{r}
#| label: outlier-compare
#| echo: true

# Count flags per home
care_homes$num_flags <- care_homes$outlier_z + care_homes$outlier_modz + 
                        care_homes$outlier_iqr

# Agreement summary
cat("Homes flagged by: ", paste(names(table(care_homes$num_flags)), "method(s) =", 
                                 table(care_homes$num_flags)), "\n")

# High-priority cases (2+ methods)
flagged <- care_homes[care_homes$num_flags >= 2, 
                      c("home_id", "staff_per_10", "z_score", "modified_z", "num_flags")]
flagged <- flagged[order(-abs(flagged$z_score)), ]

cat("\nHigh-priority homes (2+ methods agree):\n")
print(flagged, row.names = FALSE)
```

## Python
```{python}
#| label: outlier-compare-py
#| echo: true

# Count flags per home
care_homes['num_flags'] = (care_homes['outlier_z'].astype(int) + 
                           care_homes['outlier_modz'].astype(int) + 
                           care_homes['outlier_iqr'].astype(int))

# Agreement summary
agreement = care_homes['num_flags'].value_counts().sort_index()
print("Homes flagged by:", ", ".join([f"{k} method(s) = {v}" for k, v in agreement.items()]))

# High-priority cases
flagged = care_homes[care_homes['num_flags'] >= 2][
    ['home_id', 'staff_per_10', 'z_score', 'modified_z', 'num_flags']
].sort_values('z_score', key=abs, ascending=False)

print("\nHigh-priority homes (2+ methods agree):")
print(flagged.to_string(index=False))
```
:::

**Interpretation**: Homes with very low (<1) or very high (>7) ratios flagged by multiple methods are highest priority for investigation. Agreement across methods strengthens case that these warrant attention.

## Common Pitfalls

**Pitfall 1: Mechanical application without investigation**

Detecting outliers doesn't tell you what to do with them. Every flagged observation requires investigation to determine whether it's an error, genuine extreme, or contextual issue.

**Pitfall 2: Using wrong method for data type**

Z-scores assume normality. For highly skewed data, use modified Z-scores or IQR. Don't force methods onto data that violates assumptions.

**Pitfall 3: Removing outliers to "improve" results**

Never remove observations just to make your analysis tidier or improve model fit. Outliers contain information. Only remove when you have evidence of errors.

**Pitfall 4: Not documenting decisions**

"We removed 5 outliers" is insufficient for QA. Document which observations, which methods, what investigation revealed, and why you chose your treatment.

**Pitfall 5: Ignoring small numbers**

Small providers naturally show higher variation. An extreme value from a 15-bed home is less surprising than from a 100-bed home. Consider denominators when interpreting outliers.

**Pitfall 6: Single method only**

Using only one detection method misses the bigger picture. Apply 2-3 methods and look for consensus. Agreement across methods strengthens the case for investigation.

## Documenting Your Analysis

For QA purposes, record:

**Detection approach**:
- Which methods applied and why chosen
- Thresholds used and rationale
- Number of outliers flagged by each method
- Agreement/disagreement across methods

**Investigation process**:
- Which observations investigated
- Evidence gathered (provider contact, related data, narrative)
- Findings for each case
- Time spent on investigation

**Treatment decisions**:
- Which outliers kept, which removed, which adjusted
- Justification for each decision
- How treatment affects conclusions
- Sensitivity analysis (do conclusions change?)

**Evidence to retain**:
- Distribution plots (before and after)
- Detection method outputs with thresholds
- Investigation log with documented decisions
- Final dataset with exclusion flags and reasons

See @sec-qa-principles for full QA documentation requirements and templates.

## Related Approaches

**For different data characteristics**:
- **Multivariate outliers** â†’ Mahalanobis distance (escalate to Guild)
- **Time-series outliers** â†’ SPC methods (@sec-spc-basics)
- **Clustering-based detection** â†’ DBSCAN, Isolation Forest (escalate to Guild)

**For foundational understanding**:
- **Conceptual framework** â†’ @sec-unusual-observations (read first!)
- **Small provider variation** â†’ @sec-variation-uncertainty
- **Data quality checks** â†’ @sec-qa-principles

**For related analyses**:
- **Provider comparison** â†’ Z-scoring (@sec-z-scoring) handles expected variation
- **Missing data** â†’ @sec-missing-data-foundations (outliers vs missing different issues)

## When to Escalate to the Guild

Escalate when:

- **Multivariate outliers needed**: Outliers across multiple variables simultaneously
- **Advanced methods required**: Isolation Forest, Local Outlier Factor, or other machine learning approaches
- **High-stakes decisions**: Outlier treatment could materially affect regulatory action
- **Methodological challenge**: Stakeholder questions your outlier treatment approach
- **Unusual data structure**: Hierarchical data, spatial data, or other complex structures
- **Systematic patterns**: Many outliers suggesting data quality or population issues
- **Resource constraints**: Too many outliers to investigate manually

## Key Takeaways

::: {.callout-important icon=false}
## Essential Points

1. **Detection â‰  decision**: Flagging outliers is just the first step; investigation determines what to do
2. **Multiple methods essential**: Use 2-3 methods and look for consensus across approaches
3. **Context is critical**: An outlier in one context may be expected in another
4. **Document everything**: Which observations flagged, investigated, and treatedâ€”with justification
5. **Visual inspection first**: Always plot your data before applying detection methods
6. **Never remove mechanically**: Only remove outliers when you have evidence they're errors
7. **Consider small numbers**: Small providers show more variationâ€”don't penalize natural spread

Outlier detection is about systematically prioritizing investigation, not automatically removing inconvenient data points.
:::

## Further Reading

**Internal CQC Resources**:

- **QA Framework**: Documentation standards for outlier investigation and treatment
- **Guild Terms of Reference**: When and how to escalate for specialist support
- **Data Quality Guidance**: Distinguishing errors from genuine extremes

**External Guidance**:

- **Tukey JW** (1977). *Exploratory Data Analysis*. Addison-Wesley. (Classic text on outlier detection including box plots and fences)
- **Iglewicz B, Hoaglin DC** (1993). "How to Detect and Handle Outliers." *ASQC Basic References in Quality Control* vol. 16. (Modified Z-score method)
- **Rousseeuw PJ, Hubert M** (2011). "Robust statistics for outlier detection." *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*, 1(1), 73-79.

**Online Resources**:

- **NIST Engineering Statistics Handbook**: Section on outlier detection (freely available, comprehensive)
- **R Documentation**: `?boxplot.stats` for Tukey's method implementation
- **Python SciPy**: `scipy.stats.zscore` and `scipy.stats.median_abs_deviation` documentation