---
title: "Handling Missing Data"
---

```{r}
#| include: false
# Setup Python environment
library(reticulate)
# Use system Python (where pandas is installed)
use_python("C:/Users/fede/anaconda3/python.exe", required = TRUE)
```

::: {.callout-tip icon=false}
## Problem This Method Solves

You have missing data in your analysis and need to decide how to handle it. Missing data is common in CQC work: providers don't submit data, surveys have non-response, systems fail to record values.

**You're here because**:
- Your dataset has missing values and you need to proceed with analysis
- You need to understand **why** data is missing and **what** that means for your conclusions
- You want to handle missingness appropriately without introducing bias
- You need to document and justify your approach for QA

**Critical principle**: The most important question isn't "how do I fill in missing values?" It's "**why is this data missing, and what does that tell me?**"
:::

## What Missing Data Handling Does {#sec-missing-data-handling}

Handling missing data means making informed decisions about how to proceed with analysis when you don't have complete information. The goal is to minimize bias and properly represent uncertainty.

**What good missing data handling does**:
- **Investigates** why data is missing (the most critical step)
- **Visualizes** patterns of missingness to understand the problem
- **Chooses** an appropriate approach based on the missingness mechanism
- **Documents** assumptions and limitations transparently
- **Tests** sensitivity of conclusions to different approaches

**What it does NOT do**:
- Magically recover the true missing values
- Eliminate bias if data is Missing Not at Random (MNAR)
- Replace good data collection practices
- Guarantee correct conclusions

**Core message**: Missing data handling is about understanding and managing uncertainty, not eliminating it.

## Before You Start

::: {.callout-note icon=false}
## ðŸ“– Complete Worked Example

**Note**: A dedicated worked example for missing data handling is planned for future development. For now, refer to the in-chapter worked example below which demonstrates investigating missingness patterns and choosing appropriate approaches.

**Topics covered in the in-chapter example**:
- Visualizing missingness with upset plots
- Investigating why data is missing
- Choosing between complete case, sensitivity analysis, and imputation
- Documenting assumptions and limitations
- Both R and Python code provided

**Time**: 2-3 hours | **Complexity**: Medium
:::

::: {.callout-note icon=false}
## Note on Code Examples
The worked examples in this chapter use pseudo-randomly generated data to illustrate the methods. To ensure consistent results between R and Python, the Python code uses the same dataset generated by R (via `r.data_name`). The commented-out Python code shows how you would generate equivalent data independently if needed.
:::

::: {.callout-note icon=false}
## Prerequisites

**You need**:
- Basic understanding of your data structure and variables
- Familiarity with missing data mechanisms (MCAR, MAR, MNAR) from @sec-missing-data-foundations
- Statistical software (R or Python)

**When to use this chapter**:
- You have missing values in your dataset (any amount)
- You need to decide how to proceed with analysis
- You want to understand the impact of missingness on your conclusions

**When to escalate immediately**:
- >30-40% of data missing across multiple key variables
- Strong suspicion that poor performers systematically don't report (MNAR)
- High-stakes decision requiring sophisticated imputation methods
- Regulatory challenge expected and you need defensible approach

**Key principle**: Start simple. Most CQC analyses can use complete case analysis with clear documentation of limitations. Only use complex methods when necessary.
:::

## Step-by-Step Guide

### Step 1: Quantify and Visualize Missingness

**Purpose**: Understand how much data is missing and where.

**Calculate missingness**:
- Overall: What proportion of all values are missing?
- By variable: Which variables have the most missing data?
- By case: How many variables are missing per observation?

**Visualize patterns with upset plots** (highly recommended):
- Shows which variables are missing together
- Reveals patterns you'd miss with simple percentages
- Example: If patient experience and staffing are always missing together, that suggests a common cause

**Why this matters**: The pattern of missingness tells you about the mechanism. Random scattered missingness is different from systematic patterns.

### Step 2: Investigate WHY Data is Missing

**This is the most critical step.** Don't skip it.

**Ask these questions**:

1. **Is missingness related to observed characteristics?**
   - Do small providers have more missing data?
   - Do certain regions have more missingness?
   - Does missingness vary by time period?

2. **Is missingness related to the outcome?**
   - Do poorly-rated providers fail to report?
   - Do high-performing providers opt out of surveys?
   - This is MNAR, the hardest case

3. **Is there a substantive reason for missingness?**
   - Survey non-response (often MAR)
   - System failures (often MCAR)
   - Deliberate non-reporting (often MNAR)
   - Not applicable (structural zeros)

**Test missingness patterns**:
- Create a missingness indicator (missing vs not missing)
- Compare characteristics of complete vs incomplete cases
- Use chi-square tests for categorical variables
- Use t-tests for continuous variables

**Interpret results**:
- If missingness is unrelated to anything â†’ likely MCAR (rare)
- If missingness is related to observed variables but not outcome â†’ likely MAR
- If missingness is related to the outcome itself â†’ likely MNAR (problematic)

### Step 3: Choose Your Approach

**Decision tree**:

**If missingness is <5% and appears random (MCAR)**:
â†’ **Complete case analysis** is fine. Document and move on.

**If missingness is 5-20% and MAR**:
â†’ **Complete case analysis** with sensitivity check (see below)
â†’ Consider simple imputation if sample size is critical

**If missingness is 20-40% and MAR**:
â†’ **Sensitivity analysis** comparing complete case to simple imputation
â†’ Consider consulting Guild for multiple imputation

**If missingness is >40% or MNAR suspected**:
â†’ **Escalate to Guild immediately**
â†’ May not be possible to proceed without strong assumptions

**Practical approaches for most CQC work**:

**Complete case analysis** (most common):
- Analyze only cases with complete data
- Simple, transparent, defensible
- Document how many cases excluded and why
- Check if excluded cases differ systematically

**Sensitivity analysis**:
- Run analysis with complete cases
- Run again with simple imputation (mean, median, or mode)
- Compare results: if similar, findings are robust
- If different, investigate why and report both

**Simple imputation** (use cautiously):
- Replace missing with mean (continuous) or mode (categorical)
- Quick but underestimates uncertainty
- Only use if missingness is low and MAR
- Always compare with complete case

### Step 4: Document Your Approach

**Required documentation** (for QA):

1. **Amount of missingness**: Percentage by variable and overall
2. **Pattern of missingness**: Which variables/cases affected
3. **Investigation results**: Why you think data is missing
4. **Mechanism assumption**: MCAR, MAR, or MNAR and justification
5. **Approach chosen**: Complete case, imputation, or other
6. **Sensitivity check**: How results change with different approaches
7. **Limitations**: What assumptions you're making and their implications

**Example documentation**:
> "Patient experience data was missing for 87 of 542 GP practices (16%). Missingness was higher in small practices (Ï‡Â² = 12.3, p = 0.002) and deprived areas (t = -3.1, p = 0.002), but not related to other quality indicators, suggesting MAR. Complete case analysis was used (N=455), excluding practices with missing data. Sensitivity analysis using mean imputation produced similar results (difference in effect estimate: 0.2 points, 95% CI: [-0.1, 0.5]), suggesting findings are robust. Limitation: If small, deprived practices with poor patient experience were less likely to respond (MNAR), our estimates may be optimistic."

### Step 5: Conduct Sensitivity Analysis

**Purpose**: Test if conclusions change under different assumptions.

**Simple sensitivity check**:
1. Run analysis with complete cases (exclude missing data)
2. Run analysis with simple imputation (e.g., mean imputation)
3. Compare results

**Interpret**:
- If results are similar â†’ findings are robust to missing data handling
- If results differ substantially â†’ missingness is affecting conclusions, investigate further or escalate

**What to compare**:
- Point estimates (coefficients, means, etc.)
- Confidence intervals
- p-values and significance
- Practical conclusions

## Worked Example: Care Home Quality Ratings with Missing Data

**Scenario**: You're analyzing care home quality ratings across 450 care homes. Three key variables have missing data:
- Staffing levels: 12% missing
- Resident satisfaction: 18% missing  
- Inspection score: 8% missing

**Question**: Does staffing level predict quality ratings? How should we handle the missing data?

### Investigation Phase

**Step 1: Visualize missingness with upset plot**

::: {.panel-tabset}
## R
```{r}
#| echo: false
#| warning: false
library(UpSetR)

# Simulate care home data for demonstration
set.seed(123)
n <- 450
care_homes <- data.frame(
  id = 1:n,
  staffing = rnorm(n, 15, 3),
  satisfaction = rnorm(n, 75, 10),
  inspection = rnorm(n, 80, 8),
  size = sample(20:100, n, replace = TRUE),
  region = sample(c("North", "South", "East", "West"), n, replace = TRUE)
)

# Introduce missingness (MAR: more missing in small homes)
care_homes$staffing[care_homes$size < 40 & runif(n) < 0.20] <- NA
care_homes$satisfaction[care_homes$size < 40 & runif(n) < 0.25] <- NA
care_homes$inspection[runif(n) < 0.08] <- NA
```

```{r}
#| fig-width: 8
#| fig-height: 6
#| warning: false
# Create upset plot showing missingness patterns
# Convert to binary missingness indicators
missing_data <- data.frame(
  staffing = as.numeric(is.na(care_homes$staffing)),
  satisfaction = as.numeric(is.na(care_homes$satisfaction)),
  inspection = as.numeric(is.na(care_homes$inspection))
)

upset(missing_data, 
      sets = c("staffing", "satisfaction", "inspection"),
      order.by = "freq",
      main.bar.color = "steelblue",
      sets.bar.color = "darkred",
      text.scale = 1.2)
```

## Python
```{python}
#| echo: false
#| warning: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from upsetplot import from_indicators, UpSet
import warnings

# Suppress FutureWarnings from upsetplot
warnings.filterwarnings('ignore', category=FutureWarning)

# Simulate care home data
np.random.seed(123)
n = 450
care_homes = pd.DataFrame({
    'id': range(1, n+1),
    'staffing': np.random.normal(15, 3, n),
    'satisfaction': np.random.normal(75, 10, n),
    'inspection': np.random.normal(80, 8, n),
    'size': np.random.randint(20, 101, n),
    'region': np.random.choice(['North', 'South', 'East', 'West'], n)
})

# Introduce missingness (MAR: more missing in small homes)
small_homes = care_homes['size'] < 40
care_homes.loc[small_homes & (np.random.rand(n) < 0.20), 'staffing'] = np.nan
care_homes.loc[small_homes & (np.random.rand(n) < 0.25), 'satisfaction'] = np.nan
care_homes.loc[np.random.rand(n) < 0.08, 'inspection'] = np.nan
```

```{python}
#| fig-width: 8
#| fig-height: 6
#| warning: false
# Create upset plot
missing_indicators = care_homes[['staffing', 'satisfaction', 'inspection']].isna()
upset_data = from_indicators(missing_indicators)
UpSet(upset_data, subset_size='count', show_counts=True).plot()
plt.suptitle('Missing Data Patterns in Care Home Dataset')
plt.show()
```
:::

**Insight from upset plot**: Staffing and satisfaction are often missing together (both missing in 45 homes), suggesting they share a common cause, likely small home size.

**Step 2: Test missingness mechanism**

::: {.panel-tabset}
## R
```{r}
# Check if missingness related to home size
care_homes$missing_staffing <- is.na(care_homes$staffing)

# Compare mean size by missingness status
aggregate(size ~ missing_staffing, data = care_homes, FUN = function(x) c(mean = mean(x), n = length(x)))

# Statistical test
t.test(size ~ missing_staffing, data = care_homes)
```

## Python
```{python}
from scipy.stats import ttest_ind

# Check if missingness related to home size
care_homes['missing_staffing'] = care_homes['staffing'].isna()

care_homes.groupby('missing_staffing')['size'].agg(['mean', 'count'])

# Statistical test
complete = care_homes[~care_homes['missing_staffing']]['size']
missing = care_homes[care_homes['missing_staffing']]['size']
t, p = ttest_ind(complete, missing)
print(f"t-test: t = {t:.2f}, p = {p:.4f}")
```
:::

**Result**: Homes with missing staffing data are significantly smaller (mean = 32 beds) than complete cases (mean = 58 beds), p < 0.001. This suggests MAR (missingness related to observed size, not to staffing level itself).

### Analysis Phase

**Step 3: Complete case analysis**

::: {.panel-tabset}
## R
```{r}
# Complete case analysis
model_complete <- lm(inspection ~ staffing + size, 
                     data = care_homes)
summary(model_complete)

# How many cases excluded?
nrow(care_homes) - nrow(model_complete$model)
```

## Python
```{python}
from sklearn.linear_model import LinearRegression

# Complete case analysis
complete_data = care_homes.dropna(subset=['inspection', 'staffing', 'size'])
X = complete_data[['staffing', 'size']].values
y = complete_data['inspection'].values
model_complete = LinearRegression().fit(X, y)

print(f"Staffing coefficient: {model_complete.coef_[0]:.3f}")
print(f"Cases used: {len(complete_data)} of {len(care_homes)}")
print(f"Cases excluded: {len(care_homes) - len(complete_data)}")
```
:::

**Result**: N=370 homes (82% of sample). Staffing coefficient = 1.2 (p < 0.01).

**Step 4: Sensitivity analysis with mean imputation**

::: {.panel-tabset}
## R
```{r}
# Mean imputation
care_homes_imputed <- care_homes
care_homes_imputed$staffing[is.na(care_homes_imputed$staffing)] <- 
  mean(care_homes$staffing, na.rm = TRUE)
care_homes_imputed$inspection[is.na(care_homes_imputed$inspection)] <- 
  mean(care_homes$inspection, na.rm = TRUE)

model_imputed <- lm(inspection ~ staffing + size, 
                    data = care_homes_imputed)
summary(model_imputed)

# Compare estimates
data.frame(
  approach = c("Complete case", "Mean imputation"),
  staffing_coef = c(coef(model_complete)["staffing"], 
                    coef(model_imputed)["staffing"]),
  n = c(nrow(model_complete$model), nrow(care_homes_imputed))
)
```

## Python
```{python}
# Mean imputation
care_homes_imputed = care_homes.copy()
care_homes_imputed['staffing'].fillna(care_homes['staffing'].mean(), inplace=True)
care_homes_imputed['inspection'].fillna(care_homes['inspection'].mean(), inplace=True)

X_imputed = care_homes_imputed[['staffing', 'size']].values
y_imputed = care_homes_imputed['inspection'].values
model_imputed = LinearRegression().fit(X_imputed, y_imputed)

# Compare estimates
print("Complete case - Staffing coef:", f"{model_complete.coef_[0]:.3f}", f"(N={len(complete_data)})")
print("Mean imputation - Staffing coef:", f"{model_imputed.coef_[0]:.3f}", f"(N={len(care_homes_imputed)})")
print("Difference:", f"{abs(model_complete.coef_[0] - model_imputed.coef_[0]):.3f}")
```
:::

**Result**: Mean imputation gives staffing coefficient = 1.15 (vs 1.20 for complete case). Difference is small (0.05), suggesting findings are robust.

### Conclusion and Documentation

**Decision**: Use complete case analysis as primary approach, report sensitivity analysis.

**Documentation**:
> "Missing data: staffing (12%), satisfaction (18%), inspection (8%). Upset plot revealed staffing and satisfaction often missing together. Investigation showed missingness related to home size (smaller homes more likely to have missing data, t = -8.3, p < 0.001) but not to quality metrics, suggesting MAR. Complete case analysis used (N=370, 82% of sample). Sensitivity analysis with mean imputation produced similar results (staffing coefficient: 1.20 vs 1.15, difference = 0.05), suggesting findings robust to missing data handling. Limitation: If small homes with poor staffing were systematically less likely to report (MNAR), effect may be underestimated."

**Key lesson**: Investigation showed why data was missing (small homes), which informed our approach and limitations statement.

::: {.callout-note icon=false}
## Rethinking: CQC's Experience with Missing Data

CQC's experience with missing data varies by data source:

**Local Authority assessments**: Historically high missingness in some metrics. Guild developed MI approaches for key indicators, but found that:
- Imputation models were complex and hard to explain
- Stakeholders questioned imputed values
- Complete case analysis with clear caveats often more defensible

**ICS assessments**: Lower missingness, but concentrated in specific areas. Sensitivity analyses (best-case, worst-case) often more useful than MI for understanding impact.

**Provider-level indicators**: Usually low missingness due to mandatory reporting. When present, often MNAR (poor performers don't report), making advanced methods inappropriate.

**Key lesson**: Advanced methods are powerful but complex. Use them when:
1. Missingness is substantial and MAR
2. Simple approaches would create unacceptable bias
3. You can explain and defend the approach
4. Stakeholders value technical rigor over simplicity

Don't use advanced methods just because they exist. Transparency and defensibility matter more than sophistication.
:::

::: {.callout-warning icon=false}
## Overthinking: Multiple Imputation Technical Details

**Rubin's Rules for pooling**:

For M imputations, the pooled estimate is:
$$\bar{Q} = \frac{1}{M}\sum_{i=1}^M \hat{Q}_i$$

Total variance combines within-imputation variance ($\bar{U}$) and between-imputation variance ($B$):
$$T = \bar{U} + \left(1 + \frac{1}{M}\right)B$$

where:
- $\bar{U} = \frac{1}{M}\sum_{i=1}^M U_i$ (average within-imputation variance)
- $B = \frac{1}{M-1}\sum_{i=1}^M (\hat{Q}_i - \bar{Q})^2$ (between-imputation variance)

**Fraction of Missing Information (FMI)**:
$$\lambda = \frac{B + B/M}{T}$$

FMI indicates how much information is lost due to missingness. Higher FMI means more uncertainty.

**How many imputations (M)?**

Traditional guidance: M = 5-10 sufficient
Modern guidance: M = 20+ for better stability
Rule of thumb: M â‰¥ percentage of cases with missing data

**Imputation methods**:

- **PMM (Predictive Mean Matching)**: Imputes from observed values, preserves distributions
- **Bayesian linear regression**: For continuous variables
- **Logistic regression**: For binary variables
- **Polytomous regression**: For categorical variables
- **Random forest**: For complex non-linear relationships

**Congeniality**: Imputation model should be at least as general as analysis model. Include:
- All analysis model variables
- Auxiliary variables that predict missingness
- Interactions if analysis model has them

**Convergence**: Run multiple chains, check trace plots. If not converged, increase iterations.

**Sensitivity analysis**: Vary imputation model (different predictors, methods) to check robustness.
:::

::: {.callout-warning icon=false}
## Overthinking: When MAR Assumption Fails (MNAR)

If missingness is related to the unobserved values (MNAR), standard methods produce biased results. Options:

**Pattern-mixture models**: Model different patterns of missingness separately
- Requires strong assumptions about missing data mechanism
- Complex to implement and interpret

**Selection models**: Explicitly model missingness mechanism
- Requires specifying how missingness depends on unobserved values
- Identification often relies on untestable assumptions

**Sensitivity analysis**: Most practical approach
- Assume different values for missing data
- Best-case: Missing values are favorable
- Worst-case: Missing values are unfavorable
- Tipping point: How bad would missing values need to be to change conclusions?

**Example**:
> "If all providers with missing data had performance in the bottom 10%, our conclusion would change. However, this seems implausible given observed characteristics of providers with missing data."

**CQC practice**: For suspected MNAR, report missingness as a finding and use sensitivity analysis rather than attempting to impute. Be transparent about limitations.

**Key principle**: No statistical method can fully overcome MNAR without strong, untestable assumptions. Prevention (better data collection) is the only real solution.
:::

## Related Approaches

**For foundational concepts**:
- **Missing data mechanisms** â†’ @sec-missing-data-foundations (MCAR, MAR, MNAR definitions)
- **Data quality checks** â†’ @sec-qa-principles (detecting and documenting issues)
- **Uncertainty and variation** â†’ @sec-variation-uncertainty (understanding what we don't know)

**For different analytical contexts**:
- **Small amounts of missingness** (<5%) â†’ Complete case analysis usually fine
- **Moderate missingness** (5-20%) â†’ Complete case + sensitivity analysis
- **Substantial missingness** (>20%) â†’ Consult Guild before proceeding
- **MNAR suspected** â†’ Escalate immediately, may not be solvable

**For advanced methods** (when simple approaches insufficient):
- **Multiple Imputation** â†’ See Overthinking box, or consult Guild
- **Maximum Likelihood** â†’ Guild consultation for implementation
- **Inverse Probability Weighting** â†’ Guild consultation for specialized cases

**For related data issues**:
- **Outliers and extreme values** â†’ @sec-unusual-observations
- **Measurement error** â†’ Guild consultation
- **Data linkage problems** â†’ Guild consultation

## Documenting Your Analysis

For QA purposes, record:

**Investigation**:
- Amount of missingness (overall and by variable)
- Upset plot or missingness pattern visualization
- Tests of missingness mechanism (which variables predict missingness?)
- Substantive reasoning about why data is missing
- Conclusion about mechanism (MCAR, MAR, or MNAR)

**Approach**:
- Method chosen (complete case, simple imputation, sensitivity analysis)
- Justification for choice
- How many cases excluded (if complete case)
- What values used (if imputation)

**Results**:
- Primary analysis results
- Sensitivity analysis results
- Comparison showing robustness (or lack thereof)
- Interpretation accounting for missing data

**Limitations**:
- What assumptions you're making
- What could go wrong if assumptions fail
- How this affects interpretation
- What you can't conclude due to missingness

**Evidence to retain**:
- Upset plot or missingness visualization
- Statistical tests of missingness patterns
- Complete case and sensitivity analysis outputs
- Decision log explaining choices

See @sec-qa-principles for full QA documentation requirements and templates.

## Common Pitfalls

**Pitfall 1: Skipping the investigation**

The biggest mistake is jumping straight to "how do I handle this?" without asking "why is this missing?" Always investigate the missingness pattern first. Use upset plots and statistical tests to understand what's happening.

**Pitfall 2: Assuming MAR without evidence**

Don't assume data is Missing At Random just because it's convenient. If poor performers systematically don't report (MNAR), no amount of statistical sophistication will fix the bias. Be honest about what you can and can't know.

**Pitfall 3: Over-complicating the solution**

Most CQC analyses can use complete case analysis with clear documentation. Don't use complex imputation methods just because they exist. Simple, transparent approaches are usually more defensible.

**Pitfall 4: Ignoring the excluded cases**

If you use complete case analysis, check whether excluded cases differ systematically from included cases. If they do, this is a limitation you must acknowledge.

**Pitfall 5: Not testing sensitivity**

Always check if your conclusions change under different assumptions. If results are very sensitive to how you handle missing data, that's important information for stakeholders.

**Pitfall 6: Poor documentation**

Missing data handling must be transparent. Document: how much is missing, why you think it's missing, what you did about it, and what limitations remain. Stakeholders will ask, be ready.

## When to Escalate to the Guild

Escalate to the Quantitative Guild when:

- **Substantial missing data**: >20% missing and analysis is high-stakes
- **MNAR suspected**: Missingness likely related to unobserved values
- **Complex data structures**: Hierarchical data, longitudinal data, or survival data with missingness
- **Multiple imputation needed**: You need MI but haven't implemented it before
- **Sensitivity analysis required**: Stakeholders want to see how results change under different assumptions
- **Pattern mixture models**: Need to model different missingness patterns explicitly
- **Selection models**: Need to model the missingness mechanism directly
- **Weighting approaches**: Inverse probability weighting or other reweighting methods

## Key Takeaways

::: {.callout-important icon=false}
## Essential Points

1. **Investigation comes first**: always ask "why is this data missing?" before deciding how to handle it
2. **Use upset plots**: they reveal patterns you'd miss with simple percentages
3. **Understand the mechanism**â€”MCAR, MAR, and MNAR have different implications for bias
4. **Start simple**: complete case analysis with clear documentation is often sufficient
5. **Test sensitivity**: check if conclusions change under different assumptions
6. **Document thoroughly**: amount, pattern, mechanism, approach, limitations
7. **Be honest about MNAR**: if poor performers don't report, no statistical method fixes this
8. **Prevention beats cure**: better data collection is the real solution

Missing data handling is about managing uncertainty transparently, not eliminating it. When in doubt, consult the Guild.
:::

## Further Reading

**Internal CQC Resources**:

- **QA Framework**: Documentation standards for analyses with missing data
- **Guild Terms of Reference**: When and how to escalate for specialist support

**External Guidance**:

- **Little RJA, Rubin DB** (2019). *Statistical Analysis with Missing Data* (3rd ed.). Wiley. (The definitive textbook, technical but comprehensive)
- **Van Buuren S** (2018). *Flexible Imputation of Missing Data* (2nd ed.). CRC Press. (Practical guide to multiple imputation)
- **Sterne JAC et al.** (2009). "Multiple imputation for missing data in epidemiological and clinical research." *BMJ*, 338:b2393. (Accessible introduction)
- **AQuA Book** (Chapter on Data Quality): Handling missing data in government analysis

**Online Resources**:

- Understanding MCAR, MAR, MNAR: https://stefvanbuuren.name/fimd/sec-MCAR.html
- Multiple imputation in R (mice package): https://www.gerkovink.com/miceVignettes/
- Multiple imputation in Python (sklearn): https://scikit-learn.org/stable/modules/impute.html
- When NOT to impute: https://statisticalhorizons.com/when-not-to-impute
