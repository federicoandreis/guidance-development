{"title":"Comparing Provider Performance: Z-scoring","markdown":{"yaml":{"title":"Comparing Provider Performance: Z-scoring"},"headingText":"| include: false","containsRefs":false,"markdown":"\n\n```{r}\n# Setup Python environment\nlibrary(reticulate)\n# Use system Python (where pandas is installed)\nuse_python(\"C:/Users/fede/anaconda3/python.exe\", required = TRUE)\n```\n\n::: {.callout-tip icon=false}\n## Problem This Method Solves\n\nYou need to compare provider performance against a national benchmark and identify providers whose performance is statistically different from the average. For example:\n\n- Which GP practices have unusually high or low cancer referral rates?\n- Which care homes have incident rates significantly different from the national average?\n- Which hospitals have waiting times that warrant investigation?\n\nZ-scoring standardizes performance across providers, accounting for expected variation, so you can identify genuinely concerning differences.\n:::\n\n## What Z-scoring Does {#sec-z-scoring}\n\nZ-scoring measures how many standard deviations a provider's observed value is from the expected (mean) value. It answers: \"Is this provider's performance unusual enough that it's unlikely to be due to chance?\"\n\n**Key advantages**:\n- Standardizes different indicators to the same scale\n- Accounts for natural variation in the data\n- Provides consistent banding across all indicators\n- Identifies statistical outliers systematically\n\n**What it does NOT do**:\n- Prove causation (a high z-score shows difference, not why)\n- Account for case-mix or risk adjustment (unless you build that in first)\n- Replace professional judgment about whether to investigate\n\n## Before You Start\n\n::: {.callout-note icon=false}\n## Note on Code Examples\nThe worked examples in this chapter use pseudo-randomly generated data to illustrate the methods. To ensure consistent results between R and Python, the Python code uses the same dataset generated by R (via `r.data_name`). The commented-out Python code shows how you would generate equivalent data independently if needed.\n:::\n\nCheck these requirements before applying z-scoring:\n\n### Data Requirements\n- [ ] **Data type**: Continuous data (rates, percentages, ratios) or proportions from counts\n- [ ] **Sample size**: At least 30 providers for reliable results\n- [ ] **Comparability**: Providers are reasonably comparable (same indicator definition)\n- [ ] **Quality checks**: Completed pre-analysis QA (see @sec-qa-principles)\n\n### When Z-scoring May Not Be Appropriate\n- **Very small denominators** (e.g., <10 events): Standard z-scoring can be unreliable, see Overthinking box\n- **Highly skewed data**: May need Working z-scores instead, see Overthinking box\n- **Different populations**: If providers serve fundamentally different populations, adjust first\n- **Categorical outcomes**: Use different methods for categorical data\n\n### What You Need\n- Provider-level data with numerator and denominator (for proportions) or observed values (for continuous measures)\n- National or comparison group mean and standard deviation\n- Statistical software (R recommended, examples provided)\n\n::: {.callout-note icon=false}\n## ðŸ“– Complete Worked Examples\n\n**Example 1: GP Access Analysis** demonstrates z-scoring with realistic CQC data:\n\n- Handling survey data with missing values and varying response rates\n- Checking z-scoring assumptions (normality, independence, sample sizes)\n- Dealing with outliers and small denominators\n- Calculating z-scores and assigning performance bands\n- Conducting sensitivity analysis and interpreting results\n\n**Time**: 2-3 hours | **Complexity**: Medium\n\n[View Example 1: GP Access Analysis â†’](../examples/example-01-complete.qmd)\n\n**Example 3: Mental Health Wait Times** demonstrates z-scoring with skewed data:\n\n- Handling non-normal distributions\n- Deciding between standard and Working z-scores\n- Applying transformations appropriately\n- Interpreting results when assumptions are violated\n\n**Time**: 2-3 hours | **Complexity**: Medium-High\n\n[View Example 3: Mental Health Wait Times â†’](../examples/example-03-complete.qmd)\n:::\n\n## Step-by-Step Guide\n\n### Step 1: Calculate the Indicator Value\n\nFor each provider, calculate the indicator you're analyzing.\n\n**For proportions** (e.g., percentage of patients seen within target):\n$$\\text{Proportion} = \\frac{\\text{Number meeting target}}{\\text{Total number}}$$\n\n**For rates** (e.g., incidents per 1,000 bed days):\n$$\\text{Rate} = \\frac{\\text{Number of events}}{\\text{Denominator}} \\times 1000$$\n\n**For continuous measures** (e.g., mean waiting time):\nUse the observed value directly.\n\n### Step 2: Calculate National Mean and Standard Deviation\n\nCalculate the mean (Î¼) and standard deviation (Ïƒ) across all providers:\n\n$$\\mu = \\frac{\\sum x_i}{n}$$\n\n$$\\sigma = \\sqrt{\\frac{\\sum (x_i - \\mu)^2}{n-1}}$$\n\nwhere $x_i$ is each provider's indicator value and $n$ is the number of providers.\n\n### Step 3: Calculate Z-scores\n\nFor each provider, calculate how many standard deviations they are from the mean:\n\n$$Z = \\frac{x - \\mu}{\\sigma}$$\n\nwhere:\n- $x$ = provider's observed value\n- $\\mu$ = national mean\n- $\\sigma$ = national standard deviation\n\n### Step 4: Assign Bands\n\nCQC typically uses these z-score bands:\n\n| Z-score Range | Interpretation | Expected % of Providers |\n|---------------|----------------|------------------------|\n| z < -3 | Significantly below average | 0.1% |\n| -3 â‰¤ z < -2 | Below average | 2.1% |\n| -2 â‰¤ z < -1 | Slightly below average | 13.6% |\n| -1 â‰¤ z < +1 | As expected | 68.2% |\n| +1 â‰¤ z < +2 | Slightly above average | 13.6% |\n| +2 â‰¤ z < +3 | Above average | 2.1% |\n| z â‰¥ +3 | Significantly above average | 0.1% |\n\n![Normal distribution showing z-score bands and expected percentages. The vertical dashed lines mark standard deviations (Ïƒ) from the mean (Î¼). In a normal distribution, 68.2% of values fall within Â±1Ïƒ, 95.4% within Â±2Ïƒ, and 99.7% within Â±3Ïƒ.](../figures/zscore-distribution.png){#fig-zscore-distribution width=100%}\n\n**Direction matters**: For some indicators, high values are concerning (e.g., incident rates). For others, low values are concerning (e.g., percentage meeting targets). Interpret bands accordingly.\n\n### Step 5: Visualize and Interpret\n\nPlot the distribution to check it looks reasonable:\n- Does the distribution look roughly normal?\n- Are the proportions in each band close to expected?\n- Are there any extreme outliers?\n\nA histogram of z-scores with reference lines at Â±1, Â±2, and Â±3 standard deviations helps visualize the distribution and identify any issues.\n\n## Worked Example: GP Cancer Referral Rates\n\n**Scenario**: You're analyzing 2-week-wait cancer referral rates across 150 GP practices. You want to identify practices with unusually high or low referral rates.\n\n**Data structure**:\n\n:::: {.panel-tabset}\n## R\n```{r}\n#| label: zscore-example-data\n# Simulated data for illustration (seed for reproducibility)\nset.seed(42)\ngp_data <- data.frame(\n  practice_id = paste0(\"GP\", 1:150),\n  referrals = rpois(150, lambda = 45),  # Number of referrals\n  list_size = rnorm(150, mean = 8000, sd = 2000)  # Practice size\n)\n\n# Calculate referral rate per 1,000 patients\ngp_data$referral_rate <- (gp_data$referrals / gp_data$list_size) * 1000\n```\n\n## Python\n```{python}\nimport pandas as pd\nimport numpy as np\n\n# Use the same data from R for consistent results\ngp_data = r.gp_data\n\n# Alternative: Generate data in Python (commented out for consistency)\n# np.random.seed(42)\n# gp_data = pd.DataFrame({\n#     'practice_id': [f'GP{i}' for i in range(1, 151)],\n#     'referrals': np.random.poisson(45, 150),\n#     'list_size': np.random.normal(8000, 2000, 150)\n# })\n# gp_data['referral_rate'] = (gp_data['referrals'] / gp_data['list_size']) * 1000\n```\n::::\n\n**Step 1: Calculate indicator** (already done above)\n\n**Step 2: Calculate national statistics**:\n\n:::: {.panel-tabset}\n## R\n```{r}\n#| label: zscore-national-stats\nnational_mean <- mean(gp_data$referral_rate)\nnational_sd <- sd(gp_data$referral_rate)\n\ncat(\"National mean:\", round(national_mean, 2), \"per 1,000\\n\")\ncat(\"National SD:\", round(national_sd, 2), \"\\n\")\n```\n\n## Python\n```{python}\nnational_mean = gp_data['referral_rate'].mean()\nnational_sd = gp_data['referral_rate'].std()\n\nprint(f\"National mean: {national_mean:.2f} per 1,000\")\nprint(f\"National SD: {national_sd:.2f}\")\n```\n::::\n\n**Step 3: Calculate z-scores**:\n\n:::: {.panel-tabset}\n## R\n```{r}\n#| label: zscore-calculate\ngp_data$z_score <- (gp_data$referral_rate - national_mean) / national_sd\n```\n\n## Python\n```{python}\ngp_data['z_score'] = (gp_data['referral_rate'] - national_mean) / national_sd\n```\n::::\n\n**Step 4: Assign bands**:\n\n:::: {.panel-tabset}\n## R\n```{r}\n#| label: zscore-bands\ngp_data$band <- cut(gp_data$z_score,\n                    breaks = c(-Inf, -3, -2, -1, 1, 2, 3, Inf),\n                    labels = c(\"Sig. below\", \"Below\", \"Slightly below\",\n                              \"As expected\", \"Slightly above\", \"Above\", \"Sig. above\"))\n\n# Check distribution\ntable(gp_data$band)\n```\n\n## Python\n```{python}\ngp_data['band'] = pd.cut(gp_data['z_score'],\n                         bins=[-np.inf, -3, -2, -1, 1, 2, 3, np.inf],\n                         labels=['Sig. below', 'Below', 'Slightly below',\n                                'As expected', 'Slightly above', 'Above', 'Sig. above'])\n\n# Check distribution\ngp_data['band'].value_counts().sort_index()\n```\n::::\n\n**Step 5: Visualize the distribution**:\n\n:::: {.panel-tabset}\n## R\n```{r}\n#| label: zscore-viz-worked\n#| fig-width: 8\n#| fig-height: 5\nhist(gp_data$z_score, breaks = 30, \n     main = \"Distribution of Z-scores: GP Cancer Referral Rates\",\n     xlab = \"Z-score\", col = \"lightblue\")\nabline(v = c(-3, -2, -1, 1, 2, 3), col = \"red\", lty = 2)\n```\n\n## Python\n```{python}\nimport matplotlib.pyplot as plt\n\nplt.hist(gp_data['z_score'], bins=30, color='lightblue', edgecolor='white')\nplt.axvline(x=-3, color='red', linestyle='--')\nplt.axvline(x=-2, color='red', linestyle='--')\nplt.axvline(x=-1, color='red', linestyle='--')\nplt.axvline(x=1, color='red', linestyle='--')\nplt.axvline(x=2, color='red', linestyle='--')\nplt.axvline(x=3, color='red', linestyle='--')\nplt.title('Distribution of Z-scores: GP Cancer Referral Rates')\nplt.xlabel('Z-score')\nplt.ylabel('Frequency')\nplt.show()\n```\n::::\n\n**Step 6: Identify practices for investigation**:\n\n:::: {.panel-tabset}\n## R\n```{r}\n#| label: zscore-identify-practices\n# Practices significantly above average (potential over-referral)\nhigh_referrers <- gp_data[gp_data$z_score >= 2, \n                          c(\"practice_id\", \"referral_rate\", \"z_score\", \"band\")]\n\n# Practices significantly below average (potential under-referral)\nlow_referrers <- gp_data[gp_data$z_score <= -2,\n                         c(\"practice_id\", \"referral_rate\", \"z_score\", \"band\")]\n\nprint(high_referrers)\nprint(low_referrers)\n```\n\n## Python\n```{python}\n# Practices significantly above average (potential over-referral)\nhigh_referrers = gp_data[gp_data['z_score'] >= 2][['practice_id', 'referral_rate', 'z_score', 'band']]\n\n# Practices significantly below average (potential under-referral)\nlow_referrers = gp_data[gp_data['z_score'] <= -2][['practice_id', 'referral_rate', 'z_score', 'band']]\n\nprint(high_referrers)\nprint(low_referrers)\n```\n::::\n\n**Step 7: Visualize**:\n\n:::: {.panel-tabset}\n## R\n```{r}\n#| label: zscore-visualize-final\n#| fig-width: 12\n#| fig-height: 5\npar(mfrow = c(1, 2))\n\n# Distribution of z-scores\nhist(gp_data$z_score, breaks = 30, \n     main = \"Distribution of GP Cancer Referral Z-scores\",\n     xlab = \"Z-score\", col = \"lightblue\", border = \"white\")\nabline(v = c(-3, -2, 2, 3), col = \"red\", lty = 2, lwd = 2)\nabline(v = 0, col = \"blue\", lty = 1, lwd = 2)\n\n# Scatter plot: rate vs practice size\nplot(gp_data$list_size, gp_data$referral_rate,\n     xlab = \"Practice Size (List Size)\",\n     ylab = \"Referral Rate per 1,000\",\n     main = \"Cancer Referral Rates by Practice Size\",\n     col = ifelse(abs(gp_data$z_score) >= 2, \"red\", \"grey\"),\n     pch = 19)\nlegend(\"topright\", legend = c(\"As expected\", \"Flagged (|z| >= 2)\"),\n       col = c(\"grey\", \"red\"), pch = 19)\n\npar(mfrow = c(1, 1))\n```\n\n## Python\n```{python}\nimport matplotlib.pyplot as plt\n\n# Distribution of z-scores\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\nax1.hist(gp_data['z_score'], bins=30, color='lightblue', edgecolor='white')\nax1.axvline(x=-3, color='red', linestyle='--', linewidth=2)\nax1.axvline(x=-2, color='red', linestyle='--', linewidth=2)\nax1.axvline(x=2, color='red', linestyle='--', linewidth=2)\nax1.axvline(x=3, color='red', linestyle='--', linewidth=2)\nax1.axvline(x=0, color='blue', linestyle='-', linewidth=2)\nax1.set_title('Distribution of GP Cancer Referral Z-scores')\nax1.set_xlabel('Z-score')\nax1.set_ylabel('Frequency')\n\n# Scatter plot: rate vs practice size\ncolors = ['red' if abs(z) >= 2 else 'grey' for z in gp_data['z_score']]\nax2.scatter(gp_data['list_size'], gp_data['referral_rate'], c=colors, alpha=0.6)\nax2.set_xlabel('Practice Size (List Size)')\nax2.set_ylabel('Referral Rate per 1,000')\nax2.set_title('Cancer Referral Rates by Practice Size')\nax2.legend(['As expected', 'Flagged (|z| >= 2)'], loc='upper right')\n\nplt.tight_layout()\nplt.show()\n```\n::::\n\n## Interpreting Results\n\n### What Z-scores Tell You\n\n**z = 0**: Provider is exactly at the national average\n\n**z = +1**: Provider is 1 standard deviation above average (better or worse depending on indicator direction)\n\n**z = +2**: Provider is 2 standard deviations above averageâ€”only ~2% of providers this high by chance\n\n**z = +3 or higher**: Provider is 3+ standard deviations above averageâ€”only ~0.1% this high by chance, warrants investigation\n\n### Regulatory Interpretation\n\n**|z| < 1 (68% of providers)**: Performance as expected. No action needed.\n\n**1 â‰¤ |z| < 2 (27% of providers)**: Slightly different from average. Monitor but don't necessarily investigate.\n\n**2 â‰¤ |z| < 3 (4% of providers)**: Notably different. Consider for investigation depending on:\n- Clinical significance of the difference\n- Provider history\n- Other indicators\n- Available inspection resources\n\n**|z| â‰¥ 3 (0.2% of providers)**: Highly unusual. Strong candidate for investigation unless there's a clear explanation (e.g., specialist service, data error).\n\n**Critical point**: Statistical significance â‰  clinical significance. A z-score of 2.5 might be statistically unusual but clinically unimportant if the absolute difference is tiny. Always consider both.\n\n## Common Pitfalls\n\n**Pitfall 1: Ignoring direction**\n- High z-scores are concerning for negative indicators (incidents, mortality)\n- Low z-scores are concerning for positive indicators (screening uptake, quality measures)\n- Always check which direction matters\n\n**Pitfall 2: Treating bands as hard thresholds**\n- A provider with z = 1.99 is not fundamentally different from z = 2.01\n- Use bands as guidance, not mechanical rules\n\n**Pitfall 3: Ignoring small denominators**\n- Providers with few patients/events have more uncertain estimates\n- Their z-scores are less reliable, see Overthinking box\n\n**Pitfall 4: Multiple testing**\n- If you run z-scoring on 100 indicators, you'll flag some providers by chance\n- Consider the full picture, not just one indicator\n\n**Pitfall 5: Assuming causation**\n- Z-scoring identifies difference, not cause\n- Investigation determines whether it's a quality issue, case-mix, data error, or chance\n\n## Documenting Your Analysis\n\nFor QA purposes, record:\n\n**Data**:\n- Indicator definition\n- Time period\n- Number of providers (before/after exclusions)\n- Exclusion criteria applied\n\n**Calculations**:\n- National mean and SD\n- Z-score formula used\n- Banding thresholds applied\n\n**Results**:\n- Number/percentage of providers in each band\n- Whether distribution looks normal\n- Providers flagged for investigation\n\n**Decisions**:\n- Why you chose standard vs working z-scores\n- How you handled small denominators\n- What follow-up actions you recommend\n\n::: {.callout-note icon=false}\n## Rethinking: Why Z-scoring Succeeded at CQC\n\nCQC has used z-scoring extensively for provider comparison since the early 2010s. Why has it become a core method?\n\n**Consistency**: The same approach works across diverse indicatorsâ€”incident rates, waiting times, quality measures. This consistency aids interpretation and reduces analyst burden.\n\n**Transparency**: The method is mathematically simple and explainable to non-statisticians. Stakeholders understand \"3 standard deviations from average\" more easily than complex model outputs.\n\n**Scalability**: With hundreds of indicators and thousands of providers, we need automated, systematic approaches. Z-scoring provides this while still allowing professional judgment.\n\n**Fairness perception**: Providers understand being compared to peers. Z-scoring makes this comparison explicit and quantified.\n\n**Regulatory fit**: We need to prioritize inspection resources. Z-scoring provides a defensible, reproducible way to identify outliers for investigation.\n\n**Pattern recognition**: Z-scoring reveals patterns across many providers (significant sameness) rather than focusing on isolated comparisons (significant differences). This system-level perspective helps identify widespread issues and good practices.\n\n**Limitations acknowledged**: CQC has learned that z-scoring isn't perfectâ€”hence the development of Working z-scores for non-normal data, and ongoing work on risk adjustment. But the core approach remains valuable because it balances statistical rigor with operational feasibility.\n\nThe key lesson: A good-enough method that's consistently applied and well-understood often beats a perfect method that's complex and inconsistently used.\n:::\n\n::: {.callout-note icon=false}\n## Rethinking: Z-scores vs Funnel Plots\n\nFunnel plots and z-scores are closely related: both identify providers outside expected variation. So when should you use each?\n\n**Z-scores**:\n- Numerical output: easy to sort, filter, automate\n- Clear banding for decision-making\n- Works well in tables and databases\n- Less visual, more analytical\n\n**Funnel plots**:\n- Visual output: great for presentations and reports\n- Shows relationship between sample size and uncertainty\n- Makes small-denominator issues obvious\n- Less useful for automated flagging\n\n**CQC practice**: Use z-scores for analysis and provider flagging. Use funnel plots for communicating results to stakeholders. They're complementary, not competing.\n\n**Technical note**: Funnel plots typically show 95% and 99.8% confidence limits, which correspond roughly to z-scores of Â±2 and Â±3. A provider outside the funnel is a provider with |z| > 2 or 3.\n:::\n\n::: {.callout-warning icon=false}\n## Overthinking: When Standard Z-scoring Fails\n\nStandard z-scoring assumes your data is approximately normally distributed with constant variance. When these assumptions fail, results can be misleading.\n\n**Problem 1: Small denominators**\n\nWith small denominators (e.g., a care home with 15 beds), percentages can only take certain values (0%, 6.7%, 13.3%, etc.). This creates:\n- Discrete jumps in possible z-scores\n- Overconfidence in flagging small providers\n- Unfair comparison with large providers\n\n**Solution**: Working z-scores use the exact distribution (Binomial for proportions, Poisson for counts) rather than assuming normality. This properly accounts for small-denominator uncertainty.\n\n**Rule of thumb**: If denominators <30 or expected events <10, consider Working z-scores.\n\n**Problem 2: Overdispersion**\n\nSometimes data has more variation than expected under standard assumptions. For example, incident counts might vary more than a Poisson distribution predicts due to clustering or systematic differences.\n\n**Detection**: If >5% of providers fall outside Â±2 SD when you expect ~5%, you may have overdispersion.\n\n**Solution**: Working z-scores can model overdispersion. Alternatively, investigate why variation is higher than expectedâ€”it might indicate real systematic differences.\n\n**Problem 3: Skewed data**\n\nIf your indicator is heavily skewed (e.g., length of stay with long right tail), the normal distribution assumption fails. Standard z-scores will:\n- Flag too many providers in the tail direction\n- Miss providers in the other direction\n- Give misleading band percentages\n\n**Detection**: Plot histogram. If mean >> median or you see long tail, you have skewness.\n\n**Solutions**:\n1. Transform data (e.g., log transformation) before z-scoring\n2. Use Working z-scores with appropriate distribution\n3. Use median-based methods instead\n\n**When to escalate**: If you're unsure whether your data meets assumptions, or if you need Working z-scores, consult the Guild. Working z-scores require model-based approaches (Beta distribution for proportions, Poisson/Negative Binomial for counts) that need specialist implementation.\n:::\n\n::: {.callout-warning icon=false}\n## Overthinking: Working Z-scores Technical Details\n\n**Working z-scores** extend standard z-scoring to non-normal data by using the exact distribution of the indicator.\n\n**For proportions** (e.g., percentage of patients meeting target):\n\nApply arcsine transformation to stabilize variance:\n\n$$Y_i = \\arcsin(\\sqrt{r_i/n_i})$$\n$$T = \\arcsin(\\sqrt{t})$$\n\nwhere $r_i$ = numerator, $n_i$ = denominator, $t$ = target proportion.\n\nProvider-specific standard deviation:\n\n$$s_i = \\frac{1}{2\\sqrt{n_i}}$$\n\nWorking z-score:\n\n$$Z_i = \\frac{Y_i - T}{s_i} = \\frac{\\arcsin(\\sqrt{r_i/n_i}) - \\arcsin(\\sqrt{t})}{1/(2\\sqrt{n_i})}$$\n\n**For count ratios** (e.g., standardized mortality ratios):\n\nApply logarithmic transformation (adding 0.5 to handle zeros):\n\n$$Y_i = \\log_e\\left(\\frac{O_{i,1} + 0.5}{O_{i,2} + 0.5}\\right)$$\n$$T = \\log_e(t)$$\n\nProvider-specific standard deviation:\n\n$$s_i = \\sqrt{\\frac{1}{O_{i,1} + 0.5} + \\frac{1}{O_{i,2} + 0.5}}$$\n\nWorking z-score:\n\n$$Z_i = \\frac{Y_i - T}{s_i}$$\n\n**Overdispersion adjustment**: After calculating z-scores, check if variance exceeds 1.0. If overdispersed, apply correction factor (see CQC Working z-scores guidance).\n\n**Implementation**: Working z-scores require:\n- Fitting distributional models to data\n- Estimating parameters (Beta shape parameters, Poisson/NB dispersion)\n- Calculating z-scores from model-based expectations\n\n**CQC practice**: The Guild has R scripts for Working z-scores. Don't implement from scratchâ€”use established code that's been validated.\n\n**When to use**:\n- Denominators <30 or expected events <10\n- Clear evidence of overdispersion\n- Proportions near 0% or 100% (where normal approximation fails)\n- Stakeholder concerns about fairness to small providers\n\n**When NOT to use**:\n- Data meets standard z-score assumptions (simpler is better)\n- Sample sizes large enough that normal approximation works well\n- You don't have specialist support to implement correctly\n\n**Key principle**: Working z-scores are more technically correct but more complex. Use them when standard z-scores would be misleading, not as default.\n:::\n\n## Related Approaches\n\n**For different data challenges**:\n- **Heavily skewed data** â†’ See Overthinking box on Working z-scores above, or consult Guild for transformation approaches\n- **Proportions near 0% or 100%** â†’ Wilson score intervals (Working z-scores)\n- **Very small denominators** â†’ Working z-scores or funnel plots\n- **Monitoring over time** â†’ Statistical Process Control (@sec-spc-basics)\n- **Only two groups to compare** â†’ Consider t-tests (@sec-t-tests) if no national benchmark\n\n**For foundational concepts**:\n- **Understanding variation** â†’ @sec-variation-uncertainty\n- **Data quality checks** â†’ @sec-qa-principles\n- **Missing data investigation** â†’ @sec-missing-data-foundations\n\n**For complete workflows**:\n- **GP Access Analysis** â†’ Example 1 (worked example with z-scoring)\n- **Mental Health Wait Times** â†’ Example 3 (z-scoring with skewed data)\n\n## When to Escalate to the Guild\n\nEscalate to the Quantitative Guild when:\n\n- **Distribution extremely skewed** despite transformations or log-scaling\n- **Negative variance estimates** appear in your calculations\n- **Systematic patterns in residuals** after z-scoring (suggests model misspecification)\n- **Multiple testing** across many indicators simultaneously (need correction)\n- **Working z-scores needed** but unsure of implementation or interpretation\n- **Small numbers** combined with high stakes decisions\n- **Stakeholder challenge** to methodology requiring specialist defense\n- **New indicator development** where z-scoring appropriateness is unclear\n\n## Key Takeaways\n\n::: {.callout-important icon=false}\n## Essential Points\n\n1. **Z-scoring standardizes comparison** across providers and indicators\n2. **Interpret bands as guidance**, not mechanical thresholds\n3. **Statistical significance â‰  clinical significance**: consider both\n4. **Check assumptions**: Normal distribution, adequate sample sizes\n5. **Small denominators need special handling**: consider Working z-scores\n6. **Document decisions** for QA and defensibility\n7. **Z-scores identify difference, not cause**: investigation determines why\n\nZ-scoring is CQC's workhorse method for provider comparison. Master it, understand its limitations, and know when to escalate to specialist methods.\n:::\n\n## Further Reading\n\n**Internal CQC Resources**:\n\n- **Z-scoring Guidance Document** (Internal): Detailed technical specifications for Standard and Working z-scores\n- **QA Framework**: Documentation standards for z-scoring analyses\n- **Guild Terms of Reference**: When and how to escalate for specialist support\n\n**External Guidance**:\n\n- **Spiegelhalter DJ** (2005). \"Funnel plots for comparing institutional performance.\" *Statistics in Medicine*, 24(8), 1185-1202. (Foundational paper on funnel plots and provider comparison)\n- **AQuA Book** (Chapter on Uncertainty): Communicating uncertainty in performance metrics\n- **NHS Digital Statistical Process Control guidance**: Alternative approaches to provider comparison\n\n**Online Resources**:\n\n- Understanding z-scores: https://www.statology.org/z-score/\n- Funnel plots explained: https://www.phc.ox.ac.uk/research/resources/funnel-plots\n- Normal distribution properties: https://www.statology.org/normal-distribution/\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../custom.css"],"toc":true,"toc-depth":3,"number-sections":true,"output-file":"11-z-scoring.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../references.bib"],"theme":"cosmo","number-depth":2,"title":"Comparing Provider Performance: Z-scoring"},"extensions":{"book":{"multiFile":true}}}}}