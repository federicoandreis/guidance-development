{"title":"Outlier Detection and Treatment","markdown":{"yaml":{"title":"Outlier Detection and Treatment"},"headingText":"| include: false","containsRefs":false,"markdown":"\n\n```{r}\nlibrary(reticulate)\nuse_python(\"C:/Users/fede/anaconda3/python.exe\", required = TRUE)\n```\n\n::: {.callout-tip icon=false}\n## Problem This Method Solves\n\nYou need to systematically identify and investigate data points that stand out from the rest of your dataset. For example:\n\n- A care home reporting staff-to-resident ratios that seem implausibly high or low\n- Hospital waiting times that are extreme compared to similar providers\n- Incident rates that warrant immediate investigation\n- Data quality issues that need identification before formal analysis\n\nOutlier detection provides computational tools to flag unusual observations for investigation, going beyond visual inspection to provide systematic, reproducible criteria.\n:::\n\n## What Outlier Detection Does {#sec-outlier-methods}\n\nOutlier detection methods apply statistical rules to identify observations that are unusually far from the bulk of the data. They answer: \"Which values are extreme enough to warrant investigation?\"\n\n**Key advantages**:\n- Provides systematic, reproducible criteria for flagging unusual values\n- Scales to large datasets where visual inspection is impractical\n- Offers multiple methods for different data characteristics\n- Helps prioritize investigation efforts\n\n**What it does NOT do**:\n- Tell you whether an outlier is an error or genuine extreme value (requires investigation)\n- Provide a single \"correct\" answer (different methods flag different points)\n- Replace the conceptual understanding needed before analysis (see @sec-unusual-observations)\n- Automatically fix data problems (treatment requires judgment)\n\n## Before You Start\n\n::: {.callout-note icon=false}\n## Note on Code Examples\nThe worked examples in this chapter use pseudo-randomly generated data to illustrate the methods. To ensure consistent results between R and Python, the Python code uses the same dataset generated by R (via `r.data_name`). The commented-out Python code shows how you would generate equivalent data independently if needed.\n:::\n\nCheck these requirements before applying outlier detection:\n\n### Prerequisites\n- [ ] **Conceptual foundation**: Read @sec-unusual-observations first to understand what \"unusual\" means\n- [ ] **Data type**: Continuous or count data (categorical data needs different approaches)\n- [ ] **Context understanding**: Know what values are plausible for your indicator\n- [ ] **Quality checks**: Completed basic data quality checks (see @sec-qa-principles)\n\n### When Outlier Detection May Not Be Appropriate\n- **Bimodal distributions**: Two distinct groups may not have \"outliers\" but different populations\n- **Very small samples** (<20): Most methods unreliable\n- **Expected extreme values**: If you know some values should be extreme (e.g., specialist units)\n- **Time-series data**: Use SPC methods instead (@sec-spc-basics)\n\n### What You Need\n- Provider-level or observation-level data\n- Understanding of the indicator and what constitutes a plausible range\n- Statistical software (R or Python)\n- Documentation of why you're detecting outliers (investigation? data quality?)\n\n::: {.callout-note icon=false}\n## ðŸ“– Complete Worked Example\n\n**Example 10: Care Home Staffing Investigation** demonstrates outlier detection in practice:\n\n- Identifying extreme staffing ratios using multiple methods\n- Investigating whether outliers are errors or genuine extremes\n- Handling small provider variation\n- Documenting decisions for regulatory defensibility\n- Communicating findings to inspectors\n\n**Time**: 2-3 hours | **Complexity**: Medium\n\n[View Example 10: Care Home Staffing Investigation â†’](../examples/example-10-outliers.qmd)\n:::\n\n## Step-by-Step Guide\n\n### Step 1: Visualize Your Data First\n\nAlways start with visualization to understand the distribution and identify potential outliers visually.\n\n**Create distribution plots**:\n- Histogram with density curve to see overall shape\n- Box plot to identify outliers using IQR method automatically\n\n**What to look for**:\n- Is the distribution symmetric or skewed?\n- How many potential outliers are visible?\n- Are outliers in one tail or both?\n- Are outliers isolated points or small clusters?\n\n### Step 2: Apply Multiple Detection Methods\n\nUse 2-3 detection methods and look for consensus. Agreement across methods strengthens the case for investigation.\n\n**Method 1: Z-score**  \nFlags observations more than k standard deviations from the mean (typically k = 2.5 or 3).\n\n- **Formula**: $Z = \\frac{x - \\mu}{\\sigma}$\n- **When to use**: Data approximately normal, large sample (n > 30)\n- **Limitation**: Sensitive to extreme values, assumes normality\n\n**Method 2: Modified Z-score**  \nUses median and median absolute deviation (MAD) instead of mean/SD. More robust to outliers.\n\n- **Formula**: $Z_{mod} = \\frac{0.6745(x - median)}{MAD}$\n- **Threshold**: Typically |Z_mod| > 3.5\n- **When to use**: Data skewed or contains outliers that distort mean/SD\n- **Advantage**: Robust to outliers, works with non-normal data\n\n**Method 3: IQR Method (Tukey's Fences)**  \nUses interquartile range to define fences. The method box plots use.\n\n- **Fences**: $Q1 - 1.5 \\times IQR$ and $Q3 + 1.5 \\times IQR$\n- **When to use**: Non-normal data, want method matching box plots\n- **Note**: Can flag many points in skewed distributions\n\n### Step 3: Compare Method Results\n\nCount how many methods flag each observation:\n- **3 methods agree**: Highest priority for investigation\n- **2 methods agree**: High priority\n- **1 method only**: May be method-specific artifact\n\n### Step 4: Investigate Flagged Observations\n\nDetection is just the first step. Every flagged observation requires investigation.\n\n**For each flagged observation, ask**:\n\n1. **Data quality**: Could this be an error?\n   - Implausible value (negative, impossible)?\n   - Wrong unit or scale?\n   - Missing data coded as extreme value?\n\n2. **Context**: Why might this be genuinely extreme?\n   - Small provider (high variation expected)?\n   - Specialist service (different population)?\n   - Known event (incident, merger, closure)?\n\n3. **Consistency**: Do related values support this?\n   - Other time periods for same provider?\n   - Related indicators?\n   - Inspection reports or narrative data?\n\n### Step 5: Decide on Treatment\n\nBased on investigation, choose appropriate action:\n\n**Option 1: Keep as genuine extreme** â†’ Verified genuine, provides important information  \n**Option 2: Remove from analysis** â†’ Confirmed data error, provider not comparable  \n**Option 3: Adjust or transform** â†’ Systematic bias identified  \n**Option 4: Separate analysis** â†’ Distinct subgroups identified\n\nDocument all decisions in a decision log for QA purposes.\n\n## Worked Example: Care Home Staffing Ratios\n\n### Scenario\n\nYou're analyzing staffing ratios (qualified staff per 10 residents) across 80 care homes. You need to identify extreme values that warrant investigation before conducting your main analysis.\n\n**Data**: Simulated staffing ratios (most homes 2.5-4.5 staff per 10 residents, with some genuine extremes and data errors)\n\n### Step 1: Generate and Visualize Data\n\n::: {.panel-tabset}\n## R\n```{r}\n#| label: outlier-data\n#| echo: true\nset.seed(123)\n\n# Generate 80 care homes with staffing ratios\ncare_homes <- data.frame(\n  home_id = sprintf(\"CH%03d\", 1:80),\n  staff_per_10 = c(\n    rnorm(65, mean = 3.2, sd = 0.7),      # Normal homes\n    rnorm(5, mean = 5.5, sd = 0.3),        # High staffing (specialist units)\n    rnorm(3, mean = 1.5, sd = 0.2),        # Low staffing\n    c(0.02, 0.05, 9.8, 11.2, 0, 8.5, 7.2)  # Data errors and extremes\n  )\n)\n\n# Ensure non-negative\ncare_homes$staff_per_10 <- pmax(care_homes$staff_per_10, 0)\n\n# Display summary\nsummary(care_homes$staff_per_10)\n```\n\n```{r}\n#| label: outlier-viz\n#| echo: true\n#| fig-width: 10\n#| fig-height: 4\n\n# Visualize distribution\npar(mfrow = c(1, 2))\n\n# Histogram\nhist(care_homes$staff_per_10, breaks = 20, \n     main = \"Distribution of Staffing Ratios\",\n     xlab = \"Staff per 10 Residents\",\n     col = \"lightblue\", border = \"white\")\n\n# Box plot\nboxplot(care_homes$staff_per_10, \n        main = \"Box Plot Showing Outliers\",\n        ylab = \"Staff per 10 Residents\",\n        col = \"lightblue\", outcol = \"red\", outpch = 19)\nabline(h = c(2.5, 4.0), lty = 2, col = \"darkgreen\")\ntext(1.3, 2.5, \"Min guideline\", col = \"darkgreen\", cex = 0.8)\ntext(1.3, 4.0, \"Good practice\", col = \"darkgreen\", cex = 0.8)\n```\n\n## Python\n```{python}\n#| label: outlier-data-py\n#| echo: true\n\n# Use R's data for consistency\ncare_homes = r.care_homes.copy()\n\n# Display summary\nprint(care_homes['staff_per_10'].describe())\n```\n\n```{python}\n#| label: outlier-viz-py\n#| echo: true\n#| fig-width: 10\n#| fig-height: 4\n\nimport matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\n# Histogram\nax1.hist(care_homes['staff_per_10'], bins=20, color='lightblue', edgecolor='white')\nax1.set_xlabel('Staff per 10 Residents')\nax1.set_ylabel('Frequency')\nax1.set_title('Distribution of Staffing Ratios')\nax1.grid(alpha=0.3)\n\n# Box plot\nax2.boxplot(care_homes['staff_per_10'], patch_artist=True,\n            boxprops=dict(facecolor='lightblue'),\n            flierprops=dict(marker='o', markerfacecolor='red', markersize=8))\nax2.axhline(2.5, linestyle='--', color='darkgreen', alpha=0.6)\nax2.axhline(4.0, linestyle='--', color='darkgreen', alpha=0.6)\nax2.set_ylabel('Staff per 10 Residents')\nax2.set_title('Box Plot Showing Outliers')\nax2.set_xticks([])\nax2.grid(alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n```\n:::\n\n**Observation**: Right-skewed distribution with several extreme values in both tails. Box plot shows ~10 potential outliers.\n\n### Step 2: Apply Detection Methods\n\n::: {.panel-tabset}\n## R\n```{r}\n#| label: outlier-detect\n#| echo: true\n\n# Z-score method\ncare_homes$z_score <- scale(care_homes$staff_per_10)[,1]\ncare_homes$outlier_z <- abs(care_homes$z_score) > 2.5\n\n# Modified Z-score method\nmedian_val <- median(care_homes$staff_per_10)\nmad_val <- mad(care_homes$staff_per_10)\ncare_homes$modified_z <- 0.6745 * (care_homes$staff_per_10 - median_val) / mad_val\ncare_homes$outlier_modz <- abs(care_homes$modified_z) > 3.5\n\n# IQR method\nQ1 <- quantile(care_homes$staff_per_10, 0.25)\nQ3 <- quantile(care_homes$staff_per_10, 0.75)\nIQR_val <- IQR(care_homes$staff_per_10)\nlower_fence <- Q1 - 1.5 * IQR_val\nupper_fence <- Q3 + 1.5 * IQR_val\ncare_homes$outlier_iqr <- care_homes$staff_per_10 < lower_fence | \n                          care_homes$staff_per_10 > upper_fence\n\n# Summary table\ndetection_summary <- data.frame(\n  Method = c(\"Z-score (|z| > 2.5)\", \"Modified Z-score (|z| > 3.5)\", \"IQR (k=1.5)\"),\n  Outliers = c(sum(care_homes$outlier_z), \n               sum(care_homes$outlier_modz), \n               sum(care_homes$outlier_iqr))\n)\nprint(detection_summary, row.names = FALSE)\ncat(\"\\nIQR Fences: [\", round(lower_fence, 2), \",\", round(upper_fence, 2), \"]\\n\")\n```\n\n## Python\n```{python}\n#| label: outlier-detect-py\n#| echo: true\n\nfrom scipy import stats\nfrom scipy.stats import median_abs_deviation\n\n# Z-score method\ncare_homes['z_score'] = stats.zscore(care_homes['staff_per_10'])\ncare_homes['outlier_z'] = abs(care_homes['z_score']) > 2.5\n\n# Modified Z-score method\nmedian_val = care_homes['staff_per_10'].median()\nmad_val = median_abs_deviation(care_homes['staff_per_10'])\ncare_homes['modified_z'] = 0.6745 * (care_homes['staff_per_10'] - median_val) / mad_val\ncare_homes['outlier_modz'] = abs(care_homes['modified_z']) > 3.5\n\n# IQR method\nQ1 = care_homes['staff_per_10'].quantile(0.25)\nQ3 = care_homes['staff_per_10'].quantile(0.75)\nIQR_val = Q3 - Q1\nlower_fence = Q1 - 1.5 * IQR_val\nupper_fence = Q3 + 1.5 * IQR_val\ncare_homes['outlier_iqr'] = ((care_homes['staff_per_10'] < lower_fence) | \n                             (care_homes['staff_per_10'] > upper_fence))\n\n# Summary table\nimport pandas as pd\ndetection_summary = pd.DataFrame({\n    'Method': ['Z-score (|z| > 2.5)', 'Modified Z-score (|z| > 3.5)', 'IQR (k=1.5)'],\n    'Outliers': [care_homes['outlier_z'].sum(), \n                 care_homes['outlier_modz'].sum(), \n                 care_homes['outlier_iqr'].sum()]\n})\nprint(detection_summary.to_string(index=False))\nprint(f\"\\nIQR Fences: [{lower_fence:.2f}, {upper_fence:.2f}]\")\n```\n:::\n\n**Analysis**: Modified Z-score is more conservative (fewer false positives) due to robustness. IQR matches box plot visualization.\n\n### Step 3: Compare Methods\n\n::: {.panel-tabset}\n## R\n```{r}\n#| label: outlier-compare\n#| echo: true\n\n# Count flags per home\ncare_homes$num_flags <- care_homes$outlier_z + care_homes$outlier_modz + \n                        care_homes$outlier_iqr\n\n# Agreement summary\ncat(\"Homes flagged by: \", paste(names(table(care_homes$num_flags)), \"method(s) =\", \n                                 table(care_homes$num_flags)), \"\\n\")\n\n# High-priority cases (2+ methods)\nflagged <- care_homes[care_homes$num_flags >= 2, \n                      c(\"home_id\", \"staff_per_10\", \"z_score\", \"modified_z\", \"num_flags\")]\nflagged <- flagged[order(-abs(flagged$z_score)), ]\n\ncat(\"\\nHigh-priority homes (2+ methods agree):\\n\")\nprint(flagged, row.names = FALSE)\n```\n\n## Python\n```{python}\n#| label: outlier-compare-py\n#| echo: true\n\n# Count flags per home\ncare_homes['num_flags'] = (care_homes['outlier_z'].astype(int) + \n                           care_homes['outlier_modz'].astype(int) + \n                           care_homes['outlier_iqr'].astype(int))\n\n# Agreement summary\nagreement = care_homes['num_flags'].value_counts().sort_index()\nprint(\"Homes flagged by:\", \", \".join([f\"{k} method(s) = {v}\" for k, v in agreement.items()]))\n\n# High-priority cases\nflagged = care_homes[care_homes['num_flags'] >= 2][\n    ['home_id', 'staff_per_10', 'z_score', 'modified_z', 'num_flags']\n].sort_values('z_score', key=abs, ascending=False)\n\nprint(\"\\nHigh-priority homes (2+ methods agree):\")\nprint(flagged.to_string(index=False))\n```\n:::\n\n**Interpretation**: Homes with very low (<1) or very high (>7) ratios flagged by multiple methods are highest priority for investigation. Agreement across methods strengthens case that these warrant attention.\n\n## Common Pitfalls\n\n**Pitfall 1: Mechanical application without investigation**\n\nDetecting outliers doesn't tell you what to do with them. Every flagged observation requires investigation to determine whether it's an error, genuine extreme, or contextual issue.\n\n**Pitfall 2: Using wrong method for data type**\n\nZ-scores assume normality. For highly skewed data, use modified Z-scores or IQR. Don't force methods onto data that violates assumptions.\n\n**Pitfall 3: Removing outliers to \"improve\" results**\n\nNever remove observations just to make your analysis tidier or improve model fit. Outliers contain information. Only remove when you have evidence of errors.\n\n**Pitfall 4: Not documenting decisions**\n\n\"We removed 5 outliers\" is insufficient for QA. Document which observations, which methods, what investigation revealed, and why you chose your treatment.\n\n**Pitfall 5: Ignoring small numbers**\n\nSmall providers naturally show higher variation. An extreme value from a 15-bed home is less surprising than from a 100-bed home. Consider denominators when interpreting outliers.\n\n**Pitfall 6: Single method only**\n\nUsing only one detection method misses the bigger picture. Apply 2-3 methods and look for consensus. Agreement across methods strengthens the case for investigation.\n\n## Documenting Your Analysis\n\nFor QA purposes, record:\n\n**Detection approach**:\n- Which methods applied and why chosen\n- Thresholds used and rationale\n- Number of outliers flagged by each method\n- Agreement/disagreement across methods\n\n**Investigation process**:\n- Which observations investigated\n- Evidence gathered (provider contact, related data, narrative)\n- Findings for each case\n- Time spent on investigation\n\n**Treatment decisions**:\n- Which outliers kept, which removed, which adjusted\n- Justification for each decision\n- How treatment affects conclusions\n- Sensitivity analysis (do conclusions change?)\n\n**Evidence to retain**:\n- Distribution plots (before and after)\n- Detection method outputs with thresholds\n- Investigation log with documented decisions\n- Final dataset with exclusion flags and reasons\n\nSee @sec-qa-principles for full QA documentation requirements and templates.\n\n## Related Approaches\n\n**For different data characteristics**:\n- **Multivariate outliers** â†’ Mahalanobis distance (escalate to Guild)\n- **Time-series outliers** â†’ SPC methods (@sec-spc-basics)\n- **Clustering-based detection** â†’ DBSCAN, Isolation Forest (escalate to Guild)\n\n**For foundational understanding**:\n- **Conceptual framework** â†’ @sec-unusual-observations (read first!)\n- **Small provider variation** â†’ @sec-variation-uncertainty\n- **Data quality checks** â†’ @sec-qa-principles\n\n**For related analyses**:\n- **Provider comparison** â†’ Z-scoring (@sec-z-scoring) handles expected variation\n- **Missing data** â†’ @sec-missing-data-foundations (outliers vs missing different issues)\n\n## When to Escalate to the Guild\n\nEscalate when:\n\n- **Multivariate outliers needed**: Outliers across multiple variables simultaneously\n- **Advanced methods required**: Isolation Forest, Local Outlier Factor, or other machine learning approaches\n- **High-stakes decisions**: Outlier treatment could materially affect regulatory action\n- **Methodological challenge**: Stakeholder questions your outlier treatment approach\n- **Unusual data structure**: Hierarchical data, spatial data, or other complex structures\n- **Systematic patterns**: Many outliers suggesting data quality or population issues\n- **Resource constraints**: Too many outliers to investigate manually\n\n## Key Takeaways\n\n::: {.callout-important icon=false}\n## Essential Points\n\n1. **Detection â‰  decision**: Flagging outliers is just the first step; investigation determines what to do\n2. **Multiple methods essential**: Use 2-3 methods and look for consensus across approaches\n3. **Context is critical**: An outlier in one context may be expected in another\n4. **Document everything**: Which observations flagged, investigated, and treatedâ€”with justification\n5. **Visual inspection first**: Always plot your data before applying detection methods\n6. **Never remove mechanically**: Only remove outliers when you have evidence they're errors\n7. **Consider small numbers**: Small providers show more variationâ€”don't penalize natural spread\n\nOutlier detection is about systematically prioritizing investigation, not automatically removing inconvenient data points.\n:::\n\n## Further Reading\n\n**Internal CQC Resources**:\n\n- **QA Framework**: Documentation standards for outlier investigation and treatment\n- **Guild Terms of Reference**: When and how to escalate for specialist support\n- **Data Quality Guidance**: Distinguishing errors from genuine extremes\n\n**External Guidance**:\n\n- **Tukey JW** (1977). *Exploratory Data Analysis*. Addison-Wesley. (Classic text on outlier detection including box plots and fences)\n- **Iglewicz B, Hoaglin DC** (1993). \"How to Detect and Handle Outliers.\" *ASQC Basic References in Quality Control* vol. 16. (Modified Z-score method)\n- **Rousseeuw PJ, Hubert M** (2011). \"Robust statistics for outlier detection.\" *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*, 1(1), 73-79.\n\n**Online Resources**:\n\n- **NIST Engineering Statistics Handbook**: Section on outlier detection (freely available, comprehensive)\n- **R Documentation**: `?boxplot.stats` for Tukey's method implementation\n- **Python SciPy**: `scipy.stats.zscore` and `scipy.stats.median_abs_deviation` documentation"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../custom.css"],"toc":true,"toc-depth":3,"number-sections":true,"output-file":"15-outlier-methods.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../references.bib"],"theme":"cosmo","number-depth":2,"title":"Outlier Detection and Treatment"},"extensions":{"book":{"multiFile":true}}}}}