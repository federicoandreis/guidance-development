{"title":"Exploring Relationships: Correlation & Association","markdown":{"yaml":{"title":"Exploring Relationships: Correlation & Association"},"headingText":"| include: false","containsRefs":false,"markdown":"\n\n```{r}\n# Setup Python environment\nlibrary(reticulate)\n# Use system Python (where pandas is installed)\nuse_python(\"C:/Users/fede/anaconda3/python.exe\", required = TRUE)\n```\n\n::: {.callout-tip icon=false}\n## Problem This Method Solves\n\nYou need to understand whether two variables are related and how strong that relationship is. For example:\n\n- Do areas with higher deprivation have lower GP patient satisfaction scores?\n- Is staffing level associated with incident rates in care homes?\n- Are medication error rates related to patient complexity measures?\n- Do providers with longer wait times also have higher complaint rates?\n\nCorrelation analysis measures the strength and direction of relationships between variables, helping you understand patterns in your data and identify areas for further investigation.\n:::\n\n## What Correlation Analysis Does {#sec-correlation}\n\nCorrelation quantifies the relationship between two variables using a single number (the correlation coefficient) that ranges from -1 to +1. It answers: \"When one variable changes, does the other tend to change in a predictable way?\"\n\n**Key advantages**:\n\n- Quantifies relationship strength on a standardized scale (-1 to +1)\n- Identifies direction (positive or negative association)\n- Flags potential areas for further investigation\n- Helps understand contextual factors affecting outcomes\n- Provides evidence for or against assumed relationships\n\n**What it does NOT do**:\n\n- Prove causation (correlation ‚â† causation, see @sec-correlation-causation)\n- Account for confounding variables (without additional methods)\n- Replace domain knowledge about mechanisms\n- Justify provider comparison based on associations alone\n- Detect non-linear relationships (unless using appropriate methods)\n\n::: {.callout-warning icon=false}\n## Critical Distinction\n\nCorrelation analysis is for **thematic and contextual work**, not provider performance comparison. Use z-scoring (@sec-z-scoring) or SPC (@sec-spc-basics) for comparing providers.\n:::\n\n## Why It Matters for CQC\n\nUnderstanding relationships between variables helps you:\n\n1. **Contextualize performance**: Understand whether outcomes relate to factors outside provider control (e.g., deprivation, demographics)\n2. **Identify patterns**: Spot systematic relationships that warrant investigation\n3. **Challenge assumptions**: Test whether assumed relationships actually exist in the data\n4. **Inform policy**: Provide evidence about factors associated with quality\n5. **Target resources**: Identify characteristics associated with risk\n\n**Example**: If you find a strong correlation between deprivation and poor outcomes, this doesn't excuse poor performance, but it does inform how you interpret results and where additional support might be needed.\n\n## Before You Start\n\n::: {.callout-note icon=false}\n## Note on Code Examples\nThe worked examples in this chapter use pseudo-randomly generated data to illustrate the methods. To ensure consistent results between R and Python, the Python code uses the same dataset generated by R (via `r.data_name`). The commented-out Python code shows how you would generate equivalent data independently if needed.\n:::\n\n::: {.callout-tip icon=false}\n## üìñ Complete Worked Example\n\n**Example 9: Staffing and Quality Outcomes** demonstrates correlation analysis in a realistic CQC context:\n\n- Investigating relationship between nurse staffing ratios and pressure ulcer rates\n- Handling confounding variables (resident complexity)\n- Investigating outliers with unexpected patterns\n- Conducting sensitivity analyses\n- Distinguishing correlation from causation in reporting\n\n**Includes**: Full workflow from research question to policy recommendations + QA documentation\n\n**Time**: 3-4 hours | **Complexity**: Medium-High\n\n[View Example 9: Staffing and Quality Outcomes ‚Üí](../examples/example-09-complete.qmd)\n:::\n\nCheck these requirements before conducting correlation analysis:\n\n### Data Requirements\n\n- [ ] **Data type**: Both variables continuous, or at least ordinal with sufficient categories (5+)\n- [ ] **Sample size**: Minimum 30 observations; ideally 50+ for stable estimates\n- [ ] **Independence**: Each observation independent (not repeated measures from same provider over time)\n- [ ] **No extreme outliers**: Outliers can severely distort correlation coefficients\n- [ ] **Sufficient variation**: Both variables show meaningful variation (not all similar values)\n\n### Analytical Questions to Answer First\n\nBefore calculating any correlation, clarify:\n\n1. **What is the research question?** Not just \"are they correlated?\" but \"why would they be related?\"\n2. **Is there domain knowledge suggesting a relationship?** What's the hypothesized mechanism?\n3. **What would a relationship mean for CQC's work?** How would you use this information?\n4. **Are there confounding variables?** Could a third variable explain both?\n5. **Is the relationship likely to be linear?** Or might it be curved, threshold-based, or more complex?\n\n### When NOT to Use Correlation\n\n- **Categorical outcome variables** ‚Üí Use chi-squared test or other categorical methods (escalate to Guild)\n- **Comparing provider performance** ‚Üí Use z-scoring (@sec-z-scoring) or SPC (@sec-spc-basics)\n- **Establishing causation** ‚Üí Need experimental or quasi-experimental design (escalate to Guild)\n- **Multiple variables simultaneously** ‚Üí Need multivariate methods like regression (escalate to Guild)\n- **Time series data** ‚Üí Autocorrelation issues require specialist methods (escalate to Guild)\n- **Hierarchical data** ‚Üí Providers nested in regions, patients nested in providers (escalate to Guild)\n\n## Understanding Correlation Coefficients\n\n### The Correlation Scale\n\nCorrelation coefficients range from -1 to +1:\n\n- **+1**: Perfect positive relationship: as one variable increases, the other increases proportionally\n- **0**: No linear relationship: knowing one variable tells you nothing about the other\n- **-1**: Perfect negative relationship: as one variable increases, the other decreases proportionally\n\n**Important**: The sign (+ or -) indicates direction; the absolute value indicates strength. A correlation of -0.7 is just as strong as +0.7, just in the opposite direction.\n\n### Interpreting Strength\n\n::: {.callout-note icon=false}\n## Rethinking: Strength Guidelines Are Context-Dependent\n\nYou'll see many sources give \"rules\" like 0.3-0.5 = \"moderate\" correlation. **Ignore these rules.** What counts as a \"strong\" correlation depends entirely on your field and question.\n\nIn social science, correlations of 0.3-0.4 are often considered meaningful because human behavior is complex. In physics, a correlation below 0.9 might indicate measurement error.\n\nFor CQC work, consider:\n\n- **0.2-0.4**: Weak but potentially meaningful for complex social/health phenomena\n- **0.4-0.6**: Moderate, worth investigating further\n- **0.6-0.8**: Strong, substantial relationship\n- **0.8+**: Very strong, rare in health/social care data\n\nBut always ask: \"Is this correlation large enough to matter for our work?\" Not \"Does it exceed 0.5?\"\n:::\n\n### Statistical vs. Practical Significance\n\nA correlation can be **statistically significant** (p < 0.05) but **practically meaningless**:\n\n- With 1,000 providers, a correlation of 0.09 might be statistically significant (p < 0.01)\n- But r = 0.09 means the variables share less than 1% of their variation (r¬≤ = 0.0081)\n- This tells you almost nothing useful\n\nConversely, a correlation can be **practically important** but **not statistically significant**:\n\n- With 15 care homes, a correlation of 0.45 might not reach p < 0.05\n- But r = 0.45 suggests a moderate relationship worth investigating\n- You just lack statistical power to confirm it's not due to chance\n\n**Always report both**: the correlation coefficient (effect size) and the p-value (statistical significance). See @sec-statistical-inference for more on this distinction.\n\n## Choosing the Right Correlation Method\n\nThere are two main correlation methods used at CQC. Choose based on your data characteristics and the relationship type.\n\n### Pearson's Correlation (r)\n\n**Use when**:\n\n- Both variables are continuous\n- The relationship is linear (straight-line pattern)\n- Data is approximately normally distributed (no severe skew)\n- No extreme outliers present\n\n**How it works**: Measures the strength of the **linear** relationship. If the relationship is curved, Pearson's r will underestimate the strength.\n\n**CQC examples**:\n\n- GP patient satisfaction scores vs. practice list size\n- Average wait times vs. staffing ratios\n- Incident rates vs. bed occupancy percentages\n- Medication error rates vs. staff turnover percentages\n\n### Spearman's Rank Correlation (œÅ)\n\n**Use when**:\n\n- One or both variables are ordinal (e.g., CQC ratings, severity levels)\n- The relationship is monotonic but not necessarily linear (consistent direction but possibly curved)\n- Data has outliers or is severely skewed\n- Distributions are non-normal\n\n:**How it works**: Ranks all values from lowest to highest, then measures correlation of the ranks. More robust to outliers and doesn't assume linearity, just that the relationship is consistently positive or negative.\n\n**CQC examples**:\n\n- CQC rating (ordinal) vs. deprivation index score\n- Severity categories (ordinal) vs. length of stay\n- Staff satisfaction scores vs. turnover rates (if skewed)\n- Any analysis with outliers that you can't remove\n\n::: {.callout-note icon=false}\n## Rethinking: When in Doubt, Use Spearman\n\nIf you're unsure which to use, **default to Spearman's œÅ**. It's more robust and works for both linear and non-linear monotonic relationships. You'll rarely go wrong with Spearman.\n\nThe only time you *must* use Pearson is when you need the specific linear relationship for further calculations (e.g., as input to other statistical models). For exploratory CQC work, Spearman is usually the safer choice.\n:::\n\n## Step-by-Step Guide\n\n### 1. Visualize First (Always)\n\n:**Never calculate correlation without looking at the data first.** A single number can hide important patterns.\n\nCreate a scatter plot with:\n\n- One variable on x-axis, one on y-axis\n- Each point represents one observation (e.g., one provider, one area)\n- Look for overall pattern, outliers, non-linear relationships\n\n:**What to look for**:\n\n- **Linear pattern**: Points roughly follow a straight line ‚Üí Pearson appropriate\n- **Curved pattern**: Points follow a curve ‚Üí Use Spearman, or escalate for non-linear methods\n- **No pattern**: Random scatter ‚Üí Correlation likely near zero\n- **Outliers**: Extreme points far from the main cluster ‚Üí Use Spearman or investigate outliers\n- **Clusters**: Distinct groups ‚Üí May need to analyze separately\n\n### 2. Check Data Quality\n\nBefore calculating correlation:\n\n- [ ] Check for data entry errors (impossible values)\n- [ ] Identify and investigate outliers (see @sec-unusual-observations)\n- [ ] Verify both variables have sufficient variation\n- [ ] Confirm observations are independent\n- [ ] Check sample size is adequate (n ‚â• 30 minimum)\n\n### 3. Calculate Correlation\n\nSelect your method (Pearson or Spearman) and calculate:\n\n- **Correlation coefficient** (r or œÅ): Strength and direction\n- **P-value**: Statistical significance\n- **95% Confidence interval**: Uncertainty around the estimate\n- **Sample size**: Always report this\n\nMost statistical software provides all of these automatically.\n\n### 4. Interpret Results\n\nConsider all of these together:\n\n:**Strength**: How strong is the relationship?\n\n- Look at the coefficient magnitude (ignore sign for strength)\n- Consider r¬≤ (coefficient squared) = proportion of shared variation\n- Ask: \"Is this strong enough to matter for our work?\"\n\n:**Direction**: Positive or negative?\n\n- Positive: Both variables tend to increase together\n- Negative: As one increases, the other tends to decrease\n- Ask: \"Does this direction make sense given domain knowledge?\"\n\n:**Significance**: Is it unlikely to be due to chance?\n\n- Check p-value (typically p < 0.05 threshold)\n- Check confidence interval (does it exclude zero?)\n- Ask: \"Do I have enough data to be confident in this relationship?\"\n\n:**Context**: Does it make sense?\n\n- Does the relationship align with domain knowledge?\n- Are there plausible mechanisms explaining it?\n- Could confounding variables explain it?\n- What are the practical implications?\n\n### 5. Report Findings\n\nA complete correlation report includes:\n\n:**Essential elements**:\n\n1. **Method used**: \"Pearson's r\" or \"Spearman's œÅ\"\n2. **Sample size**: \"n = 127 GP practices\"\n3. **Coefficient with CI**: \"r = 0.42, 95% CI [0.26, 0.56]\"\n4. **P-value**: \"p < 0.001\"\n5. **Scatter plot**: Visual representation\n6. **Interpretation**: What it means in context\n7. **Limitations**: Caveats and alternative explanations\n\n:**Example report**:\n\n> \"We examined the relationship between deprivation index and GP patient satisfaction scores across 127 practices. Spearman's rank correlation showed a moderate negative association (œÅ = -0.38, 95% CI [-0.52, -0.22], p < 0.001), indicating that practices in more deprived areas tended to have lower satisfaction scores. However, this correlation explains only 14% of the variation in satisfaction (r¬≤ = 0.14), suggesting other factors play important roles. This relationship does not imply causation‚Äîdeprivation may be associated with other factors (e.g., staffing challenges, patient complexity) that affect satisfaction.\"\n\n### 6. Document Decisions\n\nAs with all CQC analysis (see @sec-qa-principles), document:\n\n- Why you chose Pearson vs. Spearman\n- How you handled outliers (if any)\n- Any data exclusions and rationale\n- Alternative explanations considered\n- Limitations of the analysis\n\n## Worked Example: Deprivation and GP Patient Access\n\n### Scenario\n\nYou're investigating whether deprivation is associated with GP access problems. Specifically, you want to know if practices in more deprived areas have lower percentages of patients who report being able to get an appointment when needed.\n\n:**Research question**: Is there a relationship between area deprivation and GP patient-reported access?\n\n:**Data**: 100 GP practices with:\n\n- **Deprivation score**: Index of Multiple Deprivation (IMD) decile (1 = most deprived, 10 = least deprived)\n- **Access percentage**: % of patients reporting they can get an appointment when needed (from GP Patient Survey)\n\n:**Hypothesis**: Practices in more deprived areas (lower IMD decile) will have lower access percentages, due to factors like higher demand, staffing challenges, or patient complexity.\n\n### Step 1: Generate and Visualize the Data\n\nFirst, create the dataset and visualize the relationship:\n\n::: {.panel-tabset}\n\n## R\n\n```{r}\n#| label: correlation-data\n#| echo: true\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate 100 GP practices\nn_practices <- 100\n\n# Generate deprivation scores (IMD decile: 1-10)\n# More practices in middle deciles (realistic distribution)\ndeprivation <- sample(1:10, n_practices, replace = TRUE, \n                      prob = c(0.12, 0.12, 0.11, 0.10, 0.10, \n                               0.10, 0.10, 0.10, 0.08, 0.07))\n\n# Generate access percentages with negative correlation to deprivation\n# More deprived areas (lower decile) tend to have lower access\n# But with substantial variation (r ‚âà -0.5)\naccess_pct <- 75 + (deprivation * 2.5) + rnorm(n_practices, mean = 0, sd = 8)\n\n# Ensure percentages stay within realistic bounds (60-95%)\naccess_pct <- pmin(pmax(access_pct, 60), 95)\n\n# Create dataframe\ngp_data <- data.frame(\n  practice_id = paste0(\"GP\", sprintf(\"%03d\", 1:n_practices)),\n  deprivation_decile = deprivation,\n  access_percentage = round(access_pct, 1)\n)\n\n# Display first 10 practices\nhead(gp_data, 10)\n```\n\n```{r}\n#| label: correlation-scatter\n#| echo: true\n#| fig-width: 7\n#| fig-height: 5\n\n# Create scatter plot\nplot(gp_data$deprivation_decile, gp_data$access_percentage,\n     xlab = \"Deprivation Decile (1 = Most Deprived, 10 = Least Deprived)\",\n     ylab = \"Access Percentage (%)\",\n     main = \"GP Patient Access vs. Area Deprivation\",\n     pch = 16, col = rgb(0, 0, 0.8, 0.5), cex = 1.2)\n\n# Add reference line (not a regression line, just to show trend)\nabline(lm(access_percentage ~ deprivation_decile, data = gp_data), \n       col = \"red\", lwd = 2, lty = 2)\n\n# Add grid for easier reading\ngrid()\n```\n\n## Python\n\n```{python}\n#| label: correlation-data-py\n#| echo: true\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Use R's data for consistency\ngp_data_py = r.gp_data\n\n# Display first 10 practices\nprint(gp_data_py.head(10))\n```\n\n```{python}\n#| label: correlation-scatter-py\n#| echo: true\n#| fig-width: 7\n#| fig-height: 5\n\n# Create scatter plot\nplt.figure(figsize=(7, 5))\nplt.scatter(gp_data_py['deprivation_decile'], \n            gp_data_py['access_percentage'],\n            alpha=0.5, s=60, color='blue')\n\n# Add trend line\nz = np.polyfit(gp_data_py['deprivation_decile'], \n               gp_data_py['access_percentage'], 1)\np = np.poly1d(z)\nplt.plot(gp_data_py['deprivation_decile'], \n         p(gp_data_py['deprivation_decile']), \n         \"r--\", linewidth=2, label='Trend line')\n\nplt.xlabel('Deprivation Decile (1 = Most Deprived, 10 = Least Deprived)')\nplt.ylabel('Access Percentage (%)')\nplt.title('GP Patient Access vs. Area Deprivation')\nplt.grid(True, alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n```\n\n:::\n\n:**Observation from scatter plot**: There appears to be a positive relationship: practices in less deprived areas (higher decile numbers) tend to have higher access percentages. The relationship looks roughly linear, though with considerable scatter.\n\n### Step 2: Calculate Correlation\n\nSince both variables are continuous and the relationship appears linear, we'll use **Pearson's correlation**. We'll also calculate **Spearman's correlation** for comparison (good practice when you're not certain about assumptions).\n\n::: {.panel-tabset}\n\n## R\n\n```{r}\n#| label: correlation-calculate\n#| echo: true\n\n# Pearson's correlation\npearson_result <- cor.test(gp_data$deprivation_decile, \n                           gp_data$access_percentage,\n                           method = \"pearson\")\n\n# Spearman's correlation\nspearman_result <- cor.test(gp_data$deprivation_decile, \n                            gp_data$access_percentage,\n                            method = \"spearman\")\n\n# Create summary table\ncorrelation_results <- data.frame(\n  Method = c(\"Pearson's r\", \"Spearman's œÅ\"),\n  Coefficient = c(pearson_result$estimate, spearman_result$estimate),\n  CI_Lower = c(pearson_result$conf.int[1], NA),\n  CI_Upper = c(pearson_result$conf.int[2], NA),\n  P_Value = c(pearson_result$p.value, spearman_result$p.value),\n  Sample_Size = c(length(gp_data$deprivation_decile), \n                  length(gp_data$deprivation_decile))\n)\n\n# Format for display\ncorrelation_results$Coefficient <- round(correlation_results$Coefficient, 3)\ncorrelation_results$CI_Lower <- round(correlation_results$CI_Lower, 3)\ncorrelation_results$CI_Upper <- round(correlation_results$CI_Upper, 3)\ncorrelation_results$P_Value <- format.pval(correlation_results$P_Value, digits = 3)\n\n# Display results\nknitr::kable(correlation_results, \n             col.names = c(\"Method\", \"Coefficient\", \"95% CI Lower\", \n                          \"95% CI Upper\", \"P-Value\", \"n\"),\n             caption = \"Correlation between Deprivation and GP Access\")\n```\n\n## Python\n\n```{python}\n#| label: correlation-calculate-py\n#| echo: true\n\n# Pearson's correlation\npearson_r, pearson_p = stats.pearsonr(gp_data_py['deprivation_decile'], \n                                       gp_data_py['access_percentage'])\n\n# Spearman's correlation\nspearman_rho, spearman_p = stats.spearmanr(gp_data_py['deprivation_decile'], \n                                            gp_data_py['access_percentage'])\n\n# Calculate confidence interval for Pearson (using Fisher's z-transformation)\nn = len(gp_data_py)\nz = np.arctanh(pearson_r)\nse = 1 / np.sqrt(n - 3)\nz_crit = 1.96  # 95% CI\nci_lower = np.tanh(z - z_crit * se)\nci_upper = np.tanh(z + z_crit * se)\n\n# Create summary table\nresults_df = pd.DataFrame({\n    'Method': [\"Pearson's r\", \"Spearman's œÅ\"],\n    'Coefficient': [pearson_r, spearman_rho],\n    'CI_Lower': [ci_lower, np.nan],\n    'CI_Upper': [ci_upper, np.nan],\n    'P_Value': [pearson_p, spearman_p],\n    'n': [n, n]\n})\n\n# Format for display\nresults_df['Coefficient'] = results_df['Coefficient'].round(3)\nresults_df['CI_Lower'] = results_df['CI_Lower'].round(3)\nresults_df['CI_Upper'] = results_df['CI_Upper'].round(3)\nresults_df['P_Value'] = results_df['P_Value'].apply(lambda x: f\"{x:.3e}\" if x < 0.001 else f\"{x:.3f}\")\n\nprint(\"\\nCorrelation between Deprivation and GP Access\")\nprint(results_df.to_string(index=False))\n```\n\n:::\n\n### Step 3: Interpret Results\n\n:**Correlation coefficient**: r = 0.52 (moderate positive correlation)\n\n:**What this means**:\n\n- As deprivation decile increases (areas become less deprived), access percentages tend to increase\n- The relationship is moderate in strength, not perfect, but substantial\n- r¬≤ = 0.27, meaning deprivation explains about 27% of the variation in access\n\n:**Statistical significance**: p < 0.001 (highly significant)\n\n- With 100 practices, we have strong evidence this relationship is not due to chance\n- The 95% CI [0.35, 0.66] excludes zero, confirming the relationship\n\n:**Pearson vs. Spearman**: Both methods give similar results (r ‚âà œÅ), confirming the relationship is approximately linear and not driven by outliers.\n\n### Step 4: Contextualize Findings\n\n:**What this tells us**:\n\n1. **Association confirmed**: There is a clear relationship between deprivation and GP access\n2. **Moderate strength**: Deprivation is an important factor, but not the only one (73% of variation unexplained)\n3. **Direction makes sense**: Less deprived areas have better access, consistent with known health inequalities\n\n:**What this does NOT tell us**:\n\n1. **Causation**: We don't know if deprivation *causes* lower access, or if both are caused by other factors\n2. **Mechanisms**: We don't know *why* the relationship exists (staffing? Demand? Patient complexity?)\n3. **Provider performance**: This doesn't tell us which individual practices are performing well or poorly given their context\n\n:**Potential confounders to consider**:\n\n- Practice size (larger practices may have different access and serve different populations)\n- Urban vs. rural location (affects both deprivation and access)\n- Practice age and resources\n- Local healthcare infrastructure\n\n### Step 5: Report and Document\n\n:**Summary for stakeholders**:\n\n> \"Analysis of 100 GP practices shows a moderate positive correlation between area deprivation and patient-reported access (Pearson's r = 0.52, 95% CI [0.35, 0.66], p < 0.001). Practices in less deprived areas (higher IMD deciles) tend to have higher percentages of patients reporting they can get appointments when needed. However, deprivation explains only 27% of the variation in access, indicating other factors also play important roles. This relationship does not establish causation‚Äîfurther investigation is needed to understand the mechanisms and identify modifiable factors that could improve access in deprived areas.\"\n\n:**Documentation**:\n\n- Method: Pearson's correlation (linear relationship, continuous variables, no extreme outliers)\n- Sample: 100 GP practices with complete data on both variables\n- Assumptions checked: Linearity confirmed via scatter plot; Spearman's œÅ similar to Pearson's r\n- Limitations: Cross-sectional data (cannot infer causation); potential confounders not controlled; ecological analysis (area-level, not individual patient-level)\n- Next steps: Investigate mechanisms; consider multivariate analysis controlling for practice size, location; examine individual practices with unexpected access levels given their deprivation\n\n## Common Pitfalls {#sec-correlation-causation}\n\n### Correlation Does Not Imply Causation\n\nThis is the most important principle in correlation analysis. **A correlation between X and Y does not mean X causes Y.**\n\n:**Three possible explanations for any correlation**:\n\n1. **X causes Y**: Deprivation causes lower access (direct causation)\n2. **Y causes X**: Lower access causes deprivation (reverse causation, unlikely here but possible in other contexts)\n3. **Z causes both X and Y**: A third variable (confounding) causes both (e.g., poor local infrastructure causes both deprivation and access problems)\n\n:**Classic spurious correlation examples**:\n\n- Ice cream sales correlate with drowning deaths (both caused by hot weather)\n- Number of firefighters at a fire correlates with fire damage (both caused by fire severity)\n- Shoe size correlates with reading ability in children (both caused by age)\n\n:**For CQC work**: Always ask \"What else could explain this relationship?\" before concluding anything about causation.\n\n::: {.callout-note icon=false}\n## Overthinking: When Can We Infer Causation?\n\nEstablishing causation requires more than correlation. You need:\n\n1. **Temporal precedence**: Cause must precede effect (longitudinal data)\n2. **Dose-response**: Stronger \"cause\" ‚Üí stronger \"effect\"\n3. **Plausible mechanism**: A credible explanation for *how* X causes Y\n4. **Alternative explanations ruled out**: Confounders controlled or eliminated\n5. **Consistency**: Relationship holds across different contexts and studies\n\nThe gold standard is a randomized controlled trial (RCT). For observational data, you need quasi-experimental designs, propensity score matching, instrumental variables, or other advanced methods.\n\n:**For CQC**: If you need to establish causation, escalate to the Guild. Correlation analysis is for exploration and hypothesis generation, not causal inference.\n:::\n\n### Simpson's Paradox\n\nA correlation can **reverse direction** when you separate data into subgroups.\n\n:**Example**: Overall, practices with higher deprivation might have lower access. But when you separate by region:\n\n- In Region A: Higher deprivation ‚Üí higher access\n- In Region B: Higher deprivation ‚Üí higher access\n- Combined: Higher deprivation ‚Üí lower access (paradox!)\n\nThis happens when the subgroups have very different baseline rates and different proportions in each deprivation category.\n\n:**Implication**: Always consider whether your data should be analyzed in subgroups (by region, provider type, etc.) rather than pooled.\n\n### Range Restriction\n\nCorrelations can be **artificially weakened** when you restrict the range of one or both variables.\n\n:**Example**: If you only analyze practices in the most deprived areas (IMD deciles 1-3), you might find no correlation with access, not because there isn't one, but because you've removed most of the variation in deprivation.\n\n:**Implication**: Be cautious about excluding data or analyzing only subsets. Report the range of values analyzed.\n\n### Outliers Can Distort Correlations\n\nA single extreme value can create, destroy, or reverse a correlation.\n\n:**Example**: 99 practices show no relationship between deprivation and access. One practice in a very deprived area has exceptionally high access (due to recent major investment). This single outlier could create a positive correlation that doesn't represent the typical pattern.\n\n:**Solution**: Always visualize first. If outliers are present, use Spearman's œÅ (more robust) or investigate and potentially exclude outliers with justification.\n\n### The Ecological Fallacy\n\nA correlation at the **group level** does not necessarily hold at the **individual level**.\n\n:**Example**: Areas with higher deprivation have lower average access. But this doesn't mean that individual deprived patients within those areas have lower access, it could be that deprived and non-deprived patients in those areas both have lower access due to area-level factors.\n\n:**Implication**: Be careful about inferring individual-level relationships from area-level data. State clearly what level your analysis applies to.\n\n### Multiple Testing\n\nIf you test many correlations, some will be \"significant\" by chance alone.\n\n**Example**: You test correlations between 20 different practice characteristics and access. Even if none are truly related, you'd expect 1 to be \"significant\" at p < 0.05 purely by chance (5% false positive rate).\n\n**Solution**: \n\n- Have a clear hypothesis before testing (don't go on \"fishing expeditions\")\n- Adjust significance thresholds if testing multiple correlations (e.g., Bonferroni correction)\n- Report all tests conducted, not just significant ones\n- Treat exploratory findings as hypothesis-generating, requiring confirmation\n\n## Documenting Your Analysis\n\nFor QA purposes, record:\n\n**Method selection**:\n- Which correlation method used (Pearson or Spearman) and why\n- Justification based on data characteristics (linearity, outliers, distribution)\n\n**Data preparation**:\n- How outliers were handled (removed, kept, investigated)\n- Any transformations applied\n- Sample size and any exclusions\n\n**Results**:\n- Correlation coefficient with confidence interval\n- p-value and interpretation\n- Scatter plot showing the relationship\n- Sample size\n\n**Interpretation**:\n- Strength and direction of relationship in context\n- Practical vs statistical significance\n- Alternative explanations considered\n- Limitations (confounders, causation, generalizability)\n\n**Decisions**:\n- What follow-up analysis or investigation is needed\n- Whether findings warrant escalation to Guild\n- How results inform CQC work\n\n**Evidence to retain**:\n- Scatter plots and any diagnostic plots\n- Full correlation output (coefficient, CI, p-value)\n- Decision log for method choice and outlier handling\n\nSee @sec-qa-principles for full QA documentation requirements and templates.\n\n## Related Approaches\n\n### Scatter Plot Matrix\n\nWhen exploring relationships among multiple variables, create a matrix of scatter plots showing all pairwise relationships. Useful for identifying which pairs warrant formal correlation analysis.\n\n### Partial Correlation\n\nMeasures the correlation between X and Y while controlling for Z. Requires multivariate methods, escalate to Guild.\n\n### Regression Analysis\n\nWhen you want to predict Y from X, or control for multiple variables simultaneously. More complex than correlation, escalate to Guild unless you have training.\n\n### Chi-Squared Test\n\nFor examining association between categorical variables (e.g., CQC rating vs. provider type). Different method from correlation, escalate to Guild if needed.\n\n## When to Escalate to the Guild\n\nEscalate to the Quantitative Guild when:\n\n- **Multiple correlated variables**: You need to understand relationships among 3+ variables simultaneously ‚Üí Need multivariate methods (multiple regression, factor analysis)\n- **Suspected confounding**: You need to control for other variables ‚Üí Need multiple regression or propensity score methods\n- **Causation questions**: You need to establish causal direction ‚Üí Need quasi-experimental designs, instrumental variables, or other causal inference methods\n- **Non-linear relationships**: Scatter plot shows clear curved pattern ‚Üí Need non-linear modeling or transformation\n- **Time series data**: Variables measured repeatedly over time ‚Üí Need time series methods (autocorrelation, cross-correlation, VAR models)\n- **Hierarchical structure**: Data nested (patients in providers, providers in regions) ‚Üí Need multilevel modeling\n- **Categorical outcomes**: Your outcome is categorical (CQC rating, yes/no) ‚Üí Need logistic regression or chi-squared tests\n- **Small sample size with important decision**: n < 30 and results will inform major decisions ‚Üí Need specialist input on interpretation and uncertainty\n\n## Key Takeaways\n\n::: {.callout-important icon=false}\n## Essential Points\n\n1. **Visualize first**: Always create a scatter plot before calculating correlation\n2. **Choose appropriately**: Pearson for linear relationships with continuous data; Spearman for ordinal data, outliers, or non-linear monotonic relationships\n3. **Report completely**: Coefficient, confidence interval, p-value, sample size, and scatter plot\n4. **Correlation ‚â† causation**: A relationship does not establish cause and effect\n5. **Context matters**: Interpret strength relative to your field, not arbitrary thresholds\n6. **Check assumptions**: Linearity, outliers, independence, sufficient variation\n7. **Document decisions**: Why you chose your method, how you handled outliers, limitations\n8. **Practical significance**: A statistically significant correlation can be too weak to matter\n9. **Confounders**: Always consider alternative explanations for observed relationships\n10. **Escalate when needed**: Complex relationships, causation questions, or multiple variables require specialist methods\n:::\n\n## Further Reading\n\n**Internal CQC Resources**:\n\n- QA Framework: Documentation standards for analytical decisions\n- Guild Terms of Reference: When to escalate for specialist support\n\n**External Guidance**:\n\n- AQuA Book (Chapter on Exploratory Data Analysis)\n- Altman DG (1991). *Practical Statistics for Medical Research*. Chapman & Hall. (Chapter 11: Correlation and regression)\n- Field A (2013). *Discovering Statistics Using R/SPSS*. Sage. (Chapters on correlation)\n\n**Online Resources**:\n\n- Understanding correlation: https://www.statology.org/correlation-coefficient/\n- Correlation vs causation: https://www.tylervigen.com/spurious-correlations (entertaining examples)\n- When to use Pearson vs Spearman: https://www.statology.org/pearson-vs-spearman-correlation/\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../custom.css"],"toc":true,"toc-depth":3,"number-sections":true,"output-file":"14-correlation.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../references.bib"],"theme":"cosmo","number-depth":2,"title":"Exploring Relationships: Correlation & Association"},"extensions":{"book":{"multiFile":true}}}}}