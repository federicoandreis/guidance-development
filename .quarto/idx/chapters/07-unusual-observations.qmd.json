{"title":"Unusual Observations and Extreme Values","markdown":{"yaml":{"title":"Unusual Observations and Extreme Values"},"headingText":"When Values Stand Out","headingAttr":{"id":"sec-unusual-observations","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n\nNot all data points are created equal. In any dataset, some values stand out—waiting times much longer than average, incident rates much higher than peers, staffing levels far below comparators. These **unusual observations** (often called outliers) demand attention, but what makes a value \"unusual\" is not as straightforward as it might seem.\n\n**The core insight**: There is no universal definition of \"unusual.\" What counts as extreme depends on your question, your data, and your context. The same value might be a concerning outlier for one analysis but perfectly reasonable for another.\n\nThis chapter addresses three fundamental questions:\n\n1. **What makes an observation unusual?** Understanding different ways to define \"extreme\"\n2. **Why do unusual values arise?** Distinguishing errors from genuine extremes\n3. **How should you handle them?** Principled approaches to investigation and decision-making\n\n**This chapter focuses on concepts.** For computational methods (Tukey's fences, modified z-scores, multivariate detection), see @sec-outlier-methods.\n\n**Building on previous chapters**: You need to understand central tendency (@sec-describing-data) and variation (@sec-variation-uncertainty) before you can define what \"far from typical\" means. You also need to understand statistical inference (@sec-statistical-inference) to interpret whether an unusual value signals a problem.\n\n## Why Unusual Values Arise\n\nBefore deciding how to handle an unusual observation, understand why it exists.\n\n### Data Errors\n\n**Most common cause in practice**. Errors create values that are impossible or implausible.\n\n**Examples**:\n- **Entry errors**: \"234\" instead of \"23.4\" (decimal point missing)\n- **Unit errors**: Days recorded as hours (72 instead of 3)\n- **System glitches**: Missing data coded as 999\n- **Copy-paste errors**: Values from one provider appearing under another\n- **Measurement failures**: Sensor malfunction, incomplete survey\n\n**CQC implication**: Before investigating a provider for extreme performance, verify the data. Many \"outliers\" vanish when you correct errors.\n\n### Genuine Extremes\n\n**Real but rare events**. Some values are accurate recordings of unusual situations.\n\n**Examples**:\n- Care home with COVID outbreak has genuinely high mortality for that month\n- Small GP practice with one serious incident has high rate per 1000 patients\n- Hospital specialist unit treats sickest patients, genuinely different outcomes\n- Provider serving unique population (e.g., only high-dependency patients)\n\n**CQC implication**: An unusual value doesn't mean poor quality. It might reflect casemix, population served, or a one-off event. Context matters.\n\n### Different Populations\n\n**The value isn't extreme for its true group, but appears extreme when mixed with others**.\n\n**Examples**:\n- Maternity unit compared to general hospital (different denominators, different populations)\n- Small care home (20 beds) compared to large nursing home (200 beds)—precision and operational models differ\n- Specialist vs generalist providers mixed in same analysis\n- Urban vs rural settings with different resource constraints\n\n**CQC implication**: Check whether you're comparing like with like. Stratify analysis by provider type, size, or setting before flagging outliers.\n\n### Process Changes or Special Causes\n\n**Something genuinely changed**. This is what SPC methods (@sec-spc-basics) are designed to detect.\n\n**Examples**:\n- New infection control protocol reduces incidents (genuine improvement)\n- Staffing crisis leads to temporary spike in complaints (temporary deterioration)\n- Regulatory inspection triggers short-term improvement (Hawthorne effect)\n- Policy change affects reporting practices (artifact, not real change)\n\n**CQC implication**: Time-based unusual values warrant investigation of what changed and whether it's sustained. One unusual month might be a blip; six consecutive unusual months signals a systematic change.\n\n## Defining \"Unusual\": Multiple Perspectives\n\nThere's no single \"correct\" way to identify unusual observations. Different definitions serve different purposes.\n\n### Distance from Centre\n\n**How far is this value from the typical value?**\n\n**Approaches**:\n- **Z-scores** (@sec-z-scoring): How many standard deviations from the mean?\n- **Distance from median**: How far from the middle value?\n- **Mahalanobis distance**: Multivariate distance accounting for correlations\n\n**Strengths**: Intuitive, quantifiable, directly connected to statistical inference\n\n**Limitations**: Assumes a particular center (mean or median). Sensitive to skewness and distribution shape.\n\n**When to use**: Symmetric or roughly normal data, comparing providers to a benchmark\n\n**CQC example**: A provider's incident rate is 12 per month, mean is 8, SD is 2. Z-score = (12-8)/2 = 2. This provider is 2 standard deviations above average—moderately unusual.\n\n### Position-Based\n\n**Where does this value sit in the ranking?**\n\n**Approaches**:\n- **Percentiles** (@sec-quantiles): Is this in the top 5%? Bottom 10%?\n- **IQR-based** (Tukey's fences): Beyond 1.5 × IQR from quartiles?\n- **Extreme order statistics**: Minimum or maximum value?\n\n**Strengths**: Robust to outliers themselves. Works with skewed data. Easy to communicate.\n\n**Limitations**: Doesn't account for how extreme the value is, just its rank. With small samples, \"top 5%\" might be very close to median.\n\n**When to use**: Skewed data, ranking providers, when distribution shape unknown\n\n**CQC example**: A care home is in the 95th percentile for falls per bed-day. This means it has more falls than 95% of similar homes. Whether this is concerning depends on how far above the 94th percentile it is and the clinical context.\n\n### Model-Based\n\n**How poorly does this value fit the model?**\n\n**Approaches**:\n- **Residuals**: Difference between observed and predicted values\n- **Leverage**: How much influence this point has on the model\n- **Cook's distance**: Combined influence and residual measure\n\n**Strengths**: Identifies values unusual *for their characteristics*. Accounts for expected variation based on covariates.\n\n**Limitations**: Requires a model. Definition of \"unusual\" depends on model specification. More complex to explain.\n\n**When to use**: Regression analysis, when you're modeling relationships, when you need to account for casemix or confounders\n\n**CQC example**: You're modeling infection rates accounting for ward type, patient acuity, and staffing levels. A hospital has higher rates than predicted by the model—it's an outlier *after accounting for these factors*. This is more informative than just \"high rate\" because it's unusual even for its circumstances.\n\n### Domain Knowledge\n\n**Is this value implausible or concerning based on clinical or operational expertise?**\n\n**Approaches**:\n- **Clinical thresholds**: Mortality >50% in a care home warrants investigation regardless of statistical methods\n- **Regulatory standards**: Staffing below minimum requirements\n- **Historical comparison**: This provider's current rate vs their own history\n- **Operational limits**: Waiting times >24 hours for emergency care\n\n**Strengths**: Incorporates expertise and context. Directly linked to regulatory standards. Easy to justify.\n\n**Limitations**: Can miss statistically unusual but not yet extreme values. Threshold-based, doesn't account for natural variation. May not be consistently applied.\n\n**When to use**: Always, as a complement to statistical methods. First line for serious safety concerns.\n\n**CQC example**: A care home reports zero incidents for 6 months. Statistically, this is an outlier (most homes report some incidents). Clinically, it's implausible—likely underreporting rather than perfect safety. Domain knowledge flags this even if statistical methods don't.\n\n## The Investigation Principle\n\n**Never automatically remove or ignore unusual observations. Always investigate first.**\n\n### Investigation Workflow\n\n**Step 1: Verify the data**\n\n- Check original source (is the value recorded correctly?)\n- Check extraction and processing (did any transformations create this?)\n- Check units and definitions (is this measuring what you think?)\n- Check for duplicates or mismatches (is this the right provider?)\n\n**If it's an error**: Correct it if possible, exclude it if not. Document what you did and why.\n\n**Step 2: Understand the context**\n\n- What kind of provider is this? (size, type, population served)\n- What period does this cover? (one-off event or sustained?)\n- What else do we know? (inspection history, complaints, other indicators)\n- Are there related unusual values? (multiple indicators flagged?)\n\n**If context explains it**: Document the context. May still warrant investigation, but for different reasons than data quality.\n\n**Step 3: Consider the pattern**\n\n- Is this value unusual relative to peers? (cross-sectional outlier)\n- Is it unusual relative to this provider's own history? (temporal outlier)\n- Are similar providers also unusual? (systematic pattern suggesting subgroup)\n- Is the unusual value in the same direction across multiple indicators? (consistent signal)\n\n**If pattern is systematic**: May indicate need to stratify analysis or revise comparator group.\n\n**Step 4: Assess regulatory significance**\n\n- Does this value indicate potential patient harm? (safety threshold)\n- Does it trigger inspection criteria? (regulatory action)\n- Is it clinically as well as statistically significant? (practical importance)\n- What are the consequences of action vs inaction? (risk trade-off)\n\n**If regulatory significance is high**: Proceed to investigation or inspection regardless of statistical uncertainty.\n\n### Documenting Your Investigation\n\nFor QA purposes (@sec-qa-principles), record:\n\n- **What flagged this value as unusual** (which method, which threshold)\n- **What you checked** (data verification steps)\n- **What you found** (error, genuine extreme, explained by context, unexplained)\n- **What you decided** (keep, correct, exclude, investigate further)\n- **Why you decided that** (rationale based on evidence)\n- **What limitations this creates** (how does this affect your analysis)\n\nThis documentation is essential if your analysis is challenged or reviewed.\n\n## Handling Options: Principles\n\nOnce you've investigated, you need to decide what to do. No single approach is always right.\n\n### Keep and Document (Default)\n\n**Principle**: Transparency over convenience. Keep the value, report it clearly, document why it's unusual.\n\n**When to use**:\n- Value is verified as accurate\n- Removing it would introduce bias (e.g., removing high performers makes everyone look worse)\n- You're describing what exists, not estimating a population parameter\n- Regulatory context requires seeing all data\n\n**Advantages**:\n- Transparent and defensible\n- No arbitrary decisions\n- Preserves information\n\n**Disadvantages**:\n- May violate statistical assumptions\n- Can dominate summary statistics (especially with small samples)\n- Requires careful interpretation\n\n**CQC example**: A small care home has one serious incident in a month, giving it the highest rate per bed-day. Keep this value and report \"one small provider had a serious incident this period, inflating their rate.\" Don't exclude it—that would hide important information.\n\n### Transform to Reduce Influence\n\n**Principle**: Mathematical transformation reduces the influence of extreme values without removing them.\n\n**Common transformations**:\n- **Log transformation**: log(x). Reduces positive skew, brings extreme high values closer to the center.\n- **Square root**: √x. Milder than log, suitable for count data.\n- **Inverse**: 1/x. For rate data where small denominators create extreme values.\n- **Winsorizing**: Replace extreme values with less extreme ones (e.g., 95th percentile).\n\n**When to use**:\n- Data is heavily skewed\n- Transformation improves adherence to statistical assumptions (e.g., normality for t-tests)\n- You're modeling and transformation improves model fit\n- Transformation has substantive interpretation (e.g., log for multiplicative processes)\n\n**Advantages**:\n- Retains all data\n- Can improve statistical properties\n- Well-established in some contexts\n\n**Disadvantages**:\n- Changes interpretation (results are on transformed scale)\n- Not always intuitive to communicate\n- May not be appropriate for all data types\n\n**CQC example**: Infection rates per 100 bed-days are heavily right-skewed. Log-transforming rates before comparison makes the distribution more symmetric, reducing the influence of a few very high values. Report results as \"geometric mean rate\" or back-transform to original scale with caveats.\n\n### Use Robust Methods\n\n**Principle**: Choose statistical methods designed to resist the influence of unusual observations.\n\n**Robust alternatives**:\n- Use **median** instead of mean (@sec-describing-data)\n- Use **IQR** instead of standard deviation (@sec-variation-uncertainty)\n- Use **Spearman's correlation** instead of Pearson's (@sec-correlation)\n- Use **non-parametric tests** instead of t-tests when assumptions violated\n- Use **robust regression** methods (available via Guild consultation)\n\n**When to use**:\n- Unusual observations are genuine but distort conventional statistics\n- You want a measure of \"typical\" that isn't affected by extremes\n- Small sample with one or two unusual values\n\n**Advantages**:\n- Simple to apply\n- Well-established alternatives exist\n- Results less sensitive to outliers\n\n**Disadvantages**:\n- Less statistically efficient with clean data\n- May not answer the original question (median ≠ mean)\n- Some robust methods require specialist implementation\n\n**CQC example**: When comparing median waiting times across providers, one provider with very long waits doesn't distort the comparison as much as if you used mean waiting times. Median is robust to this outlier.\n\n### Remove (Last Resort)\n\n**Principle**: Exclude the observation from analysis. Only after investigation and documentation.\n\n**When justified**:\n- Value is a confirmed error that cannot be corrected\n- Value represents a fundamentally different population (e.g., mixing acute hospitals with hospices)\n- Method assumptions critically violated and transformation doesn't help\n- Including the value would produce misleading conclusions\n\n**When NOT justified**:\n- \"It makes my results cleaner\"\n- \"It's more than 3 SD from the mean\" (mechanical application of a rule)\n- \"It doesn't fit my hypothesis\"\n- \"I don't know what else to do\"\n\n**If you remove, you MUST**:\n- Document what you removed and why\n- Report sample size before and after exclusions\n- Conduct sensitivity analysis showing results with and without exclusion\n- Explain why exclusion is justified, not just that the value is extreme\n\n**Disadvantages**:\n- Introduces potential bias\n- Reduces sample size\n- May be difficult to defend\n- Can lead to \"garden of forking paths\" (trying different exclusions until results look good)\n\n**CQC example**: A provider's data is flagged as extreme. Investigation reveals this is a hospice mistakenly included in general hospital analysis. Removing this provider is justified because it's a different provider type, not because the value is unusual. Document: \"One hospice was incorrectly included in hospital analysis and has been reclassified.\"\n\n### Sensitivity Analysis\n\n**Principle**: Test whether your conclusions change depending on how you handle unusual observations.\n\n**Approach**:\n- Analyze with all data included\n- Analyze with unusual observations excluded\n- Analyze with transformation\n- Analyze with robust methods\n- Compare results\n\n**When to use**:\n- High-stakes analysis where conclusions will be scrutinized\n- Borderline decisions (unclear whether to include or exclude)\n- Multiple unusual observations requiring different decisions\n- Building confidence in findings\n\n**Report**:\n- \"Results were robust to outlier treatment: the same three providers were flagged regardless of whether we included extreme values.\"\n- \"Results were sensitive to outlier treatment: Provider X was flagged only when we excluded one extreme observation. We retained all data and flagged Provider X with low confidence.\"\n\n**CQC example**: You're comparing provider incident rates. Two providers have very high rates. Analyze three ways: (1) all data, (2) excluding the two providers, (3) log-transforming rates. If the same ten providers are flagged in all three analyses, your results are robust. If the flagged list changes substantially, report both and explain the sensitivity.\n\n::: {.callout-note icon=false}\n## Rethinking: Why \"Outlier Removal\" Is Often Wrong\n\nTextbooks often present outlier detection as: (1) identify outliers, (2) remove them, (3) analyze the \"clean\" data. This is dangerous in regulatory contexts.\n\n**Why automatic removal is problematic**:\n\n1. **Outliers contain information**. A provider with extreme poor performance is exactly what regulation aims to identify. Removing it hides the problem.\n\n2. **Removal introduces bias**. If you systematically remove high or low values, your estimates of \"typical\" performance are no longer accurate. You're analyzing a subset, not the whole system.\n\n3. **Removal is subjective**. Where do you draw the line? 2 SD? 3 SD? IQR × 1.5? Different thresholds give different results. This is a \"researcher degree of freedom\" that can be exploited.\n\n4. **Removal destroys transparency**. Stakeholders ask \"what about Provider X?\" If you removed it, you have to explain why. \"It was an outlier\" isn't a satisfactory regulatory answer.\n\n5. **Extremes are often the most important cases**. In patient safety, the worst performers are exactly who we need to investigate. Statistical \"cleanliness\" is less important than regulatory purpose.\n\n**Better approach**: Keep unusual observations, investigate them, understand them, report them transparently. If a value fundamentally violates assumptions, use robust methods or transformation rather than removal.\n\n**When removal IS appropriate**: Confirmed errors, fundamentally different populations, cases where inclusion would be scientifically misleading (not just inconvenient). Always document and justify.\n\n**Key principle**: In academic research, outliers can distort estimation of population parameters. In regulation, outliers are often the point. Don't optimize for statistical elegance at the expense of regulatory purpose.\n:::\n\n::: {.callout-note icon=false}\n## Rethinking: Small Providers and the \"Outlier\" Trap\n\nSmall providers (e.g., care homes with 20 beds vs 200 beds) are frequently flagged as statistical outliers. This doesn't mean they're performing poorly—it often reflects **low precision**, not poor quality.\n\n**Why small providers appear extreme**:\n\n- **Small denominators amplify rates**: One incident in a 20-bed home = 5% incident rate. One incident in a 200-bed home = 0.5% rate.\n- **High variability**: Small samples have wider confidence intervals. Values naturally more variable.\n- **Poisson variation**: Count data from small providers has inherently higher variance.\n\n**Example**: Two care homes, both have 1 incident this month:\n- Home A (20 beds): Rate = 1/20 = 5% \n- Home B (200 beds): Rate = 1/200 = 0.5%\n\nHome A appears ten times worse, but both homes had the same number of incidents. The difference is denominator, not quality.\n\n**Regulatory implication**: \n\nA small provider flagged as an outlier requires careful interpretation:\n\n- **If flagged**: Could be genuine problem OR natural variation from small sample. Investigate, but don't assume poor quality.\n- **If not flagged**: Low statistical power means you might miss real problems. Absence of statistical signal ≠ absence of issue.\n\n**Better approaches**:\n\n- **Funnel plots**: Show wider limits for small providers (@sec-z-scoring)\n- **Account for uncertainty**: Report confidence intervals, acknowledge lower precision\n- **Use absolute numbers**: \"3 incidents\" is clearer than \"15% incident rate\" for small providers\n- **Combine indicators**: Small providers unstable on individual metrics, but patterns across multiple indicators more informative\n\n**Key message**: Statistical outlier ≠ quality outlier. For small providers, apparent extremes often reflect sample size, not performance. Don't mistake precision for accuracy.\n:::\n\n::: {.callout-warning icon=false}\n## Overthinking: Winsorizing and Trimming\n\nTwo alternative approaches to extreme values, less commonly used but worth knowing.\n\n**Trimming**: Remove a fixed percentage from each tail.\n- Example: Trim top and bottom 5%, analyze the middle 90%\n- Used in some athletic scoring (drop highest and lowest judge scores)\n- Reduces influence of extremes symmetrically\n- Problem: Arbitrary cutoff, loses information\n\n**Winsorizing**: Replace extreme values with less extreme ones.\n- Example: Replace values >95th percentile with the 95th percentile value\n- Example: Replace values <5th percentile with the 5th percentile value\n- Retains sample size while limiting influence\n- Problem: Creates artificial ceiling/floor, distorts distribution\n\n**When these might be appropriate**:\n\n- Exploratory analysis to see if results are driven by one or two extremes\n- Sensitivity analysis (compare trimmed vs full data)\n- Specific contexts where extreme measurement error suspected\n\n**When NOT appropriate**:\n\n- Regulatory analysis where extremes are the point\n- Final analysis (use for exploration only)\n- Without reporting what you did\n\n**CQC context**: Trimming and winsorizing are rarely appropriate for regulatory work because they systematically exclude or modify the very providers we might need to investigate. Use robust methods instead.\n\nIf you're considering these approaches, consult the Guild—there's usually a better option.\n:::\n\n::: {.callout-warning icon=false}\n## Overthinking: Multivariate Outliers\n\nSo far we've discussed **univariate outliers**: unusual on a single variable. But values can be unusual in combination.\n\n**Multivariate outliers**: Observations that are unusual in multi-dimensional space.\n\n**Example**: A care home has:\n- Staffing: 50th percentile (typical)\n- Incident rate: 60th percentile (slightly above average)\n- Satisfaction: 40th percentile (slightly below average)\n\nIndividually, none of these is extreme. But *together*, low satisfaction with slightly high incidents AND typical staffing might be unusual—most homes with this staffing level and incident rate have higher satisfaction.\n\n**Detection methods**:\n- **Mahalanobis distance**: Measures distance from center of multivariate distribution\n- **Cook's distance**: Influence on regression model\n- **Leverage**: How unusual the combination of predictor values is\n\n**When this matters**:\n\n- Regression analysis (influential points can change model)\n- Cluster analysis (outliers distort groupings)\n- Fraud detection (unusual combinations of characteristics)\n- Casemix adjustment (providers with unusual characteristic combinations)\n\n**CQC relevance**: Limited. Most CQC analysis examines indicators one at a time or in simple relationships. Multivariate outlier detection requires more sophisticated modeling.\n\n**When to escalate**: If you're doing multivariate modeling (regression, clustering, PCA) and suspect influential observations, consult the Guild. Multivariate outlier diagnostics require specialist interpretation.\n\nFor most CQC work, focus on univariate outliers and well-understood relationships.\n:::\n\n## Integration with Other Methods\n\nUnusual observations appear across all statistical methods. Here's how this chapter connects to methods chapters:\n\n**Z-scoring (@sec-z-scoring)**: Built around identifying providers with extreme z-scores (usually |z| > 2 or 3). This chapter provides the framework for interpreting and investigating those flagged providers.\n\n**SPC (@sec-spc-basics)**: Special cause signals are unusual observations in time. This chapter's principles apply: investigate, don't automatically react. Context matters.\n\n**t-tests (@sec-t-tests)**: Outliers can violate normality assumptions. This chapter guides investigation and choice of robust alternatives.\n\n**Correlation (@sec-correlation)**: Outliers heavily influence Pearson's r. This chapter explains when to use robust alternatives (Spearman) or investigate unusual points.\n\n**Outlier Methods (@sec-outlier-methods)**: The methods chapter provides computational details for detection. This chapter provides the conceptual foundation and decision framework.\n\n## Key Takeaways\n\n::: {.callout-important icon=false}\n## Essential Points\n\n1. **No universal definition**: \"Unusual\" depends on your question, data, and context. Distance-based, position-based, model-based, and domain-based definitions all have uses.\n\n2. **Investigate first**: Never automatically remove unusual values. Always investigate: Is it an error? A genuine extreme? Explained by context? Unexplained but real?\n\n3. **Keep and document is the default**: Transparency trumps statistical convenience. Removing unusual observations introduces bias and reduces credibility.\n\n4. **Small providers ≠ poor quality**: Small denominators create high variability. Statistical outliers from small providers often reflect imprecision, not poor performance.\n\n5. **Multiple handling approaches**: Transform, use robust methods, analyze with/without, report sensitivity. Removal is a last resort.\n\n6. **Outliers are often the point**: In regulation, extreme values may be exactly what you're looking for. Don't optimize for statistical elegance at the expense of regulatory purpose.\n\n7. **Document everything**: Your investigation, your reasoning, your decisions. This documentation is essential for QA and defensibility.\n\nThese principles underpin outlier detection methods (@sec-outlier-methods) and appear throughout all statistical methods chapters.\n:::\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../custom.css"],"toc":true,"toc-depth":3,"number-sections":true,"output-file":"07-unusual-observations.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../references.bib"],"theme":"cosmo","number-depth":2,"title":"Unusual Observations and Extreme Values"},"extensions":{"book":{"multiFile":true}}}}}