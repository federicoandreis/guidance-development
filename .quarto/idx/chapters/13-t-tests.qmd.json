{"title":"Comparing Two Groups: t-tests","markdown":{"yaml":{"title":"Comparing Two Groups: t-tests"},"headingText":"| include: false","containsRefs":false,"markdown":"\n\n```{r}\n# Setup Python environment\nlibrary(reticulate)\n# Use system Python (where pandas is installed)\nuse_python(\"C:/Users/fede/anaconda3/python.exe\", required = TRUE)\n```\n\n::: {.callout-tip icon=false}\n## Problem This Method Solves\n\nYou need to test whether two groups have different average values, or whether one group differs from a target value. For example:\n\n- Do waiting times differ between two regions?\n- Has average length of stay changed after an intervention?\n- Is a provider's mean performance different from a national target?\n\nt-tests provide a statistical test for differences in means between groups or against a target value.\n:::\n\n## What t-tests Do {#sec-t-tests}\n\nt-tests compare means (averages) and determine whether observed differences are statistically significant or likely due to chance.\n\n**Key advantages**:\n- Simple, well-understood method\n- Works with small sample sizes (unlike z-tests)\n- Provides clear yes/no answer about statistical significance\n- Widely used and accepted\n\n**What they do NOT do**:\n- Compare multiple providers simultaneously (use z-scoring for that, see @sec-z-scoring)\n- Account for varying sample sizes across providers fairly\n- Monitor changes over time (use SPC for that, see @sec-spc-basics)\n- Prove causation (only association)\n\n## When to Use t-tests vs Other Methods\n\n### t-tests vs z-tests: The Key Distinction\n\n**The fundamental difference**:\n- **z-tests**: Assume you know the population standard deviation (Ïƒ)\n- **t-tests**: Estimate the standard deviation from your sample (s)\n\n**In practice at CQC**:\n- **z-scoring** uses national data to calculate expected variation, making it appropriate for provider comparison\n- **t-tests** estimate variation from the sample itself, making them appropriate for comparing two groups\n\n**Use t-tests when**:\n- Comparing exactly two groups (or one group vs target)\n- Sample sizes are small to moderate (typically <100 per group)\n- You don't have population-level data to estimate variation\n- You need a formal significance test for a specific comparison\n- Groups are clearly defined and independent\n\n**Use z-scoring instead when** (@sec-z-scoring):\n- Comparing many providers against national benchmark\n- Sample sizes vary widely across providers\n- You need to rank or band providers\n- You have population-level data to estimate expected variation\n- **This is the most common CQC use case for provider comparison**\n\n**Use SPC instead when** (@sec-spc-basics):\n- Monitoring one provider/system over time\n- Detecting when change occurred\n- Ongoing process monitoring\n\n**Critical point**: At CQC, z-scoring is usually more appropriate than t-tests for provider comparison because we have national data. t-tests are mainly for:\n- Thematic analysis comparing two groups\n- Pilot evaluations (before/after)\n- Research questions involving exactly two groups\n- Situations where you don't have population-level comparison data\n\n## Before You Start\n\nCheck these requirements before applying t-tests:\n\n### Data Requirements\n- [ ] **Data type**: Continuous data only\n- [ ] **Sample size**: At least 5-10 observations per group (preferably more)\n- [ ] **Independence**: Observations independent unless using paired t-test\n- [ ] **Normality**: Data approximately normally distributed (or large enough for CLT)\n- [ ] **Quality checks**: Completed pre-analysis QA (see @sec-qa-principles)\n\n### When t-tests May Not Be Appropriate\n- **Comparing >2 groups**: Use ANOVA (beyond this guide, consult Guild)\n- **Many providers**: Use z-scoring instead\n- **Highly skewed data**: Consider transformation or non-parametric tests\n- **Very small samples** (<5 per group): Results unreliable\n- **Categorical outcomes**: Use different methods (Chi-squared, etc.)\n\n### What You Need\n- Continuous outcome variable\n- Group membership variable (for two-sample) or target value (for one-sample)\n- Statistical software (R recommended, examples provided)\n\n::: {.callout-note icon=false}\n## ðŸ“– Complete Worked Examples\n\n**Example 4: Medication Safety Pilot** demonstrates paired t-tests with realistic CQC data:\n\n- Comparing before/after measurements\n- Handling paired data correctly\n- Checking normality assumptions\n- Calculating and interpreting effect sizes\n- Reporting results with confidence intervals\n\n**Time**: 2-3 hours | **Complexity**: Medium\n\n[View Example 4: Medication Safety Pilot â†’](../examples/example-04-complete.qmd)\n\n**Example 5: Regional A&E Performance** demonstrates two-sample t-tests:\n\n- Comparing two independent groups\n- Checking equal variance assumption\n- Choosing between pooled and Welch's t-test\n- Interpreting practical vs statistical significance\n\n**Time**: 2-3 hours | **Complexity**: Medium\n\n[View Example 5: Regional A&E Performance â†’](../examples/example-05-complete.qmd)\n:::\n\n## Types of t-tests\n\n### One-Sample t-test\n**Use for**: Comparing one group's mean to a known target value\n\n**Example**: Is average waiting time at a hospital different from the 4-hour target?\n\n**Null hypothesis**: Group mean equals target value\n\n### Two-Sample t-test (Independent)\n**Use for**: Comparing means of two independent groups\n\n**Example**: Do waiting times differ between Region A and Region B?\n\n**Null hypothesis**: Both groups have the same mean\n\n### Paired t-test (Dependent)\n**Use for**: Comparing means of the same group measured twice\n\n**Example**: Did average satisfaction scores change after an intervention?\n\n**Null hypothesis**: Mean difference between paired measurements is zero\n\n## Step-by-Step Guide: One-Sample t-test\n\n### Step 1: State Your Hypotheses\n\n**Null hypothesis (Hâ‚€)**: The group mean equals the target value (Î¼ = Î¼â‚€)\n\n**Alternative hypothesis (Hâ‚)**: The group mean differs from the target value (Î¼ â‰  Î¼â‚€)\n\n**Example**: Testing if average waiting time equals 4-hour target\n- Hâ‚€: Mean waiting time = 4.0 hours\n- Hâ‚: Mean waiting time â‰  4.0 hours\n\n### Step 2: Check Assumptions\n\n**Normality**: Data should be approximately normally distributed\n\n**How to check**:\n1. **Visual inspection**: Create histogram or Q-Q plot\n   - Histogram should be roughly bell-shaped\n   - Q-Q plot points should follow diagonal line\n\n2. **Formal test**: Shapiro-Wilk test\n   - If p > 0.05, data consistent with normality\n   - If p < 0.05, consider transformation or non-parametric alternative\n\n**Independence**: Observations should be independent\n- Each measurement from different unit (e.g., different patients)\n- No repeated measures or clustering\n\n**Sample size**: At least 20-30 observations recommended\n- Smaller samples: Harder to assess normality, less power\n- Larger samples: More robust to normality violations\n\n### Step 3: Calculate the t-statistic\n\n**Formula**:\n$$t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}$$\n\nWhere:\n- $\\bar{x}$ = sample mean\n- $\\mu_0$ = target value (from null hypothesis)\n- $s$ = sample standard deviation\n- $n$ = sample size\n\n**Degrees of freedom**: df = n - 1\n\n**Using software**: Most statistical packages have built-in one-sample t-test functions\n- R: `t.test(data, mu = target)`\n- Python: `scipy.stats.ttest_1samp(data, target)`\n\n### Step 4: Interpret Results\n\n**p-value interpretation**:\n- p < 0.05: Reject null hypothesis (mean significantly different from target)\n- p â‰¥ 0.05: Fail to reject null hypothesis (insufficient evidence of difference)\n\n**Confidence interval**: 95% CI for the mean\n- If CI doesn't include target value â†’ significant difference\n- If CI includes target value â†’ no significant difference\n\n**Effect size**: Calculate difference from target\n- Absolute difference: $\\bar{x} - \\mu_0$\n- Standardized difference (Cohen's d): $(\\bar{x} - \\mu_0) / s$\n\n**Report**:\n- Sample mean and SD\n- t-statistic and degrees of freedom\n- p-value\n- 95% confidence interval\n- Practical interpretation in context\n\n## Step-by-Step Guide: Two-Sample t-test\n\n### Step 1: State Your Hypotheses\n\n**Null hypothesis (Hâ‚€)**: Both groups have the same mean (Î¼â‚ = Î¼â‚‚)\n\n**Alternative hypothesis (Hâ‚)**: Groups have different means (Î¼â‚ â‰  Î¼â‚‚)\n\n**Example**: Comparing waiting times between two regions\n- Hâ‚€: Mean waiting time in Region A = Mean waiting time in Region B\n- Hâ‚: Mean waiting time in Region A â‰  Mean waiting time in Region B\n\n### Step 2: Check Assumptions\n\n**Normality**: Both groups should be approximately normally distributed\n\n**How to check**:\n1. **Visual inspection**: Create histograms or Q-Q plots for each group\n2. **Formal test**: Shapiro-Wilk test for each group\n   - If both p > 0.05, data consistent with normality\n\n**Equal variances**: Both groups should have similar spread\n\n**How to check**:\n1. **Visual inspection**: Compare box plots or histograms\n   - Similar spread suggests equal variances\n2. **Formal test**: Levene's test or F-test\n   - If p > 0.05, variances are similar\n   - If p < 0.05, use Welch's t-test (doesn't assume equal variances)\n\n**Independence**: \n- Observations within each group are independent\n- Groups are independent of each other (no pairing or matching)\n\n### Step 3: Calculate the t-statistic\n\n**Formula (equal variances)**:\n$$t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$$\n\nWhere:\n- $\\bar{x}_1$, $\\bar{x}_2$ = sample means\n- $n_1$, $n_2$ = sample sizes\n- $s_p$ = pooled standard deviation\n\n**Pooled standard deviation**:\n$$s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}$$\n\n**Degrees of freedom**: df = nâ‚ + nâ‚‚ - 2\n\n**Using software**:\n- R: `t.test(group1, group2, var.equal = TRUE)` (or `FALSE` for Welch's)\n- Python: `scipy.stats.ttest_ind(group1, group2, equal_var=True)`\n\n### Step 4: Interpret Results\n\n**p-value interpretation**:\n- p < 0.05: Reject null hypothesis (groups have significantly different means)\n- p â‰¥ 0.05: Fail to reject null hypothesis (insufficient evidence of difference)\n\n**Confidence interval**: 95% CI for the difference between means\n- If CI doesn't include 0 â†’ significant difference\n- If CI includes 0 â†’ no significant difference\n\n**Effect size**: Cohen's d for standardized difference\n$$d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p}$$\n\nInterpretation:\n- |d| < 0.2: Negligible\n- |d| = 0.2-0.5: Small\n- |d| = 0.5-0.8: Medium\n- |d| > 0.8: Large\n\n**Report**:\n- Means and SDs for both groups\n- Difference between means\n- t-statistic and degrees of freedom\n- p-value\n- 95% confidence interval for difference\n- Effect size (Cohen's d)\n- Practical interpretation\n\n## Step-by-Step Guide: Paired t-test\n\n### Step 1: State Your Hypotheses\n\n**Null hypothesis (Hâ‚€)**: Mean difference between paired measurements is zero (Î¼_d = 0)\n\n**Alternative hypothesis (Hâ‚)**: Mean difference is not zero (Î¼_d â‰  0)\n\n**Example**: Testing if satisfaction scores changed after intervention\n- Hâ‚€: Mean change in satisfaction = 0\n- Hâ‚: Mean change in satisfaction â‰  0\n\n### Step 2: Check Assumptions\n\n**Normality of differences**: The *differences* (not the original values) should be approximately normally distributed\n\n**How to check**:\n1. **Calculate differences**: For each pair, compute difference = after - before\n2. **Visual inspection**: Create histogram or Q-Q plot of differences\n   - Histogram should be roughly bell-shaped\n3. **Formal test**: Shapiro-Wilk test on differences\n   - If p > 0.05, differences consistent with normality\n\n**Pairing is appropriate**: Each pair represents the same unit measured twice\n- Same patient before/after\n- Same practice in two time periods\n- Matched pairs (e.g., matched controls)\n\n**Independence of pairs**: Different pairs should be independent\n- Pair 1's measurements don't affect Pair 2's measurements\n\n### Step 3: Calculate the t-statistic\n\n**Formula**:\n$$t = \\frac{\\bar{d}}{s_d / \\sqrt{n}}$$\n\nWhere:\n- $\\bar{d}$ = mean of differences\n- $s_d$ = standard deviation of differences\n- $n$ = number of pairs\n\n**Degrees of freedom**: df = n - 1 (where n = number of pairs)\n\n**Using software**:\n- R: `t.test(after, before, paired = TRUE)`\n- Python: `scipy.stats.ttest_rel(after, before)`\n\n### Step 4: Interpret Results\n\n**p-value interpretation**:\n- p < 0.05: Reject null hypothesis (significant change)\n- p â‰¥ 0.05: Fail to reject null hypothesis (insufficient evidence of change)\n\n**Confidence interval**: 95% CI for the mean difference\n- If CI doesn't include 0 â†’ significant change\n- If CI includes 0 â†’ no significant change\n\n**Effect size**: Cohen's d for paired data\n$$d = \\frac{\\bar{d}}{s_d}$$\n\nInterpretation:\n- |d| < 0.2: Negligible\n- |d| = 0.2-0.5: Small\n- |d| = 0.5-0.8: Medium\n- |d| > 0.8: Large\n\n**Report**:\n- Mean before and after\n- Mean difference and SD of differences\n- t-statistic and degrees of freedom\n- p-value\n- 95% confidence interval for mean difference\n- Effect size (Cohen's d)\n- Practical interpretation in context\n\n## Worked Example: Evaluating a Pilot Intervention\n\n**Scenario**: A care coordination pilot was implemented in 10 GP practices, with 10 control practices for comparison. We want to determine if the pilot significantly improved patient experience scores (measured on a 0-100 scale).\n\n### Step 1: Load and Visualize Data\n\n::: {.panel-tabset}\n## R\n```{r}\n#| echo: false\n# Patient experience scores (0-100 scale)\npilot_practices <- c(78, 82, 75, 80, 77, 81, 79, 76, 80, 78)\ncontrol_practices <- c(72, 70, 73, 71, 69, 72, 70, 71, 73, 70)\n\ndata <- data.frame(\n  score = c(pilot_practices, control_practices),\n  group = rep(c(\"Pilot\", \"Control\"), each = 10)\n)\n```\n\n```{r}\n#| fig-width: 8\n#| fig-height: 5\n# Visualize the data\nboxplot(score ~ group, data = data,\n        main = \"Patient Experience Scores by Group\",\n        ylab = \"Score (0-100)\",\n        col = c(\"lightcoral\", \"lightblue\"))\n```\n\n## Python\n```{python}\n#| echo: false\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\npilot_practices = np.array([78, 82, 75, 80, 77, 81, 79, 76, 80, 78])\ncontrol_practices = np.array([72, 70, 73, 71, 69, 72, 70, 71, 73, 70])\n\ndata = pd.DataFrame({\n    'score': np.concatenate([pilot_practices, control_practices]),\n    'group': ['Pilot']*10 + ['Control']*10\n})\n```\n\n```{python}\n#| fig-width: 8\n#| fig-height: 5\n# Visualize the data\ndata.boxplot(column='score', by='group', figsize=(8, 5))\nplt.title('Patient Experience Scores by Group')\nplt.suptitle('')\nplt.ylabel('Score (0-100)')\nplt.show()\n```\n:::\n\n**Initial observation**: Pilot practices appear to have higher scores than control practices.\n\n### Step 2: Descriptive Statistics\n\n::: {.panel-tabset}\n## R\n```{r}\n# Create summary table\nsummary_stats <- data.frame(\n  Group = c(\"Pilot\", \"Control\"),\n  N = c(length(pilot_practices), length(control_practices)),\n  Mean = c(mean(pilot_practices), mean(control_practices)),\n  SD = c(sd(pilot_practices), sd(control_practices)),\n  Min = c(min(pilot_practices), min(control_practices)),\n  Max = c(max(pilot_practices), max(control_practices))\n)\n\nknitr::kable(summary_stats, digits = 2, \n             caption = \"Descriptive Statistics by Group\")\n```\n\n## Python\n```{python}\n# Create summary table\nsummary_stats = pd.DataFrame({\n    'Group': ['Pilot', 'Control'],\n    'N': [len(pilot_practices), len(control_practices)],\n    'Mean': [np.mean(pilot_practices), np.mean(control_practices)],\n    'SD': [np.std(pilot_practices, ddof=1), np.std(control_practices, ddof=1)],\n    'Min': [np.min(pilot_practices), np.min(control_practices)],\n    'Max': [np.max(pilot_practices), np.max(control_practices)]\n})\n\nprint(summary_stats.to_string(index=False))\n```\n:::\n\n**Observation**: Pilot practices have higher mean scores (difference â‰ˆ 7.5 points) with slightly more variability.\n\n### Step 3: Check Assumptions\n\n::: {.panel-tabset}\n## R\n```{r}\n# Test normality for both groups\nshapiro_pilot <- shapiro.test(pilot_practices)\nshapiro_control <- shapiro.test(control_practices)\n\n# Test equal variances\nvar_test <- var.test(pilot_practices, control_practices)\n\n# Create assumption check table\nassumption_checks <- data.frame(\n  Test = c(\"Normality (Pilot)\", \"Normality (Control)\", \"Equal Variances\"),\n  Statistic = c(shapiro_pilot$statistic, shapiro_control$statistic, var_test$statistic),\n  p_value = c(shapiro_pilot$p.value, shapiro_control$p.value, var_test$p.value),\n  Result = c(\n    ifelse(shapiro_pilot$p.value > 0.05, \"Pass\", \"Fail\"),\n    ifelse(shapiro_control$p.value > 0.05, \"Pass\", \"Fail\"),\n    ifelse(var_test$p.value > 0.05, \"Pass\", \"Fail\")\n  )\n)\n\nknitr::kable(assumption_checks, digits = 4,\n             caption = \"Assumption Checks (p > 0.05 = Pass)\")\n```\n\n## Python\n```{python}\n# Test normality for both groups\nstat_p, p_p = stats.shapiro(pilot_practices)\nstat_c, p_c = stats.shapiro(control_practices)\n\n# Test equal variances\nstat_var, p_var = stats.levene(pilot_practices, control_practices)\n\n# Create assumption check table\nassumption_checks = pd.DataFrame({\n    'Test': ['Normality (Pilot)', 'Normality (Control)', 'Equal Variances'],\n    'Statistic': [stat_p, stat_c, stat_var],\n    'p-value': [p_p, p_c, p_var],\n    'Result': [\n        'Pass' if p_p > 0.05 else 'Fail',\n        'Pass' if p_c > 0.05 else 'Fail',\n        'Pass' if p_var > 0.05 else 'Fail'\n    ]\n})\n\nprint(assumption_checks.to_string(index=False))\n```\n:::\n\n**Conclusion**: All assumptions met (all p-values > 0.05). We can proceed with standard two-sample t-test.\n\n### Step 4: Perform t-test\n\n::: {.panel-tabset}\n## R\n```{r}\n# Two-sample t-test\nresult <- t.test(pilot_practices, control_practices, var.equal = TRUE)\n\n# Calculate effect size (Cohen's d)\npooled_sd <- sqrt(((9 * var(pilot_practices)) + (9 * var(control_practices))) / 18)\ncohens_d <- (mean(pilot_practices) - mean(control_practices)) / pooled_sd\n\n# Create results table\nresults_table <- data.frame(\n  Metric = c(\"Difference in Means\", \"95% CI Lower\", \"95% CI Upper\", \n             \"t-statistic\", \"df\", \"p-value\", \"Cohen's d\"),\n  Value = c(\n    mean(pilot_practices) - mean(control_practices),\n    result$conf.int[1],\n    result$conf.int[2],\n    result$statistic,\n    result$parameter,\n    result$p.value,\n    cohens_d\n  )\n)\n\nknitr::kable(results_table, digits = 4,\n             caption = \"Two-Sample t-test Results\")\n```\n\n## Python\n```{python}\n# Two-sample t-test\nresult = stats.ttest_ind(pilot_practices, control_practices, equal_var=True)\n\n# Calculate effect size (Cohen's d)\npooled_sd = np.sqrt(((9 * np.var(pilot_practices, ddof=1)) + \n                     (9 * np.var(control_practices, ddof=1))) / 18)\ncohens_d = (np.mean(pilot_practices) - np.mean(control_practices)) / pooled_sd\n\n# Calculate 95% CI for difference\ndiff = np.mean(pilot_practices) - np.mean(control_practices)\npooled_se = np.sqrt(stats.sem(pilot_practices)**2 + stats.sem(control_practices)**2)\nci = stats.t.interval(0.95, len(pilot_practices) + len(control_practices) - 2,\n                      loc=diff, scale=pooled_se)\n\n# Create results table\nresults_table = pd.DataFrame({\n    'Metric': ['Difference in Means', '95% CI Lower', '95% CI Upper',\n               't-statistic', 'df', 'p-value', \"Cohen's d\"],\n    'Value': [diff, ci[0], ci[1], result.statistic, \n              len(pilot_practices) + len(control_practices) - 2,\n              result.pvalue, cohens_d]\n})\n\nprint(results_table.to_string(index=False))\n```\n:::\n\n### Interpretation\n\n**Statistical Conclusion**:\n- Pilot practices scored **7.5 points higher** on average (95% CI: 5.8 to 9.2)\n- This difference is **statistically significant** (p < 0.001)\n- Effect size is **very large** (Cohen's d = 4.06)\n\n**Practical Significance**:\n- On a 0-100 scale, a 7.5-point improvement represents a **meaningful change**\n- All pilot practices scored above all control practices (no overlap)\n- Effect size of 4.06 is exceptionally large (>0.8 is considered large)\n\n**Recommendation**: The pilot intervention shows strong evidence of effectiveness. Consider rolling out to additional practices, while noting:\n- Small sample size (N=10 per group) limits generalizability\n- No information on implementation costs or sustainability\n- Need to monitor whether improvements persist over time\n\n## Understanding p-values and Significance\n\n**p-value**: Probability of observing this difference (or more extreme) if the null hypothesis were true.\n\n**p < 0.05**: Conventional threshold for \"statistical significance\"\n- Means: <5% chance of seeing this difference by random chance alone\n- Does NOT mean the difference is large or clinically important\n- Does NOT prove causation\n\n**p â‰¥ 0.05**: \"Not statistically significant\"\n- Does NOT mean groups are the same\n- Could be due to small sample size (low power)\n- Absence of evidence â‰  evidence of absence\n\n**Confidence intervals**: Often more informative than p-values\n- 95% CI: Range likely to contain the true difference\n- If CI includes zero, difference not significant at p < 0.05\n- Width of CI indicates precision of estimate\n\n## Common Pitfalls\n\n**Pitfall 1: Using t-tests for multiple provider comparison**\n- t-tests compare exactly two groups\n- For many providers, use z-scoring (@sec-z-scoring)\n- Multiple t-tests inflate false positive rate\n\n**Pitfall 2: Confusing statistical and clinical significance**\n- p < 0.05 doesn't mean the difference matters\n- Small differences can be \"significant\" with large samples\n- Large differences can be \"non-significant\" with small samples\n- Always report effect size and confidence intervals\n\n**Pitfall 3: Ignoring assumptions**\n- Normality matters more with small samples\n- Unequal variances affect results\n- Non-independence invalidates the test\n- Check assumptions, don't just assume they're met\n\n**Pitfall 4: Misinterpreting non-significance**\n- p > 0.05 doesn't prove groups are the same\n- May just lack power to detect difference\n- Report confidence intervals to show uncertainty\n\n**Pitfall 5: Using wrong test type**\n- Independent t-test for paired data â†’ wrong\n- Paired t-test for independent data â†’ wrong\n- Match test to study design\n\n## Documenting Your Analysis\n\nFor QA purposes, record:\n\n**Data**:\n- What you're comparing and why\n- Sample sizes for each group\n- Any exclusions or data quality issues\n\n**Assumptions**:\n- Normality checks performed (visual and/or tests)\n- Variance equality checks\n- How violations were handled\n\n**Results**:\n- Means and SDs for each group\n- t-statistic, degrees of freedom, p-value\n- 95% confidence interval for difference\n- Effect size (Cohen's d or similar)\n\n**Interpretation**:\n- Statistical significance\n- Clinical/practical significance\n- Limitations and caveats\n- What the results do and don't tell you\n\n::: {.callout-note icon=false}\n## Rethinking: t-tests in Regulatory Context\n\nt-tests are common in research but less common in CQC's routine provider comparison. Why?\n\n**Research context**: Comparing two well-defined groups (treatment vs control, intervention vs usual care). t-tests are perfect for this.\n\n**CQC context**: Usually comparing many providers against national benchmark, or monitoring trends over time. t-tests aren't designed for this.\n\n**When CQC uses t-tests**:\n- **Pilot evaluations**: Comparing pilot sites vs controls\n- **Thematic analysis**: Comparing two provider types or regions\n- **Research questions**: Testing specific hypotheses about two groups\n- **Before/after studies**: Paired t-tests for intervention evaluation\n\n**When CQC uses z-scoring instead**:\n- **Provider comparison**: Comparing all GP practices, care homes, hospitals\n- **Performance monitoring**: Identifying outliers for investigation\n- **Routine indicators**: Most regular analytical work\n\n**Exploratory vs confirmatory**: Most CQC t-tests are exploratory (thematic analysis, pilot evaluation) rather than confirmatory (pre-registered trials). This means:\n- Results guide further investigation, not definitive proof\n- p-values should be interpreted cautiously (optimistic due to analytical flexibility)\n- Look for consistency across multiple analyses and data sources\n- Document analytical decisions and their rationale\n\n**Key lesson**: t-tests answer \"are these two groups different?\" Z-scoring answers \"which providers are unusual?\" These are different questions requiring different methods.\n\n**Practical implication**: If you're comparing providers for regulatory purposes, you probably want z-scoring, not t-tests. If you're evaluating a specific intervention or comparing two defined groups, t-tests may be appropriate.\n:::\n\n::: {.callout-warning icon=false}\n## Overthinking: Assumption Violations and Alternatives\n\n**Normality assumption**:\n\n**When it matters**: More important with small samples (<30 per group). With larger samples, Central Limit Theorem means t-test is robust to non-normality.\n\n**How to check**:\n- Visual: Histogram, Q-Q plot\n- Formal: Shapiro-Wilk test (but overly sensitive with large samples)\n\n**If violated**:\n- Transform data (log, square root) if skewed\n- Use non-parametric alternative (Mann-Whitney U for two-sample, Wilcoxon signed-rank for paired)\n- Increase sample size if possible\n- Consult Guild for guidance\n\n**Equal variance assumption** (two-sample t-test):\n\n**When it matters**: When variances differ by >3-4 fold and sample sizes unequal.\n\n**How to check**:\n- Visual: Side-by-side boxplots\n- Formal: F-test or Levene's test\n\n**If violated**:\n- Use Welch's t-test (var.equal = FALSE in R)\n- This is actually the default in R's t.test()\n- More conservative, doesn't assume equal variances\n\n**Independence assumption**:\n\n**Critical**: If violated, results completely invalid.\n\n**Common violations**:\n- Repeated measures on same subjects (use paired t-test or mixed models)\n- Clustered data (patients within providers, use multilevel models)\n- Time series data (use SPC methods instead)\n\n**If violated**: Don't use standard t-test. Consult Guild for appropriate method.\n\n**Small sample sizes**:\n\n**Problem**: With <5-10 observations per group, t-test has low power and assumptions hard to check.\n\n**Options**:\n- Increase sample size if possible\n- Use non-parametric tests (more robust but less powerful)\n- Report results as exploratory/hypothesis-generating\n- Acknowledge limitations clearly\n\n**Multiple comparisons**:\n\n**Problem**: If you run many t-tests, some will be \"significant\" by chance.\n\n**Example**: Testing 20 indicators, expect 1 false positive at p < 0.05.\n\n**Solutions**:\n- Bonferroni correction (divide Î± by number of tests)\n- False Discovery Rate control\n- Focus on pre-specified primary outcome\n- Report all tests performed, not just significant ones\n\n**When to escalate**: If you're unsure whether assumptions are met, or how to handle violations, consult the Guild. Getting this wrong invalidates your results.\n:::\n\n## Related Approaches\n\n**For different analytical questions**:\n- **Many providers to compare** â†’ Z-scoring (@sec-z-scoring) is more appropriate\n- **Monitoring over time** â†’ SPC (@sec-spc-basics) for time series\n- **Before/after with time series** â†’ SPC with intervention point (Example 2)\n- **Non-normal data** â†’ See Overthinking box above for alternatives\n\n**For advanced comparisons**:\n- **More than two groups** â†’ ANOVA (future section, or consult Guild)\n- **Confounding variables** â†’ ANCOVA or regression (Guild consultation)\n- **Repeated measures** â†’ Mixed models (Guild consultation)\n\n**For foundational concepts**:\n- **Understanding variation** â†’ @sec-variation-uncertainty\n- **Effect sizes and power** â†’ @sec-variation-uncertainty (Overthinking boxes)\n- **Data quality checks** â†’ @sec-qa-principles\n\n**For complete workflows**:\n- **Medication Safety Pilot** â†’ Example 4 (paired t-test)\n- **Regional A&E Performance** â†’ Example 5 (two-sample t-test)\n\n## When to Escalate to the Guild\n\nEscalate to the Quantitative Guild when:\n\n- **Assumption violations**: Data is severely non-normal or variances very unequal, and you're unsure about alternatives\n- **Multiple groups**: Need to compare more than two groups (requires ANOVA, not t-tests)\n- **Repeated measures**: Same providers measured multiple times (requires paired or repeated measures methods)\n- **Unequal sample sizes with unequal variances**: Welch's t-test may be needed but you're unsure\n- **Complex study designs**: Crossover designs, cluster randomization, or other specialized designs\n- **Non-parametric alternatives**: Data doesn't meet assumptions and you need Mann-Whitney or Wilcoxon tests\n- **Power calculations**: Need to determine required sample size before conducting study\n- **Equivalence testing**: Want to show groups are similar (not just fail to show difference)\n\n## Key Takeaways\n\n::: {.callout-important icon=false}\n## Essential Points\n\n1. **t-tests compare means** of two groups or one group vs target\n2. **Choose the right type**: One-sample, two-sample, or paired based on study design\n3. **Check assumptions**: Normality, equal variances (for two-sample), independence\n4. **p-values aren't everything**: Report effect sizes and confidence intervals\n5. **For CQC provider comparison, usually use z-scoring instead**: t-tests are for specific two-group comparisons\n6. **Statistical significance â‰  practical significance**: Always consider context\n7. **When in doubt, consult the Guild**: Especially for assumption violations or complex designs\n\nt-tests are useful for specific two-group comparisons but are not CQC's primary provider comparison method. Understand when they're appropriate and when z-scoring or SPC is better suited to your question.\n:::\n\n## Further Reading\n\n**Internal CQC Resources**:\n\n- **QA Framework**: Documentation standards for comparative analyses\n- **Guild Terms of Reference**: When and how to escalate for specialist support\n\n**External Guidance**:\n\n- **Altman DG** (1991). *Practical Statistics for Medical Research*. Chapman & Hall. (Chapter 9: Comparison of groups - comprehensive coverage)\n- **AQuA Book** (Chapter on Uncertainty): Interpreting statistical tests and confidence intervals\n- **Bland JM, Altman DG** (1995). \"Multiple significance tests: the Bonferroni method.\" *BMJ*, 310(6973), 170. (When testing multiple comparisons)\n\n**Online Resources**:\n\n- Understanding t-tests: https://www.statology.org/t-test/\n- One-sample t-test: https://www.statology.org/one-sample-t-test-excel/\n- Two-sample t-test: https://www.statology.org/two-sample-t-test-excel/\n- Paired t-test: https://www.statology.org/paired-samples-t-test-excel/\n- Effect sizes (Cohen's d): https://www.statology.org/cohens-d/\n- Checking normality assumption: https://www.statology.org/test-for-normality-in-r/\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../custom.css"],"toc":true,"toc-depth":3,"number-sections":true,"output-file":"13-t-tests.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","bibliography":["../references.bib"],"theme":"cosmo","number-depth":2,"title":"Comparing Two Groups: t-tests"},"extensions":{"book":{"multiFile":true}}}}}